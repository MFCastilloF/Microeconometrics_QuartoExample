[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Microeconometrics: Methods and Applications",
    "section": "",
    "text": "Introduction\nSome info about the context."
  },
  {
    "objectID": "00_01_About.html",
    "href": "00_01_About.html",
    "title": "About this book",
    "section": "",
    "text": "Some info about the book."
  },
  {
    "objectID": "00_02_Download.html",
    "href": "00_02_Download.html",
    "title": "How to download the required data",
    "section": "",
    "text": "In order to download the required data for the excersices in this virtual resource you will find the datasets for free acces in (I guess this will not be the definitive repository, but for the moment I will write mine) this GitHub repository."
  },
  {
    "objectID": "01_00_FollowingR.html#introduction",
    "href": "01_00_FollowingR.html#introduction",
    "title": "Following Along with R",
    "section": "Introduction",
    "text": "Introduction\nIn this section we will provide a primer in R, which seeks to provide you with the necessary tools to explore the microeconometric methods introduced throughout the book. R is an open source language focused on statistical computation. R has a long history, dating the early 1980s, and also an ever-growing body of user-contributed add-on packages. It also has an active community providing online support on forums such as Stack Overflow, and a substantial corpus of code online, meaning that large language models will be well suited to support you in your programming in R. It provides you with the large majority of tools you will need throughout this book, as well as the programming environment necessary to extend available tools where required.\nThe goal of this site is not to provide you with a comprehensive introduction to the language, but instead we it seeks to provide you with an overview of the basic tools to understand the required tools that we will use to get up and running in this book. In the first section we will focus on a brief rundown of some principal elements of R without yet getting into the empirical methods discussed in the textbook. Thereafter, we will focus on causal econometric methods, but in each section will also introduce any further tools required to complete key analyses or end-of-chapter questions. The goal of this resource is that after following along with these sections you will be sufficiently well-versed in R that you will comfortably be able to work with real data and econometric implementations. Nevertheless, below we point you to further resources if you are seeking a comprehensive overview of R as a language."
  },
  {
    "objectID": "01_00_FollowingR.html#installing-and-working-with-r",
    "href": "01_00_FollowingR.html#installing-and-working-with-r",
    "title": "Following Along with R",
    "section": "Installing and Working with R",
    "text": "Installing and Working with R\nInformation on how to download R is available on the R website, and further support can be found searching the web. In general, we recommend that you install an Integrated Development Environment (IDE) such as RStudio. RStudio provides a user-friendly interface for R, allowing you to view any elements that you have generated in the memory in R, output like graphics, help documentation, and so forth. An example of what RStudio looks like is provided below, where you can see that code is visible, as well as graphical output, and information about what is in the your “environment”. For further information on installing RStudio, see the RStudio website. You are of course welcome to use R however you prefer, and you may prefer to edit code in your favourite text editor and execute code from the command line. Throughout the rest of these resources, we will assume that you have R installed on your computer and have some way to interact with it, either via an IDE or some other work flow that suits you.\n\n\n\nRStudio screenshot"
  },
  {
    "objectID": "01_00_FollowingR.html#further-resources",
    "href": "01_00_FollowingR.html#further-resources",
    "title": "Following Along with R",
    "section": "Further Resources",
    "text": "Further Resources\nFrom here we will move to a first tutorial about R as a language, and an overview of a number of hey elements. If you are interested in generating a more complete overview of R, a number of free resources can be consulted, such as the following books provided free online:\n\nWickham, H., Cetinkaya-Rundel, M. & Grolemund, G. (2023). R for Data Science . O’Reilly Media. 2nd Edition.\nWickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis, Springer-Verlag New York. Link\n\nWilke, C. (2019). Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media. 1st Edition. Link\n\nA range of other textbooks are available that may help you with your work in R including:\n\nMatloff, N., (2011) The Art of R Programming, No Starch Press.\n\nand, with a particular focus on economics and causal methods:\n\nHuber, M., (2023) Causal Analysis: Impact Evaluation and Causal Machine Learning with Applications in R, MIT Press."
  },
  {
    "objectID": "01_01_IntroductionR.html",
    "href": "01_01_IntroductionR.html",
    "title": "An Introduction to R",
    "section": "",
    "text": "Preliminaries\nWe will assume that when you are reading this you have downloaded R and have been able to access R in some way. This may be via the command line, but most likely we imagine you will have some GUI which allows you to see a number of windows including a space for graphical output, a description of the contents of your R Environment (for example any data you have loaded), potentially a place to write your code in R, and, most importantly, a “shell”, or R interface, which looks like the following:\nHere, R code can be entered directly following a prompt which is the > symbol at the bottom of the window above. From the moment you open R you are in an R session until the moment you close it, from now on understand a session as this time between you open and close R. The most simple way to execute code in Ris to simply input commands into your R console. So, for example, you could write the following at to the right of the > symbol, and R will evaluate this code:\nAs you can see, R has understood the instruction that you want to add 4 plus 4 and show the result, which is evaluated, and output directly below the code in the R console. Later in this chapter we will properly explain these mathematical operations and the different R data types. Beyond evaluating such simple mathematical operations, R has a host of in-built functions which can be invoked at the command line. While we will interact with this in more detail later, a simple example is the following, which you can type in your R console:\nThis function which can be understood as “get working directory” will show you the path on your computer where R is currently working. Thus, any files saved will be exported there, and R will search for any thing to import from this directory. If you want to change this directory you must use the function setwd(\"Path/to/the/directory/you/want/to/use\") with the path written between \" \" or ' ' in order to R understand that is a character, again a topic that will be seen deeper latter. It is important to note that paths should be separated with the slash character: / rather than the backlash \\, and this will work on any operating system. Indeed, if you try to set directories with a backslash, you will see that R returns an error.\nWe will return to discuss functions in more detail a bit later on. For now you will note that we have simply been typing instructions to R directly into the console, and R has been providing us with output. Generally, we will not work this way in an empirical research project, as we will want to develop code over time, and be able to return to R and replicate code in the future. For this reason, generally we will work in R scripts, or files named as something.R (where something will be logical name for the file). We will use these scripts to store the commands which we require for particular processes or routines, and can then save them to the disk of our computer before running them in R. These scripts can be generated in any type of text editor, and then can be run in R using the source() command. For example, below, you can see a screenshot of a sessin on R Studio where we have a basic R script named firstRscript.R, and below this, in the R console we see that the script is run using source and output is provided.\nYou may notice something strange about the code and the output in the R console. The R script has 7 distinct lines of text, but the console only produces two lines of output. The reason for this is because we have included comments, or messages for human eyes, but not for the computer to interpret. Any time that the # character is included in code, R understands that this is a comment, and hence is ignored in executing the code. So, for example, when we write:\nin our code, the R console correctly echoes Hello World! back at us. However, if we enter precisely the same code, but begin the line with a comment symbol, R will not do anything given that it interprets everythin to the right of # as plain text which should be ignored:\nSimilarly, comments can be used within lines:\nAs you can, this lines has executed the instruction to the left of # but ignores all those to the right. It is good practice to comment code extensively, both for sharing code with other users, but also for sharing code with yourself in the future, where you may not remember precisely what you were thinking when you originally wrote your code!\nOne final thing to note from this R script is that the output from the final line of code (4+4) was not printed in the console. In R, when you type 4+4 at the command line, you get immediate output because R is operating in an interactive mode. In interactive mode, R evaluates expressions as soon as you enter them and displays the result to the console. However, when you write 4+4 in a script and then run the script, there is no automatic output to the console. However, it is important to note that the calculation is conducted. You can confirm this yourself if you’d like by explicitly requesting that Rprovide this output when it runs the code, replacing 4+4 in the code with print(4+4).\nUntil now we have executed some code in R and seem some specific functions which print output, but we have never stored any results for later use within our code. Generally we will want to store the output of intermediate steps in our code for later use. This can be done in R using any of the following assignment symbols: ->, = or <- (though -> is generally preferred). For example, below we store a number of values as a variable which we define, arbitrarily named x:\nWith the use of -> or <- the arrows make clear that we are assigning some particular value (or values) to a named variable, where the named variable is indicated by the arrow head. However, with the use of = the assignment must respect name = value. If, instead, we try to assign some variable as value=name an error will occur:\nThis error implies that the code has not been executed, and if we inspect the contents of x we will see that its prior value remains.\nIn general, a command can produce a number of conditions which contain important messages for us. These are:\nTo end this initial introduction, we will briefly explore the generation of basic plots in R. The basic function to make a plot in R is, perhaps unsurprisingly, the plot function. This function maps pairs of points in the \\((x,y)\\) coordinate axis and places them on a graph. For example, let’s consider a case where \\(x\\) is a sequence of integers between -5 and 5. This can be generated as below:\nNow consider the variable \\(y = x^2\\)\nplot, by default generates a scatter plot of points, making reasonable decisions for axes and labelling:\nIf you access help documentation for plot you can see how to add further elements to the graph. For example, below we will request a line plot instead of a scatter plot (type=\"l\"), and add a title and some axis labels:\nThere are many ways to further customise graphics in R, and indeed, we will see a range of other plots throughout the remainder of these pages. One widely-used package to allow for the customisation of graphis in R is the ggplot2 package, incorporated as part of the tidyverse. Given that we have previously loaded the tidyverse we do not need to load ggplot2 again, but if you haven’t loaded it, remember that with the below code you can do so:\nThe ggplot2 package consists of a graphs which can be built up in a step by step fashion. To see this, we will first define an employ plot region:\nThis provices us with a blank canvas where we can add elements, as an example, below we add a line object using the geom_line() function:\nHowever, so far we have not added any data, so ggplot2 is still generating a blank canvas. Now we can add a pair of points, which is done inside the aes mapping, which refers to “aesthetic mappings” we will add to our graphs.\nWe can continue to add elements if we would like, concatenating additional portions of the plot using a + symbol.\nIf using a data frame, which will often be the case when graphing, you can include this is an argument to ggplot, and then directly define aes inside ggplot function, as follows:\nFurther editing graphs is quite simple, allowing alternative themes, colours, and so forth:\nThis results in plots which can be built up piece by piece, for example the below plot will be extended to include an OLS regression line and 95% confidence intervals using the geom_smooth() function, and additionally adding axis labels:\nWe will take forward many of the examples laid out on this page in the chapters ahead. For now, this introductory session aims to provide you with a sufficient overview to get up and running in R, and apply your knowledge to microeconometric methods introduced in the book."
  },
  {
    "objectID": "01_01_IntroductionR.html#data-types",
    "href": "01_01_IntroductionR.html#data-types",
    "title": "An Introduction to R",
    "section": "Data Types",
    "text": "Data Types\n\nText (String) Data\nTo get up and running with R, it is useful to understand the different ways which data can be stored. R can hold a number of different types of objects in its working memory, and it is important that R understands what type of information we are passing or storing. Among others, R allows for information to be stored as characters, numeric, arrays, data frames and lists. Different types of data allow different types of operations, and should be stored in different ways. Characters are perhaps the conceptually simplest type of data. Characters are simply strings of text, and should be specified between \" \", or ' '. We have seen the use of characters when printing “Hello world” previously. If characters are not indicated between strings, R will understand that they refer to variable names. For example, returning to Hello World, if this is entered without quotes, an error will appear:\n\nHello World\n\nError: <text>:1:7: unexpected symbol\n1: Hello World\n          ^\n\n\nIndeed, if we look carefully at the error message we may see that the error has occurred when R arrives to World. This is because it assumes that Hello is a variable name, but this must be followed by some valid assignment. For example, had we written Hello<-\"world\" this would have been perfectly fine! Alternatively, if we do actually want to write this as characters, we can do so simply by enclosing the text in quotes:\n\n\"Hello World\"\n\n[1] \"Hello World\"\n\n\nWe can confirm that this is effectively understood to be character data by using the class() function. This function indicates the type of data of the argument it receives, as we can see below. Note also that if we generate some variable using the assignment operators discussed above, class() will return to us the type of data that the variable contains:\n\nclass(\"Hello World\")\n\n[1] \"character\"\n\nmy_var <- \"Hello World\"\nclass(my_var)\n\n[1] \"character\"\n\n\n\n\nNumeric Data\nNumeric data is entered without any special behaviour, simply writing the number in the case of scalar values. Indeed, if numeric data is accidentally enclosed between quotes, it will be treated as character, and not numeric data. For example, below we assign two variables based on the number 2 and confirm that one is numeric while the other is viewed as a character.\n\nnum1<-2\nnum2<-\"2\"\nclass(num1)\n\n[1] \"numeric\"\n\nclass(num2)\n\n[1] \"character\"\n\n\nIf you try to perform numerical operations based on the above variables (more on this below), you will see that num1 can be involved in such calculations, while num2 cannot.\n\n\nVectors and Arrays\nGenerally, we will be dealing with more than a single number in our work, and for this reason need ways to collect groups of numbers. Vectors and arrays (or matrices) provide some ways to do this. An easy way to create vectors or arrays is via the c() function, which stands for “combine”. This allows for numbers to be combined in a vector. For example,\n\nc(1,2)\n\n[1] 1 2\n\n\nThis is required to group numbers. If we simply write a series of numbers without wrapping them in the c function, error will occur:\n\n1 2 \n\nError: <text>:1:3: unexpected numeric constant\n1: 1 2\n      ^\n\n\n\n1,2\n\nError: <text>:1:2: inesperado ','\n1: 1,\n     ^\n\n\nA precaution to note here is related to mixing of data types in objects that combine different elements because different data types will influence the way this combination works. For example vectors will coerce all data to the most restrictive data type, if you mix characters with numbers in a vector, R will coerce the vector type to a character:\n\nclass(c(1,2))\n\n[1] \"numeric\"\n\nclass(c(1,\"2\"))\n\n[1] \"character\"\n\n\nThis behaviour can be overruled using as. which allows for specific data types to be indicated. For example, if we take the previous vector of a number and a character, we can ensure that the result is numerica as follows:\n\nas.numeric(c(1,\"2\"))\n\n[1] 1 2\n\n\nHowever, note that this does has limits if the underlying object cannot be converted to that data type!\n\nas.numeric(c(1,\"A\"))\n\nWarning: NAs introducidos por coerción\n\n\n[1]  1 NA\n\n\nIn the above, the inclusion of a string which cannot be logically converted to a number implies that R has issued a warning and converted this element to NA. NA is R’s “Not available” or missing value indentifier. We will discuss a bit later on about how to deal with this in your programming.\nYou can also create arrays with the matrix function, this allows you to get objects in two dimensiones, rows and columns\n\nmatrix(c(1,2,3,4), nrow = 2)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nIt is worth noting a couple of potentially important details with these matrices. A first point which we already have come across is related to coercion of types. For example, if passing a vector via the matrix command, R will coerce the matrix to the more restrictive data type:\n\nmatrix(c(1,2,3,\"4\"), nrow = 2)\n\n     [,1] [,2]\n[1,] \"1\"  \"3\" \n[2,] \"2\"  \"4\" \n\n\nA second, potentially more surprising behaviour is the “recycling” of values if matrices are not of the correct length. For example, as you can see below, if we add a 5th element to the vector, this can no longer correctly fit in two rows. So R’s default behaviour is to begin “recylcing” values from the beginning until the matrix is full, and so it returns to the first element of the vector and fills in the “missing” cell. If, however, you actually want this to be an NA value, you need to instruct R about this, as in the second case below.\n\nmatrix(c(1,2,3,4,5), nrow = 2)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    1\n\nmatrix(c(1,2,3,4,5,NA), nrow = 2)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4   NA\n\n\n\n\nData frames\nLikely the type of object which we will be working with the most in our analysis in R are data frames. Data frames allow us to mix mutliple other types of data, viewing the collection of variables and observations as a database with multiple rows and columns. Within data frames, we can easily subset to certain rows (observations), columns (variables), and conduct univariate and multivariate analysis. It will likely be the starting point for much of your work when you import a standard dataset into R. As an example we create a data frame named df that will use later in other examples:\n\ndf <- data.frame(A = c(1, 2, 3), B = c(4, 5, 6), \n                 C = c(\"XXX\", \"YYY\", \"ZZZ\"))\ndf\n\n  A B   C\n1 1 4 XXX\n2 2 5 YYY\n3 3 6 ZZZ\n\n\nAs we have entered the data frame, we have three columns named “A”, “B” and “C”. Each column consists of three observations, and when the data frame is printed, we see that R has helpfully assigned observation numbers in the left-most side of the data frame. Shortly we will see more about how to manipulate and work with data frames.\nThe last data type we will briefly introduce in this section are lists. Lists allow us a very flexible way to group things together, allowing us to store any data type we need. For example, as we see below in a single object we are combining characters, numbers and a data frame, all in a single “container” called a list. The flexibility of lists to contain many data types is why many functions that we will work with later on return their results as lists or as objects created from lists. As in data frames, in lists, each element is given its own name, and when printed are referred to by this name:\n\nl <- list(Character = c(\"Hello\", \"World\"),\n          Number = c(1, 2, 3, 4),\n          DataFrame = df)\nl\n\n$Character\n[1] \"Hello\" \"World\"\n\n$Number\n[1] 1 2 3 4\n\n$DataFrame\n  A B   C\n1 1 4 XXX\n2 2 5 YYY\n3 3 6 ZZZ"
  },
  {
    "objectID": "01_01_IntroductionR.html#subsetting-multidimensional-elements",
    "href": "01_01_IntroductionR.html#subsetting-multidimensional-elements",
    "title": "An Introduction to R",
    "section": "Subsetting Multidimensional Elements",
    "text": "Subsetting Multidimensional Elements\nAs we saw in data frames and lists, often we will work with objects that store multiple elements. Sometimes we may want to work with a subset of this multidimensional data instead the entire object. In data frames, or lists, you can easily access an entire column, or element, by using $ followed by the column, or object, name:\n\ndf$A\n\n[1] 1 2 3\n\nl$Character\n\n[1] \"Hello\" \"World\"\n\n\nBut sometimes you may want to access more, or less, than one entire column or object, so the more general way to access to the elements are with the use of [ ] to subset. For one dimensional objects like vectors or a column of a data frame, we can access elements by using the number of the element position between [ ]. For example, for a column “A” in our data frame:\n\ndf$A[1]\n\n[1] 1\n\n\nIdentical nomenclature is used for vectors.\nThis syntax work to access more than a single object by combining it with a vector, for example as follows to access the first and third element of df$A:\n\ndf$A[c(1, 3)]\n\n[1] 1 3\n\n\nIn the case of multidimensional elements such as an entire data frame or array you should use the [row,column] syntax to control subsetting. If you would like to access an entire row, this can be requested by just leaving the row position blank:\n\ndf[,1]\n\n[1] 1 2 3\n\n\nwith the same logic holding to access an entire column:\n\ndf[1,]\n\n  A B   C\n1 1 4 XXX\n\n\nSimilarly, all of the above can be mixed to grab multiple rows or columns.\n\ndf[2,2]\n\n[1] 5\n\ndf[c(1,3), 2]\n\n[1] 4 6\n\n\nIn lists the syntax is slightly different given the way that lists are internally stored. In this case, first you must access the object of interst with a double [ ] and then once you access the object it behaves in the same way that we saw above. For example, if we wanted to access the second row and second column of our data frame df stored in the list l, we can do so as follows, noting that the data frame is stored as the third item in the list:\n\nl[[3]][2,2]\n\n[1] 5\n\n\nAlternatively, between the first brackets you can also use the element’s name to select it for later processing. The following code exactly replicates the previous example, however arguably is less error prone, because it is clear precisely which elements we wish to select (“DataFrame” from the list, and column “B” from the data frame):\n\nl[[\"DataFrame\"]][2,\"B\"]\n\n[1] 5"
  },
  {
    "objectID": "01_01_IntroductionR.html#basic-operations",
    "href": "01_01_IntroductionR.html#basic-operations",
    "title": "An Introduction to R",
    "section": "Basic Operations",
    "text": "Basic Operations\nThe standard mathematical operations in R likely work how you would expect. Basic mathematical operations are as follows, with an example of their use below:\n\n\n\nSymbol\nOperation\n\n\n\n\n+\nAddition\n\n\n-\nSubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n** or ^\nPower\n\n\n%%\nModulus\n\n\n%/%\nInteger Division\n\n\n\nA few examples\n\n10 + 3\n\n[1] 13\n\n10 - 3\n\n[1] 7\n\n10 * 3\n\n[1] 30\n\n10 / 3\n\n[1] 3.333333\n\n10 ^ 3\n\n[1] 1000\n\n10 ** 3\n\n[1] 1000\n\n10 %% 3\n\n[1] 1\n\n10 %/% 3\n\n[1] 3\n\n\nAlso when you combine different operators it respects the PEMDAS order, first resolving Parentheses, second solve Exponents, third Multiplication and Division, fourth Addition and Substraction. Both multiplication and division, and addition and substraction are evaluated left to right, rather than sequentially by operation, so it is important to indicate to R explicitly what you need using parentheses in certain cases. For example in order to solve \\[-\\frac{5 + 3^{5-3}}{5\\times 3}\\] You should use the code\n\n-(5+3^(5-3))/(5*3)\n\n[1] -0.9333333\n\n\nSpecifically in the case of matrices, mathematical operations are assumed to refer to cell by cell calculations. If, instead, you would like to undertake matrix multiplication, this must be requested with %*%. For example, consider the two operations below and why they differ, also noting the t() is the matrix transpose operation.\n\na=matrix(c(1,2,3,4,5,6), nrow=2)\na*a\n\n     [,1] [,2] [,3]\n[1,]    1    9   25\n[2,]    4   16   36\n\na%*%t(a)\n\n     [,1] [,2]\n[1,]   35   44\n[2,]   44   56\n\n\nR also has logical operatos that returns a boolean value (TRUE or FALSE, also abbreviated as T or F) pointing if the statement is true or false. This operators are:\n\n\n\nSymbol\nComparison\n\n\n\n\n==\nEquals\n\n\n!=\nDifference\n\n\n>\nGreater\n\n\n>=\nGreater or Equal\n\n\n<\nLess than\n\n\n<=\nLess than or Equal\n\n\n%in%\nIf it is in\n\n\n|\nOr\n\n\n&\nAnd\n\n\n\nA few examples\n\n4 == 3\n\n[1] FALSE\n\n4 != 3\n\n[1] TRUE\n\n4 > 3\n\n[1] TRUE\n\n4 >= 3\n\n[1] TRUE\n\n4 < 3\n\n[1] FALSE\n\n4 <= 3\n\n[1] FALSE\n\n4 %in% c(1,2,3,4)\n\n[1] TRUE\n\nTRUE | FALSE\n\n[1] TRUE\n\nTRUE & FALSE\n\n[1] FALSE\n\n\nThis boolean values are interpreted by R as the numbers 0 for F and 1 for T, characteristic that allows you work with them numerically and is useful when you want to check a series of conditions."
  },
  {
    "objectID": "01_01_IntroductionR.html#functions",
    "href": "01_01_IntroductionR.html#functions",
    "title": "An Introduction to R",
    "section": "Functions",
    "text": "Functions\nBeyond these logical and mathematical operators, R comes with a large suite of built in functions which perform specific tasks, as well as a large number of user-contributed functions (discussed below) which can extend the basic functionality. A basic installation of R comes with a range of packages. The functions included in R’s base package can be consulted by typing the following at your R console:\n\nlibrary(help=\"base\")\n\nFunctions are designed to perform specific tasks, potentially accepting input (known as “arguments”), and returning output which you can interact with. We have already seen a number of functions above such as getwd() which we used without arguments, and print() which we have used with specific arguments such as “Hello World!”. Arguments are simply the contents we provide the function inside the parentheses. Understanding how packages work, including the arguments they accept and the objects they return is key. This can be found by consulting the help file. If you know the function you wish to use, all these details can be found by typing help(function_name) or simply ?function_name. For example, if we type help(getwd) we will see a section entitled “Arguments” which explains to us the arguments the function accepts, as well as the elements returned under the heading “Value”. In certain cases if we are not sure exactly what function we need we can type ??search_term and R will advise us of any functions which may do what we’re after. For example, if we type ?? sum, we see a range of functions, including one called sum. We can then consult the help file to see if this is precisely what we are after. Reading the help file we see that the function “returns the sum of all the values present in its arguments”, and accepts numerical vectors. We can confirm that this is the case by applying this function to our data:\n\nsum(df$A)\n\n[1] 6\n\n\nIf you consult the help file to sum you will note quite prominently the option for “na.rm”, with the explanation Should missing values (including ‘NaN’) be removed?. You will also note that in the “Usage” section of the help file, “na.rm = FALSE” is explicitly indicated. Here we have encountered two particularly important elements: firstly, the concept of NA which we briefly mentioned above, and secondly the use of default options in functions. We will discuss these in turn. NA formally refers to Not available, or missing data. Similar concepts exist in other languages, such as NaN in Matlab, “.” in Stata, and so forth. Frequently we may encounter data where certain individuals have no information for certain variables. For example, we may have a data with some individuals who participate in the labour market and some who do not, where the hourly wage is a variable. For individuals who do not participate in the labour market, it probably makes most sense for their hourly wage to be recorded as NA. In R, these missing values can be recorded as NA. Functions must know how to deal with values which are recorded as NA. This is what sum does with its na.rm option. If na.rm = TRUE, values of NA will be removed prior to processing. Otherwise, vales of NA will remain in the data when processing. What’s more, the fact that in its “Usage” section the help file tells us na.rm = FALSE implies that FALSE is the default behaviour. Should we not say anything, R understands that as us accepting this default. To see how this works, we will define a vector x below with a missing value NA\n\nx <- c(1, 2, NA)\n\nNow, if we use the sum function this will return to us an NA value, given that we can’t logically sum over a vector containing NA, because NA has no mathematical value:\n\nsum(x)\n\n[1] NA\n\n\nHowever, if we explicitly tell R that we wish to remove the NAs prior to processing, it will, returning us the sum of the remaining two elements:\n\nsum(x, na.rm = TRUE)\n\n[1] 3\n\n\nThis behaviour ensures that we will know if our vectors contain NA, and in effect R makes us confirm to it that we are happy with this to return numerical values.\nAlong with the sum function, R comes with a considerable range of functions in the base environment, including functions to generate descriptive statistics, work with dates, conduct statistical modelling, amond many other things. These functions are sufficient to do nearly anything we will need to do, but are not necesarily optimal for all our tasks. As we will see now, there are a large number of useful functions and data types that don’t come with base R, but are avaialble as packages which be installed externally."
  },
  {
    "objectID": "01_01_IntroductionR.html#managing-packages",
    "href": "01_01_IntroductionR.html#managing-packages",
    "title": "An Introduction to R",
    "section": "Managing packages",
    "text": "Managing packages\nWhat are packages? Packages are a collection of functions, help files, and potentially data, which allow for the implementation of some group of procedures. These are add-on elements to R, which can significantly ease certain routines which are not a part of base R. In order to install a package you have several alternatives depending on how the package is distributed, though installing a package is a process which you only need to do once, and from then on this package will be avaialble to you should you wish to load it and use it. The majority of stable R packages are stored on the CRAN (the Comprehensive R Archive Network), which is a repository from which packages can be installed directly into your version of R. By far the most frequent way to install R packages is to install them from CRAN. This can be done quite simply, using the install.packages() function. This function accepts as an argument the name of the package you wish to install. For example, to install dplyr, a package we will work with beloe for data manipulation, you simply execute the following code:\n\ninstall.packages(\"dplyr\")\n\nNow that we have installed this package, it is available to you in the future and you don’t need to install it again unless you wish to update the package. However, that a package is installed doesn’t mean that R automatically loads the package (along with its functions and data types). You must instruct R in every session that you wish to use load the package. For example, if you want to use the function mutate which is a function available as part of the dplyr package that allows you generate new columns as functions of old columns in data, you cannot unless you first tell R that you wish to load dplyr. For example, below we try to conduct a valid operation with mutate without having loaded dplyr:\n\nmutate(df, C = A + B)\n\nError in mutate(df, C = A + B): no se pudo encontrar la función \"mutate\"\n\n\nTo avoid such errors, we must simply load the package with R’s library() function:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nNow if you want to use dplyr’s function mutate it will work\n\nmutate(df, C = A + B)\n\n  A B C\n1 1 4 5\n2 2 5 7\n3 3 6 9\n\n\nTo avoid unexpected errors, it is important to ensure that packages are correctly lodad each time you use R. Generally, the best practice is to simply load the packages that you require in the R script that you are working on, as then each time you run the script you will be sure that you have the correct packages loaded.\nAt first glance, it may seem like failing to load a package is not so serious, because R will not find the package and simply produce an error, which you can then remedy by loading the package. But this is not necessarily the case! At times, different packages may have functions named in exactly the same way, and if you fail to tell R you are using a particular package, it may find another function with the same name, that does not necessarily do the same thing. This can lead to considerable confusion, hence the importance of being careful with your loading of packages. As a matter of fact, this is something that we have seen when we loaded the dplyr package! If you note above, a Message was printed when we loaded dplyr to draw our attention to some functions which exist in dplyr, and also in base R. When we load a package, these pre-existing functions will be masked, implying that R will use the newly loaded functions, and not the masked versions. To see why this matters, consider the first message above: “The following objects are masked from ‘package:stats’:”. And note what happens when we use dplyr’s lag function:\n\ndf$A\n\n[1] 1 2 3\n\nlag(df$A)\n\n[1] NA  1  2\n\n\nIn this case, we see it produces a vector which takes, for each observation, the observation one unit prior in the vector. However, we can also see what the function does with the stats package by using the syntax package::function, without first loading the package:\n\nstats::lag(df$A)\n\n[1] 1 2 3\nattr(,\"tsp\")\n[1] 0 2 1\n\n\nHere, the same function produces a different result: namely for each observation it takes the mathematical value one below it. In many cases such clashes in function names will not occur, but we note the importance of this so that you can be cautious to always load the packages you need, and if in some strange case you do actually need two functions with the same name in different packages, that you have the knowledge that you can “unmask” functions by pre-empting the function name with the package name."
  },
  {
    "objectID": "01_01_IntroductionR.html#loading-and-viewing-data",
    "href": "01_01_IntroductionR.html#loading-and-viewing-data",
    "title": "An Introduction to R",
    "section": "Loading and viewing data",
    "text": "Loading and viewing data\nIn most – if not all – of our research projects, we will not actually type in data to R by hand which is a cumbersome and highly error-prone activity, but rather we will wish to read in data which we have stored on our disk in one or a number of files. Such data may be stored in many formats; among others this may be free formats such as csv or txt, or even proprietry formats such as excel or dta. The first such formats do not require specific packages to be loaded into R, but the latter do.\nFor data stored as csv (comma seperated values) or txt formats R comes with the functions read.csv, read.csv2, read.delim and read.delim2. If you consult the help documentation, which is something I would recommend anytime you come across a new function, you will see that the four of these functions belons to the family of the read.table function and have a very similar usage, along the lines of the following:\n\nread.csv(file=\"Path/To/Your/File.csv\")\n\nNote that you could do this all directly with the read.table function, but would just need to make clear any file-specific details, such as the characters which indicate column breaks, whether a header is included, and so forth. So, for a file where columns are separated by commas and which variable names are included, we could mimic the above behaviour by typing:\n\nread.table(file=\"Path/To/Your/File.csv\", header=TRUE, sep=\",\")\n\nSimilar such procedures can be followed for other data stored in flat formats such as .txt or others, as laid out in the help file.\nIn the case of files which are stored in other formats, more specialised packages are likely required. For example, for excel files you could consider one of a number of packages available from CRAN, such as readxl, openxlsx and xlsx. Each package provides functions to deal with data stored in xlsx format, along with different options such as reading or writing files depending on the particular package. Once again, in general the basic syntax is simple, and further control can be achieved as desired following instructions in the help fules. For example, if we have installed and loaded the readxl library, we can use functions such as read_xls, read_xlsx and read_excel as follows.\n\nread_xlsx(path = \"Path/To/Your/File.xlsx\")\n\nIn practice the syntax can be more complex as you may wish to import only some specific sheet, or some specific range of rows or columns.\nFinally if you wish to import other proprietry data formats such as Stata’s dta files, you could install the haven package, and use the functions read_stata or read_dta, both with the basic usage as follows\n\nread_dta(file = \"Path/To/Your/File.dta\")\n\nThe examples so far have been to illustrate basic syntax, though imported data has not been stored in any particular object for later manipulation. In general we will of course want to go on to use the data we load, which is why we loaded it in the first place. This can be done in the same way as any other assignment in R. Below we will import a particular dataset which is available internally in R (ie you will not have to download anything), to have some data to work with below. In particular, we will load a dataset from the package wooldridge, that contains 115 data sets from Wooldridge’s undergraduate Econometrics Textbook (“Introductory Econometrics: A Modern Approach”, 7th Edition by Jeffrey Wooldridge):\n\nmroz <- wooldridge::mroz\n\nThis dataset contains the data from T. A. Mroz (1987), “The Sensitivity of and Empirical Model of Married Women´s Hours of Work to Economic and Statistical Assumptions”, Econometrica 55, 765-799, and we have stored it as an object called mroz. We can inspect this element to see how the data has been imported:\n\nclass(mroz)\n\n[1] \"data.frame\"\n\n\nseeing that, conveniently, it has been stored as a data frame. We can also look at a quick snapshot of the data by using the head() command, which will print just the first few rows of our data:\n\nhead(mroz)\n\n  inlf hours kidslt6 kidsge6 age educ   wage repwage hushrs husage huseduc\n1    1  1610       1       0  32   12 3.3540    2.65   2708     34      12\n2    1  1656       0       2  30   12 1.3889    2.65   2310     30       9\n3    1  1980       1       3  35   12 4.5455    4.04   3072     40      12\n4    1   456       0       3  34   12 1.0965    3.25   1920     53      10\n5    1  1568       1       2  31   14 4.5918    3.60   2000     32      12\n6    1  2032       0       0  54   12 4.7421    4.70   1040     57      11\n  huswage faminc    mtr motheduc fatheduc unem city exper  nwifeinc      lwage\n1  4.0288  16310 0.7215       12        7  5.0    0    14 10.910060 1.21015370\n2  8.4416  21800 0.6615        7        7 11.0    1     5 19.499981 0.32851210\n3  3.5807  21040 0.6915       12        7  5.0    0    15 12.039910 1.51413774\n4  3.5417   7300 0.7815        7        7  5.0    0     6  6.799996 0.09212332\n5 10.0000  27300 0.6215       12       14  9.5    1     7 20.100058 1.52427220\n6  6.7106  19495 0.6915       14        7  7.5    1    33  9.859054 1.55648005\n  expersq\n1     196\n2      25\n3     225\n4      36\n5      49\n6    1089\n\n\nDepending on the way that we are working with R, we can also explore the data more interactively using the View() function."
  },
  {
    "objectID": "01_01_IntroductionR.html#tidying-data",
    "href": "01_01_IntroductionR.html#tidying-data",
    "title": "An Introduction to R",
    "section": "Tidying data",
    "text": "Tidying data\nOnce we have loaded data in R, we will likely want to manipulate the data in a number of ways. This can include relatively simple things like sub-setting to certain columns or rows (which we have already encountered above), but also more complicated things like aggregating data to a higher level, calculating statistics for one or mutliple variable(s) over other variables, joining together a number of datasets, or “reshaping” data in certain ways. Here we will provide a very brief overview of these procedures, and introduce other things as we need them thoughout the book.\nWhile these things can all be done in Base R, an increasingly common way to do such tasks is via a group of functions known as the tidyverse. These packages seek to provide a common logic for data manipulation (and other) tasks, and includes packages such as dplyr (data manipulation), tibble (a simplified version of data frames), tidyr (tidying or otherwise “wrangling” data) and others like ggplot2 for plots, lubridate for dates and stringr for strings. We will discuss some of these libraries below. As with other packages, the tidyverse can be loaded from the CRAN, though note that this will install a range of packages and so may a quite memory-heavy operation. Provided that we have installed the tidyverse, we can load it as standard:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ ggplot2   3.4.3     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\n\nAggregating Data\nTo start with some basic data manipulations, we can return to the dplyr package discussed previously. By typing help(package=\"dplyr\") we can see a full index of the range of functions included. For example, imagine that we wished to turn our individual level data frame we imported as mroz above into some group-level averages. We can do this using dplyr’s summarise function. Below, we will calculate the mean number of children aged less than six:\n\nsummarise(mroz, mean(kidslt6))\n\n  mean(kidslt6)\n1     0.2377158\n\n\nIndeed, here with summarise we have requested that it provide us with the mean of this variable, but we could just as easily request some other statistic such as sd, min, max, median, quantile etc. It is quite simple to concatenate operations to conduct a range of operations. For example, imagine that if instead of wishing to simply know the mean number of children less than 6 in our data, we wanted to know this for women in the labour market and those out of the labour market. We could easily add this distinction using dplyr’s group_by function. We will also name the resulting average “meankidslt6”:\n\nsummarise(group_by(mroz, inlf), meankidslt6 = mean(kidslt6))\n\n# A tibble: 2 × 2\n   inlf meankidslt6\n  <int>       <dbl>\n1     0       0.366\n2     1       0.140\n\n\nAs we will see below, these operations can become somewhat complex and a bit tricky to follow, and so dplyr allows for an alternative way to write these commands, essentially breaking out the functions into a series of steps, where the output of each step is “piped” into the next step. This is by using the “pipe operator”, written as %>%. To see what this looks like, consider the previous command, and note that we can alternatively write it as follows:\n\nmroz %>% group_by(inlf) %>% summarise(meankidslt6 = mean(kidslt6))\n\n# A tibble: 2 × 2\n   inlf meankidslt6\n  <int>       <dbl>\n1     0       0.366\n2     1       0.140\n\n\nYou will see that the output is identical, though potentially the input is easier to follow. In some examples further below, we will see that when the number of functions in a pipe grows, it is likely easier to follow in pipe notation than as a single nested function.\nYou may have noticed that the output of these commands was a type of data which we have not come across before. This is a tidyverse-specific construct known as a “tibble”, essentially a modified data frame including features including enhanced printing (refer to help(tibble) for full information). However, if you actually did prefer to generate a standard data frame, we could certainly do this, either via piping or just enclosing everything in a data.frame() function. For example, in the below line we will save the output from the above pipe in a data frame named kids:\n\nkids<- mroz %>% group_by(inlf) %>% summarise(meankidslt6 = mean(kidslt6)) %>%\n  data.frame()\n\nNote that the only addition to the pipe is the last step converting output to a data frame.\n\n\nMerging/Joining Data\nWe will also, at times, wish to join two (or more) pieces of data together, for example “merging” data frames or tibbles by a column or columns. For example, it may be that we have a survey where each individuals surveyed belongs to a household, where all of the individual-level questions in the data are stored as an individual-level file (along with the individual’s household ID), while the household level measures are stored in another file at the household level. If we wish to work with information on the individual and their household characteristics, we will need to “join”, or “merge” the file using the household ID.\nWe can see an example of this using a series of joining functions included as part of dplyr. A simple example is if we would like to take the grouped-level statistics generated previously, and join them back into our original mroz data. What we want to do then, is for each individual who is in the labour force (inlf==1), join the information on the mean number of children under 6 in this group to their row of data, and similarly, for individuals not in the labour force (inlf==0), incorporate the group-level value to their row.\n\nmroz %>% group_by(inlf) %>% summarise(MeanKidslt6 = mean(kidslt6)) %>%\n   left_join(mroz, by = \"inlf\")\n\n# A tibble: 753 × 23\n    inlf MeanKidslt6 hours kidslt6 kidsge6   age  educ  wage repwage hushrs\n   <int>       <dbl> <int>   <int>   <int> <int> <int> <dbl>   <dbl>  <int>\n 1     0       0.366     0       0       1    49    12    NA       0   2550\n 2     0       0.366     0       2       0    30    16    NA       0   1928\n 3     0       0.366     0       1       0    30    12    NA       0   1100\n 4     0       0.366     0       0       4    41    12    NA       0   3193\n 5     0       0.366     0       0       1    45    12    NA       0   2250\n 6     0       0.366     0       0       5    43    12    NA       0   2012\n 7     0       0.366     0       0       1    42    13    NA       0   3856\n 8     0       0.366     0       0       0    60    12    NA       0   1645\n 9     0       0.366     0       0       0    57    12    NA       0   1554\n10     0       0.366     0       0       2    38    10    NA       0   2352\n# ℹ 743 more rows\n# ℹ 13 more variables: husage <int>, huseduc <int>, huswage <dbl>,\n#   faminc <dbl>, mtr <dbl>, motheduc <int>, fatheduc <int>, unem <dbl>,\n#   city <int>, exper <int>, nwifeinc <dbl>, lwage <dbl>, expersq <int>\n\n\nYou will note that the only new part of the pipe is the addition of left_join() where we state that we wish to join the output of our pipe with the original mroz data, joining by the variable inlf. It would be exceedingly easy to generalise this, for example if we wished to insted have the aggregates by labour market status and the individual’s age:\n\nmroz %>% group_by(inlf,age) %>% summarise(MeanKidslt6 = mean(kidslt6)) %>%\n   left_join(mroz, by = c(\"inlf\", \"age\"))\n\n`summarise()` has grouped output by 'inlf'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 753 × 23\n# Groups:   inlf [2]\n    inlf   age MeanKidslt6 hours kidslt6 kidsge6  educ  wage repwage hushrs\n   <int> <int>       <dbl> <int>   <int>   <int> <int> <dbl>   <dbl>  <int>\n 1     0    30        1.26     0       2       0    16    NA       0   1928\n 2     0    30        1.26     0       1       0    12    NA       0   1100\n 3     0    30        1.26     0       2       0    17    NA       0   3570\n 4     0    30        1.26     0       2       2    11    NA       0   2352\n 5     0    30        1.26     0       2       0    17    NA       0   2240\n 6     0    30        1.26     0       1       2    12    NA       3   2940\n 7     0    30        1.26     0       2       2    12    NA       0   2100\n 8     0    30        1.26     0       1       0    12    NA       0   2315\n 9     0    30        1.26     0       1       1    12    NA       0   2496\n10     0    30        1.26     0       1       1    12    NA       0   3119\n# ℹ 743 more rows\n# ℹ 13 more variables: husage <int>, huseduc <int>, huswage <dbl>,\n#   faminc <dbl>, mtr <dbl>, motheduc <int>, fatheduc <int>, unem <dbl>,\n#   city <int>, exper <int>, nwifeinc <dbl>, lwage <dbl>, expersq <int>\n\n\nHere we have used the function left_join() to conduct our merge. In this particular setting, we could have used any if inner_join, left_join, right_join or full_join. This is because in this case, all of the levels of “inlf” or “inlf” and “age” which are in one set of data are also in the other set of data. In many cases this may not be the case, and we may have observations in one dataset that are not in the other. In this case, we must be careful to indicate whether we wish to keep observations that appear in both sets of data (inner_join), keep those that only appear in the left-hand side of the pipe (left_join), those that only appear in the right-hand side of the pipe (right_join), or those appearing in either data set (full_join).\n\n\nFiltering Data\nPreviously we have discussed how to select particular columns or rows of data, however we can also filter on observation values. This is straightforward, and can be incorporated in pipes using the filter function. Additionally, if we wish to use the standardised syntax of tidyverse we can use the select function to choose certain variables or groups of variables. For example, below we will subset our data first to a pair of variables by using select, and then keep only individuals with at least 2 children under 6 years of age using the filter function:\n\nmroz %>% select(kidslt6, hours) %>% filter(kidslt6 > 2)\n\n  kidslt6 hours\n1       3     0\n2       3     0\n3       3     0\n\n\n\n\nGenerating variables\nAbove we have briefly introduced the idea of generating variables using the mutate function. In tibbles as well as in data frames the use of $ operator also creates new variables:\n\nmroz$exper2 <- mroz$exper^2\nhead(mroz)\n\n  inlf hours kidslt6 kidsge6 age educ   wage repwage hushrs husage huseduc\n1    1  1610       1       0  32   12 3.3540    2.65   2708     34      12\n2    1  1656       0       2  30   12 1.3889    2.65   2310     30       9\n3    1  1980       1       3  35   12 4.5455    4.04   3072     40      12\n4    1   456       0       3  34   12 1.0965    3.25   1920     53      10\n5    1  1568       1       2  31   14 4.5918    3.60   2000     32      12\n6    1  2032       0       0  54   12 4.7421    4.70   1040     57      11\n  huswage faminc    mtr motheduc fatheduc unem city exper  nwifeinc      lwage\n1  4.0288  16310 0.7215       12        7  5.0    0    14 10.910060 1.21015370\n2  8.4416  21800 0.6615        7        7 11.0    1     5 19.499981 0.32851210\n3  3.5807  21040 0.6915       12        7  5.0    0    15 12.039910 1.51413774\n4  3.5417   7300 0.7815        7        7  5.0    0     6  6.799996 0.09212332\n5 10.0000  27300 0.6215       12       14  9.5    1     7 20.100058 1.52427220\n6  6.7106  19495 0.6915       14        7  7.5    1    33  9.859054 1.55648005\n  expersq exper2\n1     196    196\n2      25     25\n3     225    225\n4      36     36\n5      49     49\n6    1089   1089\n\n\nThis is the standard way that we will generate variables in R, and it can work with any of the mathematical or logical operations, as well as many numerical outputs from functions discussed above.\n\n\n\n\n\nReshaping Data\nFinally the last tool that we introduce for data manipulation is the idea of “reshaping” or transforming data from a wide format to a long format (and vice versa). We refer to data in a long format if for each observation we have multiple rows of data, covering different levels of some other variable. For example, a classic example of this would be a panel of data over countries and years. On the other hand, we refer to data as being in a wide format if for each observation we have multiple columns (or variables) which cover different levels of some other variable. In certain settings we will wish to work with data in one format, and in other settings we may find it more convenient to work with data in another format. To make this a bit clearer, consider the below data frame:\n\nData <- data.frame(Zone = c(\"A\", \"A\", \"A\", \"B\", \"B\", \"B\"), \n                   Year = c(2020, 2021, 2022, 2020, 2021, 2022),\n                   Sales = c(90, 100, 115, 82, 98, 106))\nData\n\n  Zone Year Sales\n1    A 2020    90\n2    A 2021   100\n3    A 2022   115\n4    B 2020    82\n5    B 2021    98\n6    B 2022   106\n\n\nHere we can see that data is in the long format, with multiple observations for one unit (zone in this example) over a number of years. To convert this data to wide format we can use the pivot_wider function, as follows:\n\nData = pivot_wider(data = Data, id_cols = Zone, names_from = Year,\n                   values_from = Sales)\nData\n\n# A tibble: 2 × 4\n  Zone  `2020` `2021` `2022`\n  <chr>  <dbl>  <dbl>  <dbl>\n1 A         90    100    115\n2 B         82     98    106\n\n\nAs you can see now there is now one observation per unit, where each observation has three resulting columns (one for each year). Now, if we wish to return to the long format we can do so with the similar pivot_longer function:\n\nData = pivot_longer(data = Data, names_to = \"Year\", values_to = \"Sales\",\n                    cols = -Zone)\nData\n\n# A tibble: 6 × 3\n  Zone  Year  Sales\n  <chr> <chr> <dbl>\n1 A     2020     90\n2 A     2021    100\n3 A     2022    115\n4 B     2020     82\n5 B     2021     98\n6 B     2022    106\n\n\nIn practice, we will generally use these commands with a considerably larger number of units, but the basic principles are the same regardless."
  },
  {
    "objectID": "01_02_Chapter2R.html",
    "href": "01_02_Chapter2R.html",
    "title": "Details required for Chapter 2",
    "section": "",
    "text": "Here will be the Chapter 2 excersises and examples.\n\nBlock 2.1\n\n\n\nCall:\nlm(formula = ind_fin_el1 ~ treatment, data = subset(data, el1 == \n    1))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2555 -0.3219 -0.1846  0.0458  4.6377 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.135439   0.030306   4.469 8.97e-06 ***\ntreatment   -0.009589   0.041626  -0.230    0.818    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5931 on 813 degrees of freedom\n  (166 observations deleted due to missingness)\nMultiple R-squared:  6.527e-05, Adjusted R-squared:  -0.001165 \nF-statistic: 0.05307 on 1 and 813 DF,  p-value: 0.8179\n\n\nCoefficient for treatment in regression: -0.009589323 \n\n\nDifference in means (Treatment - Control): -0.009589323 \n\n\nThe coefficient from the regression should be equal to the difference in means to demonstrate equivalence.\n\n\n\n\nBlock 2.2\nRandomization inference, although a theoretical concept, is best illustrated with practical examples. A particularly illustrative approach is visualization through tabular permutation. The following online coding resource provides a detailed introduction to this method. In this context, we will work with data from the study “Long-Term effects of the Targeting the Ultra Poor Program” conducted by Abhijit Banerjee, Esther Duflo, and Garima Sharma.\n\n\n$observed_effect\n[1] -0.009589323\n\n$p_value\n[1] 0.8263\n\n\n$observed_effect\n[1] 0.4077528\n\n$p_value\n[1] 8e-04"
  },
  {
    "objectID": "01_03_Chapter3R.html",
    "href": "01_03_Chapter3R.html",
    "title": "Details required for Chapter 3",
    "section": "",
    "text": "Here will be the Chapter 3 excersises and examples."
  },
  {
    "objectID": "01_04_Chapter4R.html#two-way-fixed-effects-models---numerical-example",
    "href": "01_04_Chapter4R.html#two-way-fixed-effects-models---numerical-example",
    "title": "Details required for Chapter 4",
    "section": "Two-Way Fixed Effects Models - Numerical Example",
    "text": "Two-Way Fixed Effects Models - Numerical Example\nIn this sections we show you two numerical examples of the two methodologies seen in section 4.2.2 of the book. The goal is to estimate the treatment effect \\(\\tau\\) in the next “two-way fixed effects” model \\[y_{st} = \\gamma_s + \\lambda_t + \\tau w_{st} + \\varepsilon_{st}\\] Where \\(y_{st}\\) is the outcome variable, \\(\\gamma_s\\) and \\(\\lambda_t\\) are state (unit) and time fixed effects, \\(w_{st}\\) is the binary treatment variable that takes the value of 1 if a state (unit) \\(s\\) is treated at time \\(t\\) and otherwise takes 0. Precisely we focus on Goodman-Bacon (2021) and Chaisemartin and D’Haultfoeuille (2020) methodologies.\nThe results from Goodman-Bacon (2021) and those from Chaisemartin and D’Haultfoeuille (2020) are similar, however they take quite different paths to get there. Goodman-Bacon’s (like that laid out in Athey and Imbens (2022)) is “mechanical” in that it is based on the underlying difference-in-differences comparisons. The result in Chaisemartin and D’Haultfoeuille (2020) is based on a potential outcomes frame-work and a series of assumptions underlying the regression. Thus to examine how these methods work requires somewhat different frameworks. In the case of Goodman-Bacon (2021), we should consider all possible DD comparisons, while in the case of Chaisemartin and D’Haultfoeuille (2020) we should consider each unit’s ATE, which requires knowing the observed and counterfactual state. To examine this in a more applied way, let’s consider a constructed example.\nConsider a panel of 3 states/areas over the 10 years (\\(t\\)) of 2000 to 2009. One of these units is entirely untreated (\\(unit = 1\\) or group \\(U\\)), one is treated at an early time period 2003 (\\(unit = 2\\) or group \\(k\\)), and the other is treated at a later time period 2006 (\\(unit = 3\\) or group \\(l\\))\n\nData <- data.frame(unit = ceiling(1:30/10), year = rep(2000:2009, 3))\n\nWe will consider a simple-case where the actual data-generating process is known as: \\[y_{unit,t} = 2 + 0.2 \\times (t - 2000) + 1 \\times unit + \\beta_1 \\times post \\times unit + \\beta_2 \\times post \\times unit \\times (t - treat)\\] In this mode \\(unit\\) refers to the unit number listed above (1, 2 or 3), \\(post\\) indicates that a unit is receiving treatment in the relevant time period \\(t\\), and \\(treat\\) refers to the treatment period (2003 for unit 2, and 2006 for unit 3).\n\nData$treat <- ifelse(Data$unit == 2, 2006, ifelse(Data$unit == 3, 2003, 0))\nData$time <- ifelse(Data$treat == 0, 0, Data$year - Data$treat)\nData$post <- ifelse(Data$time >= 0 & Data$treat != 0, 1, 0)\n\nThis specification allows for each unit to have its own fixed effect, given that \\(unit\\) is multiplied by 1, and allows for a general time trend increasing by 0.2 units each period across the whole sample. The impact of treatment comes from the units \\(\\beta_1\\) and \\(\\beta_2\\). The first of these, \\(\\beta_1\\), captures an immediate unit-specific jump when treatment is implemented which remains stable over time. The second of these, \\(\\beta_2\\), implies a trend break occurring only for the treated units once treatment comes into place. We will consider 2 cases below. In one case \\(\\beta_1 = 1\\) and \\(\\beta_2 = 0\\) (a simple case with a constant treatment effect per unit)\n\nData$y1 <- 2 + (Data$year - 2000) * 0.2 + 1 * Data$unit + 1 * Data$post * Data$unit + \n  0 * Data$post * Data$unit * (Data$time)\n\nAnd in a second case \\(\\beta_1 = 1\\) and \\(\\beta_2 = 0.45\\) (a more complex case in which there are heterogeneous treatment effects over time).\n\nData$y2 <- 2 + (Data$year - 2000) * 0.2 + 1 * Data$unit + 1 * Data$post * Data$unit +\n  0.45 * Data$post * Data$unit * (Data$time)\n\nThese two cases are plotted next where the line with empty circles refers to group \\(U\\), the line with black filled circles refers to group \\(k\\) and the line with squares refers to group \\(l\\)\n\n\nShow the plot code\nlibrary(ggplot2)\nlibrary(ggpubr)\np1 <- ggplot(data = Data, aes(x = year, y = y1, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5) +\n  geom_point(aes(shape = as.factor(unit)), size = 2) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 12, by = 2),\n                     labels = seq(from = 0, to = 12, by = 2),\n                     limits = c(0,12)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\nt1 <- ggplot() + geom_text(aes(x = 0, y = 0, label = \"(a) Simple Decomposition\")) +\n  theme_void()\np2 <- ggplot(data = Data, aes(x = year, y = y2, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5) +\n  geom_point(aes(shape = as.factor(unit)), size = 2) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 20, by = 5),\n                     labels = seq(from = 0, to = 20, by = 5),\n                     limits = c(0,20)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\nt2 <- ggplot() + geom_text(aes(x = 0, y = 0, label = \"(b) Decomposition with trends\")) +\n  theme_void()\nggarrange(plotlist = list(p1, p2, t1, t2), ncol = 2, nrow = 2, heights = c(0.9, 0.1))\n\n\n\n\n\n\n\n\n\nGoodman-Bacon (2021) Decomposition\nNow we follow the Goodman-Bacon (2021) decomposition for each case:\n- (a) Simple Decomposition\n- (b) Decomposition with trends\nThis methodology basically calculate all \\(2 \\times 2\\) combinations of states and time in order to get (in this example) four specific effects and obtain \\(\\widehat{\\tau}\\) as a weighted mean. The specific effects results are:\n\nA. \\(\\widehat{\\beta}^{2\\times2}_{kU}\\) from the comparison of the early treated unit with the untreated unit.\n\nB. \\(\\widehat{\\beta}^{2\\times2}_{lU}\\), from the comparison of the latter treated unit with the untreated unit.\n\nC. \\(\\widehat{\\beta}^{2\\times2,k}_{kl}\\), from the comparison of the early and latter treated units, when the early unit begin to be treated.\n\nD. \\(\\widehat{\\beta}^{2\\times2,l}_{kl}\\), from the comparison of the early and latter treated units, when the latter unit begin to be treated.\n\nBasically weights in a mean all the possible DD comparisons resulting from the analysis.\n\n(a) Simple Decomposition\nIn this case the Goodman-Bacon (2021) methodology estimate \\(\\widehat{\\tau}\\) weighting the next four DD comparisons\n\n\nShow the plot code\nlibrary(dplyr)\np1 <- ggplot(data = Data, aes(x = year, y = y1, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(1,0.1,1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 12, by = 2),\n                     labels = seq(from = 0, to = 12, by = 2),\n                     limits = c(0,12)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\np2 <- ggplot(data = Data, aes(x = year, y = y1, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(1,1,0.1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 12, by = 2),\n                     labels = seq(from = 0, to = 12, by = 2),\n                     limits = c(0,12)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\np3 <- Data %>% filter(year < 2006) %>%\n  ggplot(aes(x = year, y = y1, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(0.1,1,1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2),\n                     limits = c(2000,2009)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 12, by = 2),\n                     labels = seq(from = 0, to = 12, by = 2),\n                     limits = c(0,12)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\np4 <- Data %>% filter(year >= 2003) %>%\n  ggplot(aes(x = year, y = y1, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(0.1,1,1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2),\n                     limits = c(2000,2009)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 12, by = 2),\n                     labels = seq(from = 0, to = 12, by = 2),\n                     limits = c(0,12)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\nt1 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"A. Early Group v/s Untreated Group\"), size = 3) +\n  theme_void()\nt2 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"B. Later Group v/s Untreated Group\"), size = 3) +\n  theme_void()\nt3 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"C. Early Group v/s Later Group Before 2006\"), \n            size = 3) +\n  theme_void()\nt4 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"D. Early Group v/s Later Group After 2003\"), \n            size = 3) +\n  theme_void()\nggarrange(plotlist = list(t1, t2, p1, p2, t3, t4, p3, p4), ncol = 2, nrow = 4, \n          heights = c(0.1, 0.4, 0.1, 0.4))\n\n\n\n\n\n\n\n\nAs seen in the plots, in the simple decomposition these effects are constants of 3 and 2 for early and later treated units given that the “treatment effect” is simply \\(1 \\times unit\\) in each case.\n\nA. Early Group v/s Untreated Group\nIn order to calculate the effects we start making the simple DD comparison of the untreated group \\(U\\) (\\(unit = 1\\)) with the early treated group \\(k\\) (\\(unit = 3\\)) getting \\(\\widehat{\\beta}^{2 \\times 2}_{kU}\\) as \\[\\widehat{\\beta}^{2 \\times 2}_{kU} = \\left( \\overline{y}_k^{Post(k)} - \\overline{y}_k^{Pre(k)} \\right) - \\left( \\overline{y}_U^{Post(k)} - \\overline{y}_U^{Pre(k)} \\right)\\] Where \\(\\overline{y}_k^{Post(k)}\\) is the mean of the outcome variable for the early treated group \\(k\\) (\\(unit = 3\\)) posterior to treatment, from 2003, \\(\\overline{y}_k^{Pre(k)}\\) is the mean for of the outcome variable for the early treated group \\(U\\) (\\(unit = 3\\)) previous to treatment, until 2002, and \\(\\overline{y}_U^{Post(k)}, \\overline{y}_U^{Post(k)}\\) are the analogous for the untreated group \\(U\\) (\\(unit = 1\\))\n\n(mean(Data$y1[Data$unit == 3 & Data$post == 1]) -\n   mean(Data$y1[Data$unit == 3 & Data$post == 0])) -\n  (mean(Data$y1[Data$unit == 1 & Data$year >= 2003]) -\n     mean(Data$y1[Data$unit == 1 & Data$year < 2003]))\n\n[1] 3\n\n\nThis result also can be obtained from the next linear regression with the canonical DD formula \\[y_{unit,t} = \\alpha_0 + \\alpha_1 \\times Post(k) + \\alpha_2 \\times \\mathbf{1}(unit = 3) + \\beta_{kU}^{2\\times2} \\times Post(k) \\times \\mathbf{1}(unit = 3) + \\varepsilon_i\\] Where \\(Post(k)\\) indicates that the year is equal or greater than the year where the group \\(k\\) (\\(unit = 3\\)) received the treatment (2003) and \\(\\mathbf{1}(unit = 3)\\) indicates if the observation is from the early treated group \\(k\\) (\\(unit = 3\\))\n\nsummary(lm(y1 ~ factor(year >= 2003) + factor(unit) + factor(year >= 2003):factor(unit), \n           data = Data, subset = (unit != 2)))\n\n\nCall:\nlm(formula = y1 ~ factor(year >= 2003) + factor(unit) + factor(year >= \n    2003):factor(unit), data = Data, subset = (unit != 2))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -0.6   -0.2    0.0    0.2    0.6 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                              3.2000     0.2236  14.311 1.54e-10 ***\nfactor(year >= 2003)TRUE                 1.0000     0.2673   3.742  0.00178 ** \nfactor(unit)3                            2.0000     0.3162   6.325 1.01e-05 ***\nfactor(year >= 2003)TRUE:factor(unit)3   3.0000     0.3780   7.937 6.14e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3873 on 16 degrees of freedom\nMultiple R-squared:  0.9804,    Adjusted R-squared:  0.9767 \nF-statistic: 266.1 on 3 and 16 DF,  p-value: 7.35e-14\n\n\nA third way to obtain this is from the next linear regression \\[y_{unit,t} = \\alpha_0 + \\beta_{kU}^{2 \\times 2} \\times Post + \\sum_{i = 2001}^{2009} \\alpha_{i-2000} \\times \\mathbf{1}(year = i) + \\alpha_{10} \\times \\mathbf{1}(unit = 3) + \\varepsilon_i\\] Where in this case \\(Post\\) indicates if the unit is treated (note for group \\(U\\) this will be always 0), \\(\\mathbf{1}(year = i)\\) indicates if the observation is in period \\(i \\in \\{2001, \\ldots, 2009\\}\\) and \\(\\mathbf{1}(unit = 3)\\) keep its meaning\n\nsummary(lm(y1 ~ post + factor(year) + factor(unit), data = Data, subset = (unit != 2)))\n\n\nCall:\nlm(formula = y1 ~ post + factor(year) + factor(unit), data = Data, \n    subset = (unit != 2))\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-2.324e-15 -3.940e-16  0.000e+00  3.940e-16  2.324e-15 \n\nCoefficients:\n                  Estimate Std. Error   t value Pr(>|t|)    \n(Intercept)      3.000e+00  1.217e-15 2.466e+15   <2e-16 ***\npost             3.000e+00  1.454e-15 2.063e+15   <2e-16 ***\nfactor(year)2001 2.000e-01  1.490e-15 1.342e+14   <2e-16 ***\nfactor(year)2002 4.000e-01  1.490e-15 2.684e+14   <2e-16 ***\nfactor(year)2003 6.000e-01  1.658e-15 3.619e+14   <2e-16 ***\nfactor(year)2004 8.000e-01  1.658e-15 4.825e+14   <2e-16 ***\nfactor(year)2005 1.000e+00  1.658e-15 6.031e+14   <2e-16 ***\nfactor(year)2006 1.200e+00  1.658e-15 7.238e+14   <2e-16 ***\nfactor(year)2007 1.400e+00  1.658e-15 8.444e+14   <2e-16 ***\nfactor(year)2008 1.600e+00  1.658e-15 9.650e+14   <2e-16 ***\nfactor(year)2009 1.800e+00  1.658e-15 1.086e+15   <2e-16 ***\nfactor(unit)3    2.000e+00  1.217e-15 1.644e+15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.49e-15 on 8 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 5.002e+30 on 11 and 8 DF,  p-value: < 2.2e-16\n\n\nNow we store this result for posterior use\n\nbku <- lm(y1 ~ post + factor(year) + factor(unit), data = Data,\n          subset = (unit != 2))$coefficient[\"post\"]\n\n\n\nB. Later Group v/s Untreated Group\nThe next DD comparison we get is for the untreated group \\(U\\) (\\(unit = 1\\)) with the later treated group \\(l\\) (\\(unit = 2\\)), getting \\(\\widehat{\\beta}^{2 \\times 2}_{lU}\\)\n\nblu <- lm(y1 ~ post + factor(year) + factor(unit), data = Data, \n   subset = (unit != 3))$coefficient[\"post\"]\nblu\n\npost \n   2 \n\n(mean(Data$y1[Data$unit == 2 & Data$post == 1]) -\n   mean(Data$y1[Data$unit == 2 & Data$post == 0])) -\n  (mean(Data$y1[Data$unit == 1 & Data$year >= 2006]) -\n     mean(Data$y1[Data$unit == 1 & Data$year < 2006]))\n\n[1] 2\n\nsummary(lm(y1 ~ factor(year >= 2006) + factor(unit) + factor(year >= 2006):factor(unit), \n           data = Data, subset = (unit != 3)))\n\n\nCall:\nlm(formula = y1 ~ factor(year >= 2006) + factor(unit) + factor(year >= \n    2006):factor(unit), data = Data, subset = (unit != 3))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -0.5   -0.3    0.0    0.3    0.5 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                              3.5000     0.1369  25.560 2.12e-14 ***\nfactor(year >= 2006)TRUE                 1.0000     0.2165   4.619 0.000285 ***\nfactor(unit)2                            1.0000     0.1936   5.164 9.42e-05 ***\nfactor(year >= 2006)TRUE:factor(unit)2   2.0000     0.3062   6.532 6.91e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3354 on 16 degrees of freedom\nMultiple R-squared:  0.9571,    Adjusted R-squared:  0.9491 \nF-statistic: 119.1 on 3 and 16 DF,  p-value: 3.726e-11\n\n\n\n\nC. Early Group v/s Later Group Before 2006\nNext we calculate the effects from the DD comparisons of early and later treated groups, starting for the period previous to 2006 \\[\\widehat{\\beta}^{2 \\times 2, k}_{kl} \\equiv \\left( \\overline{y}^{Mid(k,l)}_{k} - \\overline{y}^{Pre(k)}_{k} \\right) - \\left( \\overline{y}^{Mid(k,l)}_{l} - \\overline{y}^{Pre(k)}_{l} \\right)\\] Where \\(\\overline{y}^{Mid(k,l)}_{k}\\) is the mean of the outcome variable for the early treated group \\(k\\) (\\(unit = 3\\)) in the period between the treatment for the group \\(k\\) and the group \\(l\\) (\\(unit = 2\\)), from 2003 to 2005, \\(\\overline{y}^{Pre(k)}_{k}\\) is the mean for of the outcome variable for the early treated group \\(k\\) (\\(unit = 3\\)) previous to treatment, until 2002, and \\(\\overline{y}^{Mid(k,l)}_{l}, \\overline{y}^{Pre(k)}_{l}\\) are the analogous for the later treated group \\(l\\) (\\(unit = 2\\))\n\nbklk <- lm(y1 ~ post + factor(year) + factor(unit), data = Data,\n   subset = (unit != 1 & year < 2006))$coefficient[\"post\"]\nbklk\n\npost \n   3 \n\n(mean(Data$y1[Data$unit == 3 & (Data$year >= 2003 & Data$year < 2006)]) -\n   mean(Data$y1[Data$unit == 3 & Data$year < 2003])) -\n  (mean(Data$y1[Data$unit == 2 & (Data$year >= 2003 & Data$year < 2006)]) -\n     mean(Data$y1[Data$unit == 2 & Data$year < 2003]))\n\n[1] 3\n\nsummary(lm(y1 ~ factor(year >= 2003) + factor(unit) + factor(year >= 2003):factor(unit), \n           data = Data, subset = (unit != 1 & year < 2006)))\n\n\nCall:\nlm(formula = y1 ~ factor(year >= 2003) + factor(unit) + factor(year >= \n    2003):factor(unit), data = Data, subset = (unit != 1 & year < \n    2006))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -0.2   -0.2    0.0    0.2    0.2 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                              4.2000     0.1155  36.373 3.58e-10 ***\nfactor(year >= 2003)TRUE                 0.6000     0.1633   3.674 0.006271 ** \nfactor(unit)3                            1.0000     0.1633   6.124 0.000282 ***\nfactor(year >= 2003)TRUE:factor(unit)3   3.0000     0.2309  12.990 1.17e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2 on 8 degrees of freedom\nMultiple R-squared:  0.9918,    Adjusted R-squared:  0.9887 \nF-statistic: 322.7 on 3 and 8 DF,  p-value: 1.106e-08\n\n\n\n\nD. Early Group v/s Later Group After 2003\nThe last DD comparison is for early and later treated groups, starting from 2006 \\[\\widehat{\\beta}^{2 \\times 2, l}_{kl} \\equiv \\left( \\overline{y}^{Post(l)}_{l} - \\overline{y}^{Mid(k,l)}_{l} \\right) - \\left( \\overline{y}^{Post(l)}_{k} - \\overline{y}^{Mid(k,l)}_{k} \\right)\\] Where \\(\\overline{y}^{Post(l)}_{l}\\) is the mean of the outcome variable for the later treated group \\(l\\) (\\(unit = 2\\)) in the period after this group received the treatment, from 2006, \\(\\overline{y}^{Mid(k,l)}_{l}\\) is the mean for of the outcome variable for the later treated group \\(l\\) (\\(unit = 2\\)) in the period between the treatment for the group \\(k\\) (\\(unit = 3\\)) and the group \\(l\\), from 2003 to 2005, and \\(\\overline{y}^{Post(l)}_{k}, \\overline{y}^{Mid(k,l)}_{k}\\) are the analogous for the early treated group \\(k\\) (\\(unit = 3\\))\n\nbkll <- lm(y1 ~ post + factor(year) + factor(unit), data = Data,\n   subset = (unit != 1 & year > 2002))$coefficient[\"post\"]\nbkll\n\npost \n   2 \n\n(mean(Data$y1[Data$unit == 2 & Data$year > 2005]) -\n   mean(Data$y1[Data$unit == 2 & (Data$year >= 2003 & Data$year < 2006)])) -\n  (mean(Data$y1[Data$unit == 3 & Data$year > 2005]) -\n     mean(Data$y1[Data$unit == 3 & (Data$year >= 2003 & Data$year < 2006)]))\n\n[1] 2\n\nsummary(lm(y1 ~ factor(year >= 2006) + factor(unit) + factor(year >= 2006):factor(unit), \n           data = Data, subset = (unit != 1 & year > 2002)))\n\n\nCall:\nlm(formula = y1 ~ factor(year >= 2006) + factor(unit) + factor(year >= \n    2006):factor(unit), data = Data, subset = (unit != 1 & year > \n    2002))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-0.300 -0.175  0.000  0.175  0.300 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                              4.8000     0.1366  35.132 8.28e-12 ***\nfactor(year >= 2006)TRUE                 2.7000     0.1807  14.939 3.64e-08 ***\nfactor(unit)3                            4.0000     0.1932  20.702 1.53e-09 ***\nfactor(year >= 2006)TRUE:factor(unit)3  -2.0000     0.2556  -7.825 1.43e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2366 on 10 degrees of freedom\nMultiple R-squared:  0.9868,    Adjusted R-squared:  0.9829 \nF-statistic: 249.5 on 3 and 10 DF,  p-value: 1.073e-09\n\n\nThis result in the surprising behaviour flagged by Goodman-Bacon (2021) where despite each unit specific treatment effect being positive, the parameter \\(\\widehat{\\beta}^{2 \\times 2, l}_{kl}\\) is actually negative given that it compares the change from the later-adopting unit (\\(unit = 2\\)) with the unchanging portion of the earlier-adopting unit (\\(unit = 3\\)), where the treatment effect for unit 3 grows more over time than that of unit 2.\n\n\nWeights\nWe remember that the OLS estimate of this two-way fixed effect estimate is a weighted mean of the previous estimates \\[\\widehat{\\tau} = W_{kU} \\cdot \\widehat{\\beta}^{2\\times 2}_{kU} + W_{lU} \\cdot \\widehat{\\beta}^{2\\times 2}_{lU} + W_{kl}^{k} \\cdot \\widehat{\\beta}^{2\\times 2,k}_{kl} + W_{kl}^{l} \\cdot \\widehat{\\beta}^{2\\times 2,l}_{kl}\\] Where each \\(W\\) is the weight that the respective \\(\\beta\\) has in this weighted mean and they are \\[\\begin{align*}\nW_{kU} & = \\frac{(n_k + n_U)^2\\widehat{V}^D_{kU}}{\\widehat{V}^D} \\quad & , \\quad W_{lU} & = \\frac{(n_l + n_U)^2\\widehat{V}^D_{lU}}{\\widehat{V}^D} \\\\\nW_{kl}^k & = \\frac{[(n_k + n_l)(1 - \\overline{D}_l)]^2\\widehat{V}^{D,k}_{kl}}{\\widehat{V}^D} \\quad & , \\quad W_{kl}^l & = \\frac{[(n_k + n_l)(1 - \\overline{D}_k)]^2\\widehat{V}^{D,l}_{kl}}{\\widehat{V}^D}\n\\end{align*}\\] Where \\(n\\) refers to the sample share of the group\n\nnk = 1/3\nnl = 1/3\nnu = 1/3\n\n\\(\\overline{D}\\) referes to the share of time the group is treated\n\nDk = mean(Data$post[Data$unit==3])\nDl = mean(Data$post[Data$unit==2])\n\n\\(\\widehat{V}\\) refers to how much treatment varies\n\nVkU = 0.5*0.5*(Dk)*(1-Dk)\nVlU = 0.5*0.5*(Dl)*(1-Dl) \nVklk = 0.5*0.5*((Dk-Dl)/(1-Dl))*((1-Dk)/(1-Dl))\nVkll = 0.5*0.5*(Dl/Dk)*((Dk-Dl)/(Dk))\nVD   = sum(lm(post ~ factor(unit) + factor(year), \n              data = Data)$residuals^2)/30\n\nThe weights are the following\n\nwkU = ((nk + nu)^2*VkU)/VD\nwkU\n\n[1] 0.3181818\n\nwlU = ((nl + nu)^2*VlU)/VD\nwlU\n\n[1] 0.3636364\n\nwklk = (((nk + nl)*(1-Dl))^2*Vklk)/VD\nwklk\n\n[1] 0.1363636\n\nwkll = (((nk + nl)*Dk)^2*Vkll)/VD\nwkll\n\n[1] 0.1818182\n\n\nWith this in mind the \\(\\tau\\) estimate is\n\ntau = wkU * bku + wlU * blu + wklk * bklk + wkll * bkll\ntau\n\n    post \n2.454545 \n\n\n\n\n\n(b) Decomposition with trends\nIn this case the Goodman-Bacon (2021) methodology estimate \\(\\widehat{\\tau}\\) weighting the next four DD comparisons\n\n\nShow the plot code\nlibrary(dplyr)\np1 <- ggplot(data = Data, aes(x = year, y = y2, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(1,0.1,1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 20, by = 5),\n                     labels = seq(from = 0, to = 20, by = 5),\n                     limits = c(0,20)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\np2 <- ggplot(data = Data, aes(x = year, y = y2, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(1,1,0.1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 20, by = 5),\n                     labels = seq(from = 0, to = 20, by = 5),\n                     limits = c(0,20)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\np3 <- Data %>% filter(year < 2006) %>%\n  ggplot(aes(x = year, y = y2, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(0.1,1,1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2),\n                     limits = c(2000,2009)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 20, by = 5),\n                     labels = seq(from = 0, to = 20, by = 5),\n                     limits = c(0,20)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\np4 <- Data %>% filter(year >= 2003) %>%\n  ggplot(aes(x = year, y = y2, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(0.1,1,1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2),\n                     limits = c(2000,2009)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 20, by = 5),\n                     labels = seq(from = 0, to = 20, by = 5),\n                     limits = c(0,20)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\nt1 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"A. Early Group v/s Untreated Group\"), size = 3) +\n  theme_void()\nt2 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"B. Later Group v/s Untreated Group\"), size = 3) +\n  theme_void()\nt3 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"C. Early Group v/s Later Group Before 2006\"), \n            size = 3) +\n  theme_void()\nt4 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"D. Early Group v/s Later Group After 2003\"), \n            size = 3) +\n  theme_void()\nggarrange(plotlist = list(t1, t2, p1, p2, t3, t4, p3, p4), ncol = 2, nrow = 4, \n          heights = c(0.1, 0.4, 0.1, 0.4))\n\n\n\n\n\n\n\n\nAs seen in the plots, in the decomposition with trends these effects are no longer constants of 3 and 2 for early and later treated units given that the “treatment effect” is no longer simply \\(1 \\times unit\\) in each case.\n\n# 2X2 DD Regressions\nA <- lm(y2 ~ post + factor(year) + factor(unit), data = Data, subset=(unit!=2))\nB <- lm(y2 ~ post + factor(year) + factor(unit), data = Data, subset=(unit!=3))\nC <- lm(y2 ~ post + factor(year) + factor(unit), data = Data, subset=(unit!=1 & year<2006))\nD <- lm(y2 ~ post + factor(year) + factor(unit), data = Data, subset=(unit!=1 & year>2002))\n# 2x2 Betas\nbkUk <- A$coefficient[\"post\"]\nbkUl <- B$coefficient[\"post\"]\nbklk <- C$coefficient[\"post\"]\nbkll <- D$coefficient[\"post\"]\n# Share of time treated\nDk = mean(Data$post[Data$unit==3])\nDl = mean(Data$post[Data$unit==2])\n# How much treatment varies\nVkUk = 0.5*0.5*(Dk)*(1-Dk)\nVkUl = 0.5*0.5*(Dl)*(1-Dl) \nVklk = 0.5*0.5*((Dk-Dl)/(1-Dl))*((1-Dk)/(1-Dl))\nVkll = 0.5*0.5*(Dl/Dk)*((Dk-Dl)/(Dk))\nVD <- sum(lm(post ~ factor(unit) + factor(year), data = Data)$residuals^2/30)\n# Share of sample\nnk   = 1/3\nnl   = 1/3\nnu   = 1/3\n# Weights\nwkUk = ((nk + nu)^2*VkUk)/VD\nwkUl = ((nl + nu)^2*VkUl)/VD\nwklk = (((nk + nl)*(1-Dl))^2*Vklk)/VD\nwkll = (((nk + nl)*Dk)^2*Vkll)/VD\n# Tau\ntau = bkUk*wkUk + bkUl*wkUl + bklk*wklk + bkll*wkll\ntau\n\n    post \n3.804545 \n\n\n\n\n\n\nAthey, Susan, and Guido W. Imbens. 2022. “Design-based analysis in Difference-In-Differences settings with staggered adoption.” Journal of Econometrics 226 (1): 62–79. https://doi.org/https://doi.org/10.1016/j.jeconom.2020.10.012.\n\n\nChaisemartin, Clément de, and Xavier D’Haultfoeuille. 2020. “Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects.” American Economic Review 110 (9): 2964–96. https://doi.org/10.1257/aer.20181169.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with Variation in Treatment Timing.” Journal of Econometrics 225 (2): 254–77. https://doi.org/https://doi.org/10.1016/j.jeconom.2021.03.014."
  },
  {
    "objectID": "02_00_FollowingStata.html#introduction",
    "href": "02_00_FollowingStata.html#introduction",
    "title": "Following Along with Stata",
    "section": "Introduction",
    "text": "Introduction\nIn this section we will provide a primer in Stata, which seeks to provide you with the necessary tools to explore the microeconometric methods introduced throughout the book. “Stata is a complete, integrated software package that provides all your data science needs—data manipulation, visualization, statistics, and automated reporting” as is defined in its website. Stata was first developed in the mid 1980s, and is now up to release 18. Stata is particularly strong at data manipulation and statistical modelling, and along with their own developers, has an active community of researchers who contribute user-written packages. It also has an active community providing online support on forums such as Stata List. It provides you with the large majority of tools you will need throughout this book, as well as the programming environment necessary to extend available tools where required. Stata releases a new version of the program every couple of years, and a strength of Stata is that it guarantees backwards compatability. Thus, code written in older versions of Stata can be run seamlessly in future versions. However, code written in newer versions of Stata will not necessarily work in older versions of Stata, because in each version of Stata new tools are added.\nThe goal of this site is not to provide you with a comprehensive introduction to the software, but instead we it seeks to provide you with an overview of the basic tools to understand the required tools that we will use to get up and running in this book. In the first section we will focus on a brief rundown of some principal elements of Stata without yet getting into the empirical methods discussed in the textbook. Thereafter, we will focus on causal econometric methods, but in each section will also introduce any further tools required to complete key analyses or end-of-chapter questions. The goal of this resource is that after following along with these sections you will be sufficiently well-versed in Stata that you will comfortably be able to work with real data and econometric implementations. Nevertheless, below we point you to further resources if you are seeking a comprehensive overview of Stata as a software."
  },
  {
    "objectID": "02_00_FollowingStata.html#installing-and-working-with-stata",
    "href": "02_00_FollowingStata.html#installing-and-working-with-stata",
    "title": "Following Along with Stata",
    "section": "Installing and Working with Stata",
    "text": "Installing and Working with Stata\nInformation on how to order and get Stata is available on the Stata website, and further support can be found searching the web. When installing Stata, the language comes with its own Graphical User Interface (GUI) which is quite user friendly. This GUI provides a command line to interact with Stata and an output window, as well as a list of commands recently executed. It also has a number of emerging windows including a built in file editor, and a graphics window. An example of what Stata looks like is provided below, where you can see that a code editor is visible, as well as a screen below where one can interact directly with Stata.\n\n\n\nStata\n\n\n\nFurther Resources\nFrom here we will move to a first tutorial about Stata as a language, and an overview of a number of key elements. If you are interested in generating a more complete overview of Stata, I would highly recommend the list of resources provided on the Stata webpage. At this link they include things such as books and blogs on Stata, but also provide a number of video tutorials and online resources specifically related to learning Stata."
  },
  {
    "objectID": "02_01_IntroductionStata.html",
    "href": "02_01_IntroductionStata.html",
    "title": "An Introduction to Stata",
    "section": "",
    "text": "Preliminaries\nWe will assume that when you are reading this you have access to some version of Stata. Likely, the way you will interact with Stata will be with Stata’s native GUI which provides you with a number of windows including a panel showing the history of commands, an area displaying any variables you have loaded, a place to type commands, a number of emerging windows which we will discuss below, and most importantly, a “shell”, or Stat interface, which looks like the following:\nStata comes in versions ranging from 1 to 18, with later versions being assigned higher numbers. Later versions of Stata can run any code written in earlier versions of Stata, though at times new commands are introduced, which may be available in more recent versions of Stata, but not in earlier versions. In the below we will generally work in a way which will work with any recent versions of Stata (for example from version 12 onwards, given that versions prior to Stata 12 are not common), though we will note in a few cases where commands may be specific to only certain versions of Stata and later.\nIf you are in the Stata GUI, Stata code can be entered directly in the command line which is generally located at the bottom of the window. The most simple way to execute code in Stata is to simply input commands at the command like. So, for example, you could write the following in the command line and press enter, and Stata will evaluate this code:\nAs you can see, Stata has understood the instruction that you want to add 4 plus 4 and show the result, which is evaluated, and output directly in the Stata console above. Later in this chapter we will properly explain these mathematical operations and the different Stata data types. You will note that unlike in certain other languages, here we have not simply input a mathematical operation to be evaluated. This is because Stata requires every interaction to be based on a valid command, which in this case happens to be the display command. So if, for example, we were instead to type:\nwe see that the answer is not returned to us, but rather some message suggesting that we have done something invalid (we will discuss such messages at more length below). Stata has a large range of commands which can be invoked at the command line. We will become familiar with a considerable number of these throughout this introduction. As another very simple example we can type pwd for “print working directory”:\nThis command show us the path on our computer where Stata is currently working. Thus, any files saved will be exported here, and Stata will search for any thing to import from this directory. If you wish to change this directory you must use the function cd(\"Path/to/the/directory/you/want/to/use\") with the path written between \" \" in order for Stata understand that is a character, again a topic that will be introduced more deeply later. It is important to note that paths should be separated with the slash character: / rather than the backlash \\, as this will work on any operating system. Indeed, if you try to set directories with a backslash, on certain operating systems you will see that Stata returns an error. This may not happen on your operating system, but it is good practice to separate paths by “/” as it ensures code can be run seamlessly on other systems.\nWe will return to discuss the precise nature of commands in more detail a bit later on. For now you will note that we have simply been typing instructions to Stata directly into the command line, and Stata has been providing us with output. Generally, we will not work this way in an empirical research project, as we will want to develop code over time, and be able to return to Stata and replicate code in the future. For this reason, generally we will work in do files, or files named as something.do (where something will be logical name for the file). We will use these do files to store the commands which we require for particular processes or routines, and can then save them to the disk of our computer before running them in Stata. These scripts can be generated in any type of text editor, and then can be run in Stata using the do command. For example, below, you can see a screenshot of a session on Stata where we have a basic Stata do file named firstStataScript.do which we are editing in Stata’s native do file editor (which can be accessed by typing doedit, and behind this, in the Stata console we see that the script is run using do and output is provided.\nYou may notice something strange about the code and the output in the Stata console. The Stata do file has 7 distinct lines of text, but the console only executes three lines of code (the lines in Stata which do not begin with a “.”). The reason for this is because we have included comments, or messages for human eyes, but not for the computer to interpret. Any time that // is included in code, Stata understands that this is a comment, and hence is ignored in executing the code. So, for example, when we write:\nin our code, the Stata console correctly echoes Hello World! back at us. However, if we enter precisely the same code, but begin the line with a comment symbol, Stata will not do anything given that it interprets everything to the right of // as plain text which should be ignored. Similarly, the “*” symbol used at the beginning of a line also serves to act as a comment character. You will note that on line 9 of the code displayed in the do file editor, we are also perfectly able to use comments within a line in do files, and the line will only be executed up to the point which the comment is reached. It is good practice to comment code extensively, both for sharing code with other users, but also for sharing code with yourself in the future, where you may not remember precisely what you were thinking when you originally wrote your code! As a final note on comments, you can also write comments which span multiple lines if these comments are contained within the characters: /* which opens the commend and */ which closes the comment. You may wish to confirm this yourself by opening a do file and checking that you can write a multi-line comment in this way.\nUntil now we have executed some code in Stata and seem some specific commands which print output, but we have never stored any results for later use within our code. Generally we will want to store the output of intermediate steps in our code for later use. We will show a number of ways to do this below.\nIn general, when we work with Stata we will work with variables, which are stored by Stata as a dataset. We can assign the values or characters to variables, provided that we first ensure that Stata has a defined number of observations. For example:\nIf we already have data in memory, the first line of code set obs 10 will not be necessary, as we will already have a defined number of observations. However in the second line where we generate a variable named x1 and define its value as 1 for each observation, this will be generated for the number of observations in our entire dataset. We will discuss this structure shortly, as well as alternative ways to store information, when we lay out Stata’s data types below.\nIn the above code we have seen that Stata has used the command generate and then uses the equals symbol to assign a value to the variable generated. If we fail to remember to type generate, or make some other error in our assignation, Stata will inform us about our error by printing out an error message:\nThese error messages include a brief explanation of what has gone wrong, to help you diagnose and fix your code, as well as an error code (in this case r(199)), which may allow you to find further information related to the error. All error codes can be found in Stata’s manuals. When an error occurs, anything which you have asked of your code will not be performed, and so in this particular case you will not see any variable named x1 in your Stata memory.\nTo end this initial introduction, we will briefly explore the generation of basic plots in Stata. There are a variety of plots that you can create in Stata, and each plot has its own precise syntax. The range of plots which can describe relationships between two (or more) variables can be seen by consulting help twoway and I would encourage to scan the plot types, or explore online to see a graphical overview. In this subsection we are going to show you two basic plots: (i) the line plot and (ii) the scatter plot.\nIn Stata the command to do generate a line plot is, perhaps unsurprisingly, the line command. This function maps pairs of points in the \\((x,y)\\) coordinate axis and places them on a graph. For example, let’s consider a case where \\(x\\) is a sequence of integers between -5 and 5. This can be generated as below:\nIn this code first we clear the Stata memory, then we set the observations in the dataset to 11, so that Stata understands that there is a dataset, empty for the moment, which has 11 observations (rows). Finally we create a variable named x which will take the value of -6 and the row number (_n) in the dataset. In this way, in row 1, where _n=1, x will equal -5, and will successively increase by one unit down rows:\nNext we create \\(y = x^2\\) as follows\nNow with line function we create the line plot. line, by default generates a line plot of points, making reasonable decisions for axes and labelling:\nIn the help documentation you can see a series of option that allows to modify the graph aspect.\nNow we show you how to graph a scatter plot, for that we are going to simulate with rnormal() 100 pseudo-random numbers from a standard normal distribution in a variable error, then in a variable x we simulate another 100 pseudo-random numbers from a standard normal distribution and finally we create \\[y_i = 10 + 3*x_i + error_i\\]\nIf you access help documentation for scatter you can see how add to elements to the graph and further edit the graphs appearance, for example by adding axis titles, adding lines, and so forth.\nIt is also relatively easy to include multiple graphs, laying these on a single graph window. Graphs can essentially be concatenated together, separating with “||” as follows, where we add a second graph, lfitci which fits a regression line along with 95% confidence intervals:\nWe will take forward many of the examples laid out on this page in the chapters ahead. For now, this introductory session aims to provide you with a sufficient overview to get up and running in Stata, and apply your knowledge to microeconometric methods introduced in the book."
  },
  {
    "objectID": "02_01_IntroductionStata.html#data-types",
    "href": "02_01_IntroductionStata.html#data-types",
    "title": "An Introduction to Stata",
    "section": "Data Types",
    "text": "Data Types\n\nData Stored as Variables\nThe logic of Stata is generally based on the idea that we will have a principal dataset which will allow us to store related variables. This is similar to the idea of data frames in other languages, and makes sense for a lot of our econometric work where we will often wish to store information on N individuals in rows, and K variables in columns. By default, when we generate variables these will be stored in a single dataset which we can browse in Stata. As we have seen above, we can generate this dataset “by hand” by first indicating the number of observations (rows) available, and then simply provide names for any newly added variables. As we will see below, in practice we will generally actually import an existing dataset, and then generate any variables to add to this data. Traditionally, Stata obliges us to work with a single dataset in memory, and so at times we need to think carefully about how to combine datasets if we are working with multiple pieces of information at once. However, as we will see shortly, frmo version 16 of Stata onwards, we are now able to hold multiple datasets in memory simultaneously, potentially making it more simple to work with varying types of information.\n\nText (String) Data\nTo get up and running with Stata, it is useful to understand the different ways which data can be stored. Stata can hold a number of different types of information in the variables we generate, and it is important that Stata understands what type of information we are passing or storing. Among others, Stata allows for variables to hold text (strings) or numeric data types. Text is perhaps the conceptually simplest type of data. Text is simply saved as strings of characters, and should be specified between \" \". We have seen the use of characters when displaying “Hello world” previously. If characters are not indicated between strings, Stata will understand that they refer to variable names. For example, returning to Hello World, if we seek to generate a variable which stores this text, if this is entered without quotes, an error will appear:\ngenerate mivar = Hello World\nHello not found\nr(111);\nIndeed, if we look carefully at the error message we may see that the error has occurred when Stata tried to evaulate Hello. As we did not enclose Hello in double quote, Stata seeks to find some internal information such as a variable or command called Hello. When it is unable to find tis, an error is produced. However, if we had correctly enclosed “Hello World” in double quotes, we would see that this would be stored as a variable, which we are naming mivar:\n\ngenerate mivar = \"Hello World\"\n\nWe can confirm that this variable exists and is effectively understood to be string data by using the describe command. This command describes to us the variable which we request to describe, or if entered with no names, simply describes all the data held in the memory. Below we see that among other things, the storage type of this variable is denoted as str11, a string variable with 11 characters.\n\ndescribe mivar\n\n              storage   display    value\nvariable name   type    format     label      variable label\n-------------------------------------------------------------------------------\nmivar           str11   %11s                  \n\n\nNumeric data is entered without any special behaviour, simply writing the number in the case of numerica values. Indeed, if numeric data is accidentally enclosed between quotes, it will be treated as character, and not numeric data. For example, below we assign two variables based on the number 2 and confirm that one is numeric while the other is viewed as a character.\n\ngenerate num1=2\ngenerate num2 = \"2\" \ndescribe num1 num2\n\n              storage   display    value\nvariable name   type    format     label      variable label\n-------------------------------------------------------------------------------\nnum1            float   %9.0g                 \nnum2            str1    %9s                   \n\n\nIf you try to perform numerical operations based on the above variables (more on this below), you will see that num1 can be involved in such calculations, while num2 cannot.\n\n\n\nWorking with frames (\\(\\ge\\) Stata 16)\nOne of the few points we will see here which is Stata version specific is the use of frames. From Stata version 16 and beyond, the ability to work with multiple datasets simultaneously in a single session was introduced. For example, given that we have generated a number of variables in our session, we can see that there is a current data frame named default.\n\nframe\n\nIf we would like to start working with multiple frames we may wish to rename this frame for simplicity using frame rename oldname newname\n\nframe rename default my10obs\n\nNow we will create a new data frame with frame create name and change to that data frame with cwf name, where cwf refers to “Change working frame”:\n\nframe create secondframe\ncwf secondframe\ndescribe\n\nContains data\n  obs:             0                          \n vars:             0                          \nSorted by: \n\n\nAs you can see when we use the describe command, our new frame contains none of the information which we have previously generated in our session. However, if we request that Stata , but if you use\n\nframes dir\n\n* my10obs      10 x 0\n  secondframe  0 x 0\n\nNote: frames marked with * contain unsaved data\n\n\nYou will see how there is two data frames and the one named my10obs contains our previously generated data. We can now simply use cwf to toggle between data frames should we wish to work in two separate data environments.\n\n\nOther Types of Data Stored by Stata\nAlthough the most common way to work in Stata will be within a dataset or data frame, Stata provides a number of additional ways to store data including as scalars and matrices. Scalars are objects that store a single value, which can be either a string or a number. For example, we can define scalars as below, noting the use of the scalar command.\n\nscalar x1 = \"Hello World\"\ndisplay x1\nscalar x2 = 4 + 4\ndisplay x2\n\nHello World\n\n\n8\n\n\nThis code succesfully store the string Hello World in the scalar named x1 and 8 (as the result of 4 plus 4) in the scalar x2. This scalars can also be used in future operations\n\nscalar x3 = x2 + 4\ndisplay x3\n\n12\n\n\nYou may note that here we have generated a scalar named as x1, and previously we had generated a variable named x1 in our Stata dataset. There is no formal reason why you cannot name scalars in the same way as you name variables, as Stata will view them as different objects. Of course, you likely will wish to avoid the confusion of having variables and scalars named in the same way, and it is good practice to use more descriptive names when generating variables and scalars. However, it is worth noting one difference in working with scalars and variables. If we wish to generate a variable which is already generate, Stata will not allow us to, resulting in an error indicating a variable with this name is already defined. However, if we wish to replace a scalar with a new value, Stata will issue no error, simply destroying the old scalar, and creating a new one. For example, below (noting that the variable x1 and the scalar x1 have previously been defined), we can see this differing behaviour:\nqui set obs 10\ngen x1=1\ngen x1=2\nscalar x1=2\nvariable x1 already defined\nr(110);\nIf we do actually wish to save over our variable, we need to explicitly indicate this by using the replace command instead of the generate command.\n\nreplace x1=2\n\n(10 real changes made)\n\n\nFinally, we can also work with numerical matrices which can only store numbers and which work in the fashion of standard algebraic matrices, by using the group of matrix commands. To see this, consider the following matrices, which are input using matrix input, and which we name as A and B:\n\nmatrix input A = (1, 2 \\ 3, 4)\nmatrix input B = (5, 6 \\ 7, 8)\n\nStata’s matrix notation is to separate columns by commas, and rows with the backslash. We can see the result of this matrix input using the matrix list command\n\nmatrix list A\nmatrix list B\n\nA[2,2]\n    c1  c2\nr1   1   2\nr2   3   4\n\n\nB[2,2]\n    c1  c2\nr1   5   6\nr2   7   8\n\n\nThis matrices are stored and can then be operated with to conduct permitted matrix operations, as seen below.\n\nmatrix define C = A * B\nmatrix list C\n\nC[2,2]\n    c1  c2\nr1  19  22\nr2  43  50\n\n\nA list of matrix operations can be found in Stata’s manuals."
  },
  {
    "objectID": "02_01_IntroductionStata.html#basic-operations",
    "href": "02_01_IntroductionStata.html#basic-operations",
    "title": "An Introduction to Stata",
    "section": "Basic Operations",
    "text": "Basic Operations\nThe standard mathematical operations in Stata likely work how you would expect. Basic mathematical operations are as follows, with an example of their use below:\n\n\n\nSymbol\nOperation\n\n\n\n\n+\nAddition\n\n\n-\nSubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n^\nPower\n\n\n\nA few examples\n\ndisplay 10 + 3\ndisplay 10 - 3\ndisplay 10 * 3\ndisplay 10 / 3\ndisplay 10 ^ 3\n\n13\n\n7\n\n30\n\n3.3333333\n\n1000\n\n\nAlso when you combine different operators it respects the PEMDAS order, first resolving Parentheses, second solve Exponents, third Multiplication and Division, fourth Addition and Substraction. Both multiplication and division, and addition and substraction are evaluated left to right, rather than sequentially by operation, so it is important to indicate to Stata explicitly what you need using parentheses in certain cases. For example in order to solve \\[-\\frac{5 + 3^{5-3}}{5\\times 3}\\] You should use the code\n\ndisplay -(5+3^(5-3))/(5*3)\n\n-.93333333\n\n\nIn general, these operations are assumed to refer to variables or scalars. In the case of matrices, addition, substraction multiplication and division are also possible with the same symbols, where multiplication will result in matrix multiplication (and not element-wise multiplication), and division assumes the division of each element of the matrix by a single scalar. Matrix transpose can also be performed simply using '. If, however, you wish to perform more complicated matrix calculations such as element-wise operations, it may be worth using Stata’s own matrix processing language, which is known as Mata. In this introduction we will not explore Mata, however it is worth noting its existence, particularly if you wish to work with demanding numerical tasks.\nCertain mathematical operators can also work with strings, specifically + and *. Stata automatically identifies when you use + if you are adding numerical values or concatenating strings, in this case you must use the same data type. When you use * to operate with strings you must specify a string and a number, because Stata will repeat the string as many times as the number indicates:\n\nscalar x1 = \"Hello \" + \"World\"\nscalar x2 = 2 + 2\nscalar x3 = \"Hello\" * 2\ndisplay x1\ndisplay x2\ndisplay x3\n\nHello World\n\n4\n\nHelloHello\n\n\nStata also has relational and logical operators that returns a numerical value associated with the truth value of an expression (0 for False and 1 for True) pointing if the statement is true or false. This operators are:\n\n\n\nSymbol\nComparison\n\n\n\n\n==\nEquals\n\n\n!=\nDifference\n\n\n>\nGreater\n\n\n>=\nGreater or Equal\n\n\n<\nLess\n\n\n<=\nLess or Equal\n\n\n|\nOr\n\n\n&\nAnd\n\n\n! or ~\nNegation\n\n\n\nA few examples\n\ndisplay 4 == 3\ndisplay 4 != 3\ndisplay 4 > 3\ndisplay 4 >= 3\ndisplay 4 < 3\ndisplay 4 <= 3\ndisplay (4 == 3) | (4 != 3)\ndisplay (4 == 3) & (4 != 3)\ndisplay ~(4 == 3)\n\n0\n\n1\n\n1\n\n1\n\n0\n\n0\n\n1\n\n0\n\n1\n\n\nWe will return to these logical operators below when we discuss the use of conditions in manipulating variables."
  },
  {
    "objectID": "02_01_IntroductionStata.html#stata-commands",
    "href": "02_01_IntroductionStata.html#stata-commands",
    "title": "An Introduction to Stata",
    "section": "Stata Commands",
    "text": "Stata Commands\nBeyond these logical and mathematical operators, Stata comes with a large suite of built in commands which perform specific tasks, as well as additional user-contributed functions (discussed below) which can perform a number of more specialised tasks. “Out of the box” Stata comes with many commands which are all comprehensively documented. A full list can be found in the Stata manuals.\nCommands are designed to perform specific tasks, potentially accepting input (known as “arguments”), and returning output which you can interact with. We have already seen a number of functions above such as display, generate, and describe which at times we have used with arguments, and at times we have used without arguments. In order to use commands the highly-standardised Stata syntax is to provide the command’s name, followed by its arguments (variable names, conditionals for a subset of observations and other requirements) and its specific options, which are separated by a comment from the main arguments. These options allow us to further specialise commands by requesting particular optional changes to the default implementation. To understand how a command works, we should consult the help documentation which is available for each command by typing help and the command name. For example, if we wish to understand the describe function, typing help describe will provide us with the following syntax, along with a list of options, further explanation, and so forth.\ndescribe [varlist] [, memory_options]\nThe syntax is always laid out in this standardised way, where square brackets imply that something is optional. Here if we do not type a specific list of variables (varlist) Stata will describe to us all the variables in memory. We are also optionally able to include options, such as simple, and short, which are laid out below in the help file. You will also note one other thing in the syntax diagram, and that is that the d describe is underlined. Each Stata command will underline the shortest possible way a command can be written by providing this underline, and in this case this implies that even if we just write d Stata will understand that this refers to the describe command. Indeed, we can write command names as this shortest version possible, or any longer version up to and including the full name, so d, de, des and so forth are all understood as describe.\nIn certain cases if we are not sure exactly what function we need we can type search search_term and Stata will advise us of any functions which may do what we’re after. For example, if we are seeking to summarise our variables, we may type search summary, and we see a range of functions, including one called summary. We can then consult the help file to see if this is precisely what we are after. Reading the help file we see that the command “calculates and displays a variety of univariate summary statistics.”\nAs in general we will work with data in using our commands, and to see how this summarize command works in practice, we will use an internal dataset which Stata provides for us. This is the auto dataset, a simple dataset often used to explore commands. In the following section we will further discuss ways to load data into Stata, but for now just note that we can load this particular dataset using the sysuse (for “system use”) command, along with the name of the auto dataset:\n\nsysuse auto\ndes\nsummarize mpg\n\n(1978 Automobile Data)\n\n\nContains data from C:\\PROGRA~1\\Stata16\\ado\\base/a/auto.dta\n  obs:            74                          1978 Automobile Data\n vars:            12                          13 Apr 2018 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\n              storage   display    value\nvariable name   type    format     label      variable label\n-------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and Model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair Record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn Circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear Ratio\nforeign         byte    %8.0g      origin     Car type\n-------------------------------------------------------------------------------\nSorted by: foreign\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n         mpg |         74     21.2973    5.785503         12         41\n\n\nHere we can see that when loading this dataset and describing variables, there are number of variables and types, and when we type symmarize mpg (one of the variables) we are provided with summary statistics such as the mean, standard deviation, and so forth.\nIf we again consult the help file of summarize we may see that there are actually some further details we did not encounter in the help file for describe. In particular, this time the syntax diagram is as follows:\nsummarize [varlist] [if] [in] [weight] [, options]\nAs before, we see that the varlist is optional, and so we could type the command without any arguments, in which case Stata will assume that we want descriptive statistics for all variables:\n\nsu\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n        make |          0\n       price |         74    6165.257    2949.496       3291      15906\n         mpg |         74     21.2973    5.785503         12         41\n       rep78 |         69    3.405797    .9899323          1          5\n    headroom |         74    2.993243    .8459948        1.5          5\n-------------+---------------------------------------------------------\n       trunk |         74    13.75676    4.277404          5         23\n      weight |         74    3019.459    777.1936       1760       4840\n      length |         74    187.9324    22.26634        142        233\n        turn |         74    39.64865    4.399354         31         51\ndisplacement |         74    197.2973    91.83722         79        425\n-------------+---------------------------------------------------------\n  gear_ratio |         74    3.014865    .4562871       2.19       3.89\n     foreign |         74    .2972973    .4601885          0          1\n\n\nWhat is new here are the if, in and weight options, which allow us to apply the command to a particular subset of observations, in the case of if you must specify a condition. For example we will summarize the price of those those observations with a mpg greater than its mean\n\nsu price if mpg > 21.2973\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         31    4879.968    1344.659       3299       9735\n\n\nAs you see the number of observations and other statistics have changed. The use of in is similar, but instead of specifying a condition you must specify the position as row numbers. For example, if for some reason we wished to summarise the price for the first 20 observations we could do so as follows:\n\nsu price in 1/20\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         20     6359.15    3710.311       3299      15906\n\n\nAlso you can see the weight observations if you wish to perform weighted calculations. We will not delve into this point here, but I direct you to the documentation for weight which can be consulted by typing: help weights.\nWe may also wish to explore certain [, options], which are laid out in the help file below the syntax diagram. As an example in the summarize help documentation you can see that there is a detail option and its description which says “display additional statistics”. So, noting the underlined d in the help file, we can provide more specialised output to the command by invoking this option:\n\nsu price, d\n\n(1978 Automobile Data)\n\n                            Price\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%         3291           3291\n 5%         3748           3299\n10%         3895           3667       Obs                  74\n25%         4195           3748       Sum of Wgt.          74\n\n50%       5006.5                      Mean           6165.257\n                        Largest       Std. Dev.      2949.496\n75%         6342          13466\n90%        11385          13594       Variance        8699526\n95%        13466          14500       Skewness       1.653434\n99%        15906          15906       Kurtosis       4.819188\n\n\nA final point of note is something which you may have realised when we summarised the entire dataset. While our dataset contains 74 observations (something which we can confirm using the count command), but one variable (rep78) does not provide a summary for 74 observations:\n\ncount\nsu price rep78\n\n  74\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n       rep78 |         69    3.405797    .9899323          1          5\n\n\nThe reason for this is because the variable has certain missing observations.\n\ncount if mpg==.\ncount if rep78==.\ncount if rep78<1000\ncount if rep78>-1000\ncount if rep78>-1000 & rep78!=.\n\n  0\n\n  5\n\n  69\n\n  74\n\n  69\n\n\nIn Stata, missing values are recorded as ., and are, by definition, not considered when conducting numerical analyses. In the code above we first confirm that the variable mpg has no missing observations, using the count command, and the if construct discussed above. We then confirm that the missing observations in mpg are indeed recorded as .. In general, Stata makes it relatively easy to work with missing observations, simply omitting these observations from any analyses. However, it is important to note this when conducting analysis. There is one hugely important caveat here, and that is to realise that by construction, Stata views . as an infinitely large value. So, if you consider the examples above, you will note that when we count the observations with \\(mpg<1000\\), we are correctly told it is 69. However, when we could the number of observations with \\(mpg>-1000\\), we are told that it is 74, precisely because .is seen as an infinitely large value. For this reason, it will be very careful for us to ensure that we deal carefully with missing values, for example, explicitly accounting for missings, as we do in the last line above.\nWhen we first introduced commands, we noted both that they accept arguments, and that they potentially return objects. Conveniently, the objects which Stata returns are generally provided to use for further processing if desired. Anything return from commands can be found by inspecting the return list, or ereturn list for estimation commands, as follows:\n\nsu mpg\nreturn list\n\n(1978 Automobile Data)\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n         mpg |         74     21.2973    5.785503         12         41\n\n\nscalars:\n                  r(N) =  74\n              r(sum_w) =  74\n               r(mean) =  21.2972972972973\n                r(Var) =  33.47204738985561\n                 r(sd) =  5.785503209735141\n                r(min) =  12\n                r(max) =  41\n                r(sum) =  1576\n\n\nHere you can see that there is a group of scalars that are automatically generated by Stata once the summarize command is executed, and which correspond to the output of the command. You can freely operate with these scalars\n\ndisplay \"Pearson's Variation Coefficient is: \" r(sd)/abs(r(mean)) \n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n         mpg |         74     21.2973    5.785503         12         41\n\nPearson's Variation Coefficient is: .27165434\n\n\nSimilarly, we can incorporate these scalars directly into any variable generation we may wish to proceed with, for example the variable mpg_meandev \\[mpg_i - \\overline{mpg}\\] Where \\(\\overline{mpg}\\) is the mean of mpg variable\n\nsu mpg\ngenerate mpg_meandev = mpg - r(mean)\nlist mpg mpg_meandev in 1/5\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n         mpg |         74     21.2973    5.785503         12         41\n\n     +-----------------+\n     | mpg   mpg_mea~v |\n     |-----------------|\n  1. |  22    .7027027 |\n  2. |  17   -4.297297 |\n  3. |  22    .7027027 |\n  4. |  20   -1.297297 |\n  5. |  15   -6.297297 |\n     +-----------------+\n\n\nAs you can see in the list output there is a new variable named mpg_meandev which reports the deviations of each observation from the mean of the mpg variable. Note that is not necessary to use the summarize command each time we want to access the return list values, provide that no other command is used which populates the return list with other values. For example, if we now decide that we would like to generate a standardised version of this variable named mpg_zscore as \\[\\frac{mpg - \\overline{mpg}}{\\widehat{\\sigma}_{mpg}}\\] Where \\(\\widehat{\\sigma}_{mpg}\\) is the sample standard deviation of mpg variable, we can do as follows:\n\ngenerate mpg_zscore = (mpg - r(mean)) / r(sd)\nlist mpg mpg_zscore in 1/5\n\n     | mpg   mpg_zsc~e |\n     |-----------------|\n  1. |  22    .1214592 |\n  2. |  17   -.7427698 |\n  3. |  22    .1214592 |\n  4. |  20   -.2242324 |\n  5. |  15   -1.088462 |\n     +-----------------+"
  },
  {
    "objectID": "02_01_IntroductionStata.html#user-written-commands",
    "href": "02_01_IntroductionStata.html#user-written-commands",
    "title": "An Introduction to Stata",
    "section": "User-Written Commands",
    "text": "User-Written Commands\nWhile Stata provides a wide array of inbuilt commands, at times we may wish to install extensions written by users which are expected to provide both some specific functionality, as well documentation which can be read as a help file. In Stata, most user-written commands are stored on a central repository known as the Statistical Software Components (SSC). This can be done quite simply using the command ssc install packagename. As an example, in order to install outreg2, a library for formatting regression tables, you can do so with the following code:\n\nssc install outreg2\n\nNow that the package is installed you can access the help file by typing help outreg2, and in the future will not need to re-install the package unless you wish to update the package. Once you have installed a user-written command, it will be permanently available to you.\nThere are other sources such as the Stata Journal or simply a URL, in both cases the command net will provide the tools for search and install packages."
  },
  {
    "objectID": "02_01_IntroductionStata.html#loading-and-viewing-data",
    "href": "02_01_IntroductionStata.html#loading-and-viewing-data",
    "title": "An Introduction to Stata",
    "section": "Loading and viewing data",
    "text": "Loading and viewing data\nIn most – if not all – of our research projects, we will not actually type in data to Stata by hand which is a cumbersome and highly error-prone activity, but rather we will wish to read in data which we have stored on our disk in one or a number of files. Such data may be stored in many formats; among others this may be free formats such as csv or txt, or proprietry formats such as excel or Stata’s own dta files. We have already seen one example of reading in data, which is Stata’s sysuse command, but this is a special case for data that is provided by Stata at installation.\nFor data which we have stored somewhere on our hard drive in formats such as csv (comma seperated values) or txt formats Stata comes with the import delimited, command. If you consult the help documentation, which is something I would recommend anytime you come across a new function, you will see that the basic usage is relatively standard, as follows:\n\nimport delimited \"Path/To/Your/File.csv\"\n\nThis will work as is, provided that your csv file is indeed delimited by commas and has variable names on the first line. If, however, your csv does not have variable names, or is delimited by some other character, you may refer to the options in the help file to see how to proceed. Similar such procedures can be followed for other data stored in flat formats such as .txt or others, as laid out in the help file.\nThis syntax crosses over to excel files too, via the import excel command:\n\nimport excel \"Path/To/Your/File.xls\"\n\nNote that the example is based on a file assumed to end with xls but the same command can be used replacing xls with xlsx if a file is saved as .xlsx. If you further explore the help documentation you will notice that you can specify a variety of arguments that indicates options such as if the row should be used as variable names, the range of cells you wish to import, the sheet name and other such details.\nFinally in order to import dta files, which is Stata’s native data format you can simply specify the use command:\n\nuse \"Path/To/Your/File.dta\"\n\nIf you are working entirely in Stata the value of saving your data in a .dta format is that you will maintain any internal labelling you may be working with, but this may be more complex if you wish to share your data with others who are working in other languages, given that it is a Stata-specific data format.\nOnce a dataset is loaded in Stata it is immediately avaiable for further processing and use. For a number of examples below we will use another dataset which is provided by Stata upon installation (all such datasets can be seen by typing sysuse dir). This is an abstract of the 1988 US NLSW (National Longitudinal Survey of Mature and Young Women).\n\nsysuse nlsw88.dta, clear\n\n(NLSW, 1988 extract)\n\n\nOnce we have loaded a dataset if we wish we can see a quick snapshot of the data with the list command, for example using list in 1/3 to observe the first three lines:\n\nlist in 1/3\n\n  1. | idcode | age |  race | married | never_~d | grade |         collgrad |\n     |      1 |  37 | black |  single |        0 |    12 | not college grad |\n     |--------------+-------------------------------------------------------|\n     | south | smsa | c_city |               industry | occupation | union  |\n     |     0 | SMSA |      0 | Transport/Comm/Utility | Operatives | union  |\n     |----------------------------------------------------------------------|\n     |        wage     |    hours     |      ttl_exp     |       tenure     |\n     |    11.73913     |       48     |     10.33333     |     5.333333     |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  2. | idcode | age |  race | married | never_~d | grade |         collgrad |\n     |      2 |  37 | black |  single |        0 |    12 | not college grad |\n     |--------------+-------------------------------------------------------|\n     | south | smsa | c_city |               industry | occupation | union  |\n     |     0 | SMSA |      1 |          Manufacturing |  Craftsmen | union  |\n     |----------------------------------------------------------------------|\n     |        wage     |    hours     |      ttl_exp     |       tenure     |\n     |    6.400963     |       40     |     13.62179     |         5.25     |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  3. | idcode | age |  race | married | never_~d | grade |         collgrad |\n     |      3 |  42 | black |  single |        1 |    12 | not college grad |\n     |--------------+-------------------------------------------------------|\n     | south | smsa | c_city |               industry | occupation | union  |\n     |     0 | SMSA |      1 |          Manufacturing |      Sales |     .  |\n     |----------------------------------------------------------------------|\n     |        wage     |    hours     |      ttl_exp     |       tenure     |\n     |    5.016723     |       40     |     17.73077     |         1.25     |\n     +----------------------------------------------------------------------+\n\n\nDepending on the way that we are working with Stata, we may also be able to explore the data more interactively using the browse command."
  },
  {
    "objectID": "02_01_IntroductionStata.html#tidying-data",
    "href": "02_01_IntroductionStata.html#tidying-data",
    "title": "An Introduction to Stata",
    "section": "Tidying data",
    "text": "Tidying data\nOnce we have loaded data in Stata, we will likely want to manipulate the data in a number of ways. This can include relatively simple things like sub-setting to certain columns or rows (which we have already encountered above), but also more complicated things like aggregating data to a higher level, calculating statistics for one or mutliple variable(s) over other variables, joining together a number of datasets, or “reshaping” data in certain ways. Here we will provide a very brief overview of these procedures, and introduce other things as we need them thoughout the book.\n\nAggregating Data\nTo start with some basic data manipulations, we can consider the case where we may wish to convert the data which we have imported into some group-level averages. We can do many such manipulations of this type using Stata’s collapse command. At its most simple level, this command allows us to take some disaggregated data, and “collapse” it to some more aggregated data. For example, imagine we wished convert our data into a single line containing the mean wage over the entire data. We can do this simply with the collapse command:\n\ncollapse wage\nlist\n\n     |     wage |\n     |----------|\n  1. | 7.766949 |\n     +----------+\n\n\nThis aggregation can occur in a much richer way depending on how we wish to aggregate data (ie which type of statistics we desire), and over what variables we would like this aggregation to occur. For example, imagine instead of the mean wage we wished to measure mean wages and the standard deviation of wages over whether an individual graduated from college, and whether they are a member of a union. We can do so as follows, noting that statistics are indicated in parentheses along with other arguments, while the levels which we wish to group over are indicated with the by() option. Once we have generated this collapsed version of the data we will save the data file on disk using the save command for some processes below:\n\ncollapse (mean) mean_wage = wage (sd) sd_wage = wage, by(collgrad union)\nlist\nsave groupedNLSW.dta, replace\n\n     |         collgrad      union   mean_w~e    sd_wage |\n     |---------------------------------------------------|\n  1. | not college grad   nonunion    6.36869   3.351124 |\n  2. | not college grad      union   7.823589   3.849597 |\n  3. | not college grad          .   7.946638   10.00492 |\n  4. |     college grad   nonunion   10.11737   5.052896 |\n  5. |     college grad      union   10.47342   4.275456 |\n     |---------------------------------------------------|\n  6. |     college grad          .   12.53987   12.33248 |\n     +---------------------------------------------------+\n\nfile groupedNLSW.dta saved\n\n\nGenerically, the syntax of collapse is as follows,\ncollapse (stat) newname = varname (stat) newname = varname ..., by(groupvariables)\nWhere the use of ... is to denote that we could add more stats to this collapse.\n\n\nMerging/Joining Data\nWe will also, at times, wish to join two (or more) pieces of data together, for example “merging” data to match up observations by some particular column or columns. For example, it may be that we have a survey where each individuals surveyed belongs to a household, where all of the individual-level questions in the data are stored as an individual-level file (along with the individual’s household ID), while the household level measures are stored in another file at the household level. If we wish to work with information on the individual and their household characteristics, we will need to “join”, or “merge” the file using the household ID.\nSuch processes are conducted in Stata using the merge command. As a simple example of this, we could imagine that we would like to take the grouped-level statistics we generated previously and join them back into our original NLSW data.. What we want to do then, is for each individual who has a particular college and union status, join the information on the mean and standard deviation of the wage in this group to their row of data.\n\nsysuse nlsw88.dta, clear\nmerge m:1 collgrad union using groupedNLSW\nd\n\n(NLSW, 1988 extract)\n\n(label unionlbl already defined)\n(label gradlbl already defined)\n\n    Result                           # of obs.\n    -----------------------------------------\n    not matched                             0\n    matched                             2,246  (_merge==3)\n    -----------------------------------------\n\n\nContains data from C:\\PROGRA~1\\Stata16\\ado\\base/n/nlsw88.dta\n  obs:         2,246                          NLSW, 1988 extract\n vars:            20                          1 May 2018 22:52\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\n              storage   display    value\nvariable name   type    format     label      variable label\n-------------------------------------------------------------------------------\nidcode          int     %8.0g                 NLS id\nage             byte    %8.0g                 age in current year\nrace            byte    %8.0g      racelbl    race\nmarried         byte    %8.0g      marlbl     married\nnever_married   byte    %8.0g                 never married\ngrade           byte    %8.0g                 current grade completed\ncollgrad        byte    %16.0g     gradlbl    college graduate\nsouth           byte    %8.0g                 lives in south\nsmsa            byte    %9.0g      smsalbl    lives in SMSA\nc_city          byte    %8.0g                 lives in central city\nindustry        byte    %23.0g     indlbl     industry\noccupation      byte    %22.0g     occlbl     occupation\nunion           byte    %8.0g      unionlbl   union worker\nwage            float   %9.0g                 hourly wage\nhours           byte    %8.0g                 usual hours worked\nttl_exp         float   %9.0g                 total work experience\ntenure          float   %9.0g                 job tenure (years)\nmean_wage       float   %9.0g                 (mean) wage\nsd_wage         float   %9.0g                 (sd) wage\n_merge          byte    %23.0g     _merge     \n-------------------------------------------------------------------------------\nSorted by: \n     Note: Dataset has changed since last saved.\n\n\nAs we can see here, after completing the merge, we have our original NLSW data, but have additionally added the new columns mean_wage, sd_wage which are our generated variables, along with _merge which summarises the number of units which correctly merge (in this case, all of them). There are three types of merge which we may need to use. These are m:1 (as above), 1:1, or 1:m. These are indicated immediately after invoking the merge command and one of these must be indicated. These values refer to the quantity of observations in the data in Stata’s memory (in this case nlsw88.dta) and the quantity of observations in the data indicated after using (in this case groupedNLSW.dta), over which the merge should be conducted. When an m is indicated this implies that there are many observations in each level, and when a 1 is indicated it implies that there is a single observation in each level. In the above example, given that in the data on the disk there is a single observation for each of the variables collgrad and union, this is indicated as 1, while in the original data, there are multiple women in each group made up of collgrad and union (hence m:1). It is fundamental in merges to ensure that we confirm that either all observations are correctly merged to a partner in the using data, or if not, understand why this is not the case.\n\n\nFiltering Data\nFor the remaining examples, we will clear all the information from Stata’s memory to manually input a simplified data set to explore a number of tasks. We will do this using the input command, as follows:\n\nclear all\ninput str2 Zone Year Sales\n\"A\" 2021 100\n\"A\" 2022 115\n\"B\" 2021 98\n\"B\" 2022 106\nend\nlist\n\n          Zone       Year      Sales\n  1. \"A\" 2021 100\n  2. \"A\" 2022 115\n  3. \"B\" 2021 98\n  4. \"B\" 2022 106\n  5. end\n\n     +---------------------+\n     | Zone   Year   Sales |\n     |---------------------|\n  1. |    A   2021     100 |\n  2. |    A   2022     115 |\n  3. |    B   2021      98 |\n  4. |    B   2022     106 |\n     +---------------------+\n\n\nThe input command is quite a unique command in that it tells Stata that we will manually write in variable values, and will be assumed to keep going until we type end. Here we have told Stata that we will input three variables manually, and these are named as Zone, Year and Sales. You may also note the addition of str2 before the Zone variable name, which informs Stata that this variable Zone will be inputed as a string, as otherwise Stata assumes that variables will be input manually.\nOne way to filter variables is to use the keep and drop commands. These commands can either indicate variable names, in which case all indicated variables will either be kept or dropped, or can indicate conditions, in which case all observations complying with the conditions will be kept or dropped. We will illustrate this below, additionally illustrating the use of preserve and restore which allow us to “store” a dataset in a particular moment, and then return to this dataset at a later moment.\nAs an example first we are going to delete the variable Year with the drop command and next restore to its previous status\npreserve\ndrop Year\ndis \"Prior to restore\"\nlist\nrestore\ndis \"After restore\"\nlist\nPrior to restore\n\n     +--------------+\n     | Zone   Sales |\n     |--------------|\n  1. |    A     100 |\n  2. |    A     115 |\n  3. |    B      98 |\n  4. |    B     106 |\n     +--------------+\n\nAfter restore\n\n    +---------------------+\n    | Zone   Year   Sales |\n    |---------------------|\n 1. |    A   2021     100 |\n 2. |    A   2022     115 |\n 3. |    B   2021      98 |\n 4. |    B   2022     106 |\n    +---------------------+\n\nAs you can see the combination of preserve and restore allows to modify the dataset and go back, while drop remove what we specify, in the example was a variable but also can be observations in specific positions specifying in or depending on a condition specifying if. keep command works opposite to drop, as drop remove only what is specified, keep mantains only the specified and remove everything else. You may wish to confirm that this also works with particular conditions, such as if Sales>100.\n\n\nReshaping Data\nFinally the last tool that we introduce for data manipulation is the idea of “reshaping” or transforming data from a wide format to a long format (and vice versa). We refer to data in a long format if for each observation we have multiple rows of data, covering different levels of some other variable. For example, a classic example of this would be a panel of data over countries and years. On the other hand, we refer to data as being in a wide format if for each observation we have multiple columns (or variables) which cover different levels of some other variable. In certain settings we will wish to work with data in one format, and in other settings we may find it more convenient to work with data in another format. Our simplified dataset above is precisely the example of a long format, given that for each Zone we have data over multiple years. In order to transform this dataset to a wide format the code is\nreshape wide Sales, i(Zone) j(Year)\nlist\nData                               Long   ->   Wide\n-----------------------------------------------------------------------------\nNumber of observations                4   ->   2\nNumber of variables                   3   ->   3\nj variable (2 values)              Year   ->   (dropped)\nxij variables:\nSales   ->   Sales2021 Sales2022\n-----------------------------------------------------------------------------\n\n   +----------------------------+\n   | Zone   Sal~2021   Sal~2022 |\n   |----------------------------|\n1. |    A        100        115 |\n2. |    B         98        106 |\n   +----------------------------+\nAs you can see now the dataset has one observation for each zone and no repeated values for a variable. If we wish to return to the long format we can use the reverse “reshape long” function, where in this case we must provide a name for the variable which will record the levels over which A and B are observed. Namely:\nreshape long Sales, i(Zone) j(Year)\nIn practice, we will generally use these commands with a considerably larger number of units, but the basic principles are the same regardless."
  },
  {
    "objectID": "02_02_Chapter2Stata.html",
    "href": "02_02_Chapter2Stata.html",
    "title": "Details required for Chapter 2",
    "section": "",
    "text": "Block 2.1\nTo understand the equivalence between regression analysis and the comparison of means in a binary regression set-up, we refer to Section 2.1 of the online coding resource. In this section, we work with data from a randomized control trial that examines asset transfers to poor households in India, as discussed in the paper by Banerjee et al. (2021).\n\n\n\n\nclear all\nset more off\n* Load the household data\nuse \"Datasets/Banerjee_et_al_2021.dta\", clear\n\n/* Simple Regression for ind_fin_el1: We run a simple regression of the variable ind_fin_el1 on the treatment variable, filtering the data to only include observations where el1 == 1.This is done to isolate the effect of the treatment on a particular group or under certain conditions, where in this case el1 corresponds to the first wave of responses.\n*/\n  \nreg ind_fin_el1 treatment if el1==1\nscalar coef_treatment = _b[treatment]\n\n\n* Calculate mean for the treatment group\nsummarize ind_fin_el1 if treatment==1 & el1==1\nscalar mean_treatment = r(mean)\n\n\n* Calculate mean for the control group\nsummarize ind_fin_el1 if treatment==0 & el1==1\nscalar mean_control = r(mean)\n\n\n* Calculate and display the difference in means\nscalar diff_means = mean_treatment - mean_control\ndisplay \"Coefficient for treatment in regression: \" coef_treatment\ndisplay \"Difference in means (Treatment - Control): \" diff_means\n\n* Display the comparison\ndi \"The coefficient from the regression should be equal to the difference in means to demonstrate equivalence.\"\n\n      Source |       SS           df       MS      Number of obs   =       815\n-------------+----------------------------------   F(1, 813)       =      0.05\n       Model |   .01866813         1   .01866813   Prob > F        =    0.8179\n    Residual |  285.988718       813   .35176964   R-squared       =    0.0001\n-------------+----------------------------------   Adj R-squared   =   -0.0012\n       Total |  286.007386       814  .351360425   Root MSE        =     .5931\n\n------------------------------------------------------------------------------\n ind_fin_el1 |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n   treatment |  -.0095893   .0416262    -0.23   0.818    -.0912968    .0721182\n       _cons |   .1354387   .0303061     4.47   0.000     .0759513    .1949261\n------------------------------------------------------------------------------\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n ind_fin_el1 |        432    .1258494    .6050755  -1.129678   4.763554\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n ind_fin_el1 |        383    .1354387     .579295  -.3739908   3.777691\n\n\n\nCoefficient for treatment in regression: -.00958932\n\nDifference in means (Treatment - Control): -.00958932\n\nThe coefficient from the regression should be equal to the difference in means \n> to demonstrate equivalence.\n\n\nBy following these steps, we can empirically verify that in a binary regression set-up, the coefficient for the treatment variable in a simple OLS regression is equivalent to the difference in means between the treatment and control groups. This serves as a useful check for the validity of our regression model and strengthens the causal interpretation of the treatment effect."
  },
  {
    "objectID": "02_03_Chapter3Stata.html",
    "href": "02_03_Chapter3Stata.html",
    "title": "Details required for Chapter 3",
    "section": "",
    "text": "Here will be the Chapter 3 excersises and examples."
  },
  {
    "objectID": "03_00_FollowingPython.html",
    "href": "03_00_FollowingPython.html",
    "title": "Following Along with Python",
    "section": "",
    "text": "Introduction\nIn this section we will provide a primer in Python, which seeks to provide you with the necessary tools to explore the microeconometric methods introduced throughout the book. Python was developed in the late 1980s by Guido van Rossum, and is an open source language which has gained considerable traction with users and developers, meaning that a diverse array of packages now exist to expand the base language. Like R, it has an active community providing online support on forums such as Stack Overflow, and a lot of online code available. It provides you with the large majority of tools you will need throughout this course, as well as the programming environment necessary to extend available tools where required. Python has had a number of major version upgrades throughout its history, but now versions 3 and above of Python are widely adopted. I recommend working with some version greater than 3 when you install Python, as code written in version 2 will not necessarily work in version 3.\nThe goal of this site is not to provide you with a comprehensive introduction to the language, but instead we it seeks to provide you with an overview of the basic tools to understand the required tools that we will use to get up and running in this book. In the first section we will focus on a brief rundown of some principal elements of Python without yet getting into the empirical methods discussed in the textbook. Thereafter, we will focus on causal econometric methods, but in each section will also introduce any further tools required to complete key analyses or end-of-chapter questions. The goal of this resource is that after following along with these sections you will be sufficiently well-versed in Python that you will comfortably be able to work with real data and econometric implementations. Nevertheless, below we point you to further resources if you are seeking a comprehensive overview of Python as a language.\n\n\nInstalling and Working with Python\nInformation on how to install Python can be found at the Python website, which has a brief wiki discussing installation on various operating systems. Once you have Python installed in your operating system you could work directly with Python from the command line by typing “Python” and you may see output like the below.\n\npython\n\nPython 3.8.10 (default, May 26 2023, 14:05:08)\n[GCC 9.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nI recommend that along with Python, you install in Integrated Development Environment (IDE). There are many IDEs which can be used with Python, see for example the list here. I can recommend Spyder (see below for a screen shot of what this looks like). The value of having this IDE is that you can easily view any elements that you have generated in the memory in Python, output like graphics, help documentation, and so forth. An example of what Spyder looks like is provided below, where you can see that code is visible on the left, as well as graphical output, and calls to the command line on the right. You can also display other pieces of information such as documentation and information about what is in the your “environment”. For further information on installing Spyder, see the Spyder website. You are of course welcome to use Python however you prefer, and you may prefer to edit code in your favourite text editor and execute code from the command line. Throughout the rest of these resources, we will assume that you have Python installed on your computer and have some way to interact with it, either via an IDE or some other work flow that suits you.\n\n\n\nSpyder screenshot\n\n\n\n\nFurther Resources\nA number of broader resources are available about Python. A very nice free online text book is:\n\nMcKinney, Wes (2023) Python for Data Analysis. O’Reilly Media, 3rd Edition."
  },
  {
    "objectID": "03_01_IntroductionPython.html",
    "href": "03_01_IntroductionPython.html",
    "title": "An Introduction to Python",
    "section": "",
    "text": "Preliminaries\nWe will assume that when you are reading this you have downloaded Python and have been able to access Python in some way. This may be via the command line, but most likely we imagine you will have some type of IDE which allows you to see a number of windows including a space for graphical output, a description of the contents of your Python Environment (for example any data you have loaded), potentially a place to write your code in Python, and, most importantly, a “shell”, or Python interface, which looks like the following:\nIt may be prefereble to install ipython (details here) which provides a slightly more interactive version of Python. This is available by default in certain IDEs such as Spyder. The value of working in an ipython shell over a standard python shell is the availability of a number of “magic commands” such as simple ways to get help, run files, and so forth. We will delve into this slightly more below, but under the hood, both Python and IPython work with the Python language, and have a very similar feel; for example, see below for an example of the IPython shell compared to Python above.\nHere, Python code can be entered directly following a prompt which is the >>> symbol at the bottom of the python window above, or In [1]: in the case of IPython. From the moment you open Python you are in a Python session until the moment you close it, and from now on we will understand a session as this time between when we open and close Python. The most simple way to execute code in Python is to simply input commands into your Python prompt. So, for example, you could write the following at to the right of the >>> symbol, and Python will evaluate this code:\nAs you can see, Python has understood the instruction that you want to add 4 plus 4 and show the result, which is evaluated, and output directly below the code in the Python prompt. Later in this chapter we will properly explain these mathematical operations and the different Python data types. Beyond evaluating such simple mathematical operations, Python has a host of in-built functions which can be invoked at the command line. While we will interact with this in more detail later, a simple example is the following, which you can type in your Python console:\nThis function simply sums the list of values provided within it, printing the output (10) below. We will return to discuss functions in more detail a bit later on.\nFor now you will note that we have simply been typing instructions to Python directly into the command prompt, and Python has been providing us with output. Generally, we will not work this way in an empirical research project, as we will want to develop code over time, and be able to return to Python and replicate code in the future. For this reason, generally we will work in Python scripts, or files named as something.py (where something will be logical name for the file). We will use these scripts to store the commands which we require for particular processes or routines, and can then save them to the disk of our computer before running them in Python. These scripts can be generated in any type of text editor, and then can be run in Python. If you are using IPython, running a script can be done simply using the %run command, by typing %run something.py. This is however a magic command, and so is available in IPython, but not in Python. Similarly, in certain other IDEs there may be other environment-specific ways to run .py files, for example the runfile command which is used in Spyder below:\nYou may notice something strange about the code and the output in the Python console in the image above. The Python script has 7 distinct lines of text, but the console only produces two lines of output. The reason for this is because we have included comments, or messages for human eyes, but not for the computer to interpret. Any time that the # character is included in code, Python understands that this is a comment, and hence is ignored in executing the code. So, for example, when we write:\nin our code, the Python console correctly echoes Hello World! back at us. However, if we enter precis ely the same code, but begin the line with a comment symbol, Python will not do anything given that it interprets everything to the right of # as plain text which should be ignored:\nSimilarly, comments can be used within lines:\nAs you can, this lines has executed the instruction to the left of # but ignores all instructions to the right. It is good practice to comment code extensively, both for sharing code with other users, but also for sharing code with yourself in the future, where you may not remember precisely what you were thinking when you originally wrote your code! If you wish to include multiple line comments, you can also do this by enclosing various lines in three inverted commas. For example, any lines contained between an opening \"\"\" and a closing \"\"\" will be assumed to be one long comment. You can confirm this for yourself in a Python script.\nOne final thing to note from this Python script is that the output from the final line of code (4+4) was not printed in the console. In Python, when you type 4+4 at the command line, you get immediate output because Python automatically sends output to the console. However, when you write 4+4 in a script and then run the script, there is no automatic output to the console. However, it is important to note that the calculation is conducted. You can confirm this yourself if you’d like by explicitly requesting that Python provide this output when it runs the code, replacing 4+4 in the code with print(4+4).\nUntil now we have executed some code in Python and seen some specific functions which print output, but we have never stored any results for later use within our code. Generally we will want to store the output of intermediate steps in our code for later use. This can be done in Python using the assignment symbol =. For example, below we store a value as a variable which we define, arbitrarily named x:\nThe use of = in the assignment must respect name = value. If, instead, we try to assign some variable as value=name an error will occur:\nThis error implies that the code has not been executed, and if we inspect the contents of x we will see that its prior value remains.\nThese errors tell you that something has gone wrong and Python couldn’t execute your code. Error messages include a brief explanation of what has gone wrong, to help you diagnose and fix your code. Any expected results from your command will not be assigned. At certain times, you may also see warnings which are printed when certain processes are run. These “warnings” that something has occurred that could potentially cause problems, but is not critical for code execution. In these cases, Python will still execute the code and give the result, but warn you that not everything is necessarily in order and this may cause inaccurate results or future problems. At times these messages can be ignored without further issues, but warnings can also be a sign of peril, and so time should be taken to review warnings and understand whether they are problematic.\nTo end this initial introduction, we will briefly explore the generation of basic plots in Python. In Python one of the most used modules to create plots is pyplot from the matplotlib module. If you have not previously installed this module, you can do so using pip install matplotlib, and then we will import the pyplot submodule as plt.\npyplot maps pairs of points in the \\((x,y)\\) coordinate axis and places them on a graph. For example, let’s consider a case where \\(x\\) is a sequence of integers between -5 and 5. This can be generated as below:\nNow consider the variable \\(y = x^2\\):\nWe will store these two objects as a data frame, by creating a dictionary containing x and y (though we could proceed to plot without first storing our data as a data frame):\nThe relevant function in pyplot is plot, which by default generates a line plot, making reasonable decisions for axes and labelling. Unless you have an IDE which automatically displays your plot in a plot panel, you should use the show method from pyplot to display your plot:\nIf you wish to change the default behaviour of plot you can do so with a range of options. For example, in order to change the plot to a scatter plot, we can set the linestyle to \"\" and the markers to points:\nThere are many ways to further customise graphics in Python, and indeed, we will see a range of other plots throughout the remainder of these pages. In general, editing basic plots can be done relatively straight-forwardly, allowing us to build up plots bit by bit. For example, we could use the mroz data we have worked with previously to plot the log(wage) against years of experince:\nAnother widely-used package to allow for further customisation of graphs in Python is the seaborn library, which can be installed from pip. For example, if we wish to overlay a regression line on the scatter plot above, along with its 95% confidence intervals, we can do so quite easily with seaborns regplot function. To see this, we will first import seaborn, and then finally generate the plot:\nWe will take forward many of the examples laid out on this page in the chapters ahead. For now, this introductory session aims to provide you with a sufficient overview to get up and running in Python, and apply your knowledge to microeconometric methods introduced in the book."
  },
  {
    "objectID": "03_01_IntroductionPython.html#data-types",
    "href": "03_01_IntroductionPython.html#data-types",
    "title": "An Introduction to Python",
    "section": "Data Types",
    "text": "Data Types\n\nText (String) Data\nTo get up and running with Python, it is useful to understand the different ways which data can be stored. Python can hold a number of different types of objects in its working memory, and it is important that Python understands what type of information we are passing or storing. Among other things, Python allows for information to be stored as characters, numeric values, arrays, data frames and lists. Different types of data allow different types of operations, and should be stored in different ways. Characters are perhaps the conceptually simplest type of data, and Python has excellent text processing libraries. Characters are simply strings of text, and should be specified between \" \", or ' '. We have seen the use of characters when printing “Hello world” previously. If characters are not indicated between quotes, Python will understand that they refer to variable names. For example, returning to Hello World, if this is entered without quotes, an error will appear:\nHello World\nSyntaxError: invalid syntax\nIn practice, what has actually occurred here is that, because we did not tell it otherwise, Python assumes that Hello is a variable name, but this must be followed by some valid assignment. For example, had we written Hello = \"world\" this would have been perfectly fine (though probably not what we were trying to do)! Alternatively, if we do actually want to write this as characters, we can do so simply by enclosing the text in double or single quotes:\n\n\"Hello World\"\n\n'Hello World'\n\n\nWe can confirm that this is effectively understood to be character data by using the type() function. This function indicates the type of data of the argument it receives, as we can see below. Note also that if we generate some variable using the assignment operators discussed above, type() will return to us the type of data that the variable contains:\n\ntype(\"Hello World\")\n\nstr\n\n\n\nmy_string = \"Hello World\"\ntype(my_string)\n\nstr\n\n\nIn both cases Python correctly informs us that we are working with string (text) data, printing str.\n\n\nNumeric Data\nNumeric data is entered without any special behaviour, simply entering the number in the case of scalar values. Indeed, if numeric data is accidentally enclosed between quotes, it will be treated as character, and not numeric data. For example, below we assign two variables based on the number 2 and confirm that one is numeric while the other is viewed as a character.\n\nnum1 = 2\nnum2 = \"2\"\ntype(num1)\n\nint\n\n\n\ntype(num2)\n\nstr\n\n\nIf you try to perform numerical operations based on the above variables (more on this below), you will see that num1 can be involved in such calculations, while num2 generally cannot.\n\n\nTuples, Lists and Arrays\nTypically, we will be dealing with more than a single number in our work, and for this reason need ways to collect groups of numbers. Tuples and lists (or arrays) provide some ways to do this.\nTo create a list of numbers, we simply need to enclose a group of values within square parentheses, separating the numbers by commas. For example, to create a list with two values:\n\nmy_list = [1,2]\ntype(my_list)\n\nlist\n\n\nThis is required to group numbers. If we simply write a series of numbers without separating them by commas, an error will occur:\n[1 2]\nSyntaxError: invalid syntax. Perhaps you forgot a comma?\nthough the error message has helpfully pointed us to the source of our error. Similarly, if we forget the comma and fail to wrap our numbers if [ ], we will once again generate an error.\nHowever, we can generate an alternative way to store numbers without the square brackets if we separate numbers by commas. This will generate a tuple, as we can see below:\n\nmy_tuple = 1,2,3\ntype(my_tuple)\n\ntuple\n\n\nAs this data types just combine elements they allow to combine different data types. For example:\n\ntype([1,\"Hello\"])\nmy_tuple = 1, 'Hello'\ntype(my_tuple)\n\ntuple\n\n\nIn order to create arrays with a matrix-like form, we can list lists, which allows us to get objects in two dimensiones: rows and columns. To see this, we will generate a matrix below, and then print it out to the left of the string “Matrix =”:\n\nmatrix = [[1,2],[3,4]]\nprint(\"Matrix =\", matrix)\n\nMatrix = [[1, 2], [3, 4]]\n\n\nWhile this allows us to view data in a matrix-like form, we would likely not want to start here if we actually want to do matrix-based calculations. Indeed, there are a range of extended functionalities to Python which we can access by installing external packages and libraries. We will hold off our discussion of the installation of packages until a little bit later in this section, but in the interests of completion would just note that there is a package known as NumPy which allows for more convenient matrix processing and numerical operations. Provided that you have this package installed, you can generate a numpy array as follows, noting that we will discuss the installation of packages and the idea behind import slightly later in this document.\n\nimport numpy\nnumpy.array([[1,2],[3,4]])\n\narray([[1, 2],\n       [3, 4]])\n\n\nUnlike in lists, tuples, and so forth, numpy’s interest in systematic processing of elements means that it will force the data into a uniform type, in particular coercing the array to the most restrictive data type, as you can see below, where the list [1,2] has been coerced to ['1', '2']:\n\nnumpy.array([[1,2],[\"Hello\", \"World\"]])\n\narray([['1', '2'],\n       ['Hello', 'World']], dtype='<U11')\n\n\nWe will return to discuss numpy at more length below.\n\n\nDictionaries\nDictionaries are similar to lists, however values can be assigned to a particular key. As we will see shortly, in certain circumstances this may be preferable in terms of providing control over elements within the dictionary.\n\nmy_dictionary = {'First': [1,2], 'Second':[3,4]}\nmy_dictionary\n\n{'First': [1, 2], 'Second': [3, 4]}\n\n\n\n\nData frames\nLikely the type of object which we will be working with the most in our analysis in Python are data frames. Data frames allow us to mix mutliple other types of data, viewing the collection of variables and observations as a database with multiple rows and columns. Within data frames, we can easily subset to certain rows (observations), columns (variables), and conduct univariate and multivariate analysis. It will likely be the starting point for much of your work when you import a standard dataset into Python. Similarly to the case of numpy arrays, data frames in Python are handled by an external library known as Pandas, and again, we will discuss the installation of libraries properly below.\nHowever, to see way that a data frame can be used in Python, if you have Pandas installed on your computed you can generate a data frame as follows:\n\nimport pandas\ndf = pandas.DataFrame({'Col1': [1, 2], 'Col2': [\"Hello\", \"World\"],'Col3': [3, \"String\"]})\ndf\n\n\n\n\n\n  \n    \n      \n      Col1\n      Col2\n      Col3\n    \n  \n  \n    \n      0\n      1\n      Hello\n      3\n    \n    \n      1\n      2\n      World\n      String\n    \n  \n\n\n\n\nYou may notice that all we have done to generate this data frame has been to enclose a dictionary inside of the command pandas.DataFrame(). As is the case with numpy arrays, given that Pandas seeks consistency within columns of data, all types of data in a single column will be coerced to the most restrictive type. In the case of df above, the third column (Col3) will be stored as a string. If you would like to work with Pandas immediately, you may wish to jump ahead to the section where we discuss the Installation of libraries, and then return here."
  },
  {
    "objectID": "03_01_IntroductionPython.html#subsetting-multidimensional-elements",
    "href": "03_01_IntroductionPython.html#subsetting-multidimensional-elements",
    "title": "An Introduction to Python",
    "section": "Subsetting Multidimensional Elements",
    "text": "Subsetting Multidimensional Elements\nAs we saw in data frames and lists, often we will work with objects that store multiple elements. Sometimes we may want to work with a subset of this multidimensional data instead the entire object. In the case of lists you can easily gain access to any specific value by using the list’s name followed by square parentheses and the location of the item(s) you wish to extract:\n\nmy_list[0]\n\n1\n\n\nA particularity of Python is that it uses zero-based indexing, so element [0] refers to the first item in a list, while the last item in a list will be element N-1. However, items can also be accessed by referring to their position with regards to the end of the list. For example, specifying [-1] will return the final item in the list:\n\nmy_list[-1]\n\n2\n\n\nIf you wish to access more than one object you can use the slice operator which consists of a colon : between [ ] in the format of [StartIndex : EndIndex]. As an example we will replace my_list with a longer list and access from the first to the second element.\n\nmy_list = [1,2,3,4,5,6]\nmy_list[0:2]\n\n[1, 2]\n\n\nYou will likely have notede here that the EndIndex was 2, which in Python points to the third element of the list. But as we can see, the third element wasn’t included in the output, and this is because Python returns elements up until EndIndex but not including EndIndex. Similarly, with the slide operator we can also use positions relative to the end of the list, provided that EndIndex is greater than StartIndex.\n\nmy_list[2:-1]\n\n[3, 4, 5]\n\n\nIn the case of the Pandas data frames that we have discussed, there are a number of ways that we can access data: either referring to column names, or referring to the order of columns, along with the row numbers that we wish to access. If we wish to access columns based on their names, we can use the loc method, while if we wish to access columns based on their column number, we can use the iloc method. Previously we have generated a data frame which we named df:\n\ndf\n\n\n\n\n\n  \n    \n      \n      Col1\n      Col2\n      Col3\n    \n  \n  \n    \n      0\n      1\n      Hello\n      3\n    \n    \n      1\n      2\n      World\n      String\n    \n  \n\n\n\n\nIf we wish to access data through column names (also known as column index) we use the syntax: DataFrameName.loc[RowIndex, ColIndex]. In the data frame df, our column index is: Col1, Col2 and Col3 while the row index is 0 and 1. This, in order to access the “World” element we can simply write:\n\ndf.loc[1, 'Col2']\n\n'World'\n\n\nAlternatively, the iloc method works similarly, however accesses rows through their numeric index, which as standard begins at 0, and goes up from their. So, to do the equivalent of the above using iloc we can simply write the following.\n\ndf.iloc[1,1]\n\n'World'\n\n\nIn this case, because the rows are named numerically, the RowIndex value is 1 in both examples. Frequently our data frames will have rows named numerically, and so this is quite standard. If however we want to name our rows differently, we can do this by using df.index as below:\n\ndf.index=['a','b']\n\nNow, if we attempt to use the previous loc call where 1 was a valid row index, we will see that this now results in an error, and we instead need to use the correct row index.\n\ndf.loc[1, \"Col2\"]\n\nKeyError: 1\n\n\nSyntaxError: invalid syntax\n\ndf.loc[\"b\", \"Col2\"]\n\n'World'\n\n\nSimilarly, if we accidentally mix up our notation and try to use column naming in iloc, things will not work as we hope:\n\ndf.iloc[1, \"Col2\"]\n\nValueError: Location based indexing can only have [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array] types\n\n\nValueError: Location based indexing can only have [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array] types\n\ndf.loc[1, 1]\n\nKeyError: 1\n\n\nKeyError: 1\nAs we saw previously with lists, we can use the slice operator to access ranges of cells:\n\ndf.iloc[0:2, 2]\n\na         3\nb    String\nName: Col3, dtype: object\n\n\nIn case we wish to acess an entire row, we can do this by using the slice operator with no values, which defaults to accessing the entire row:\n\ndf.iloc[1,:]\n\nCol1         2\nCol2     World\nCol3    String\nName: b, dtype: object\n\n\nThis can similarly be used with loc:\n\ndf.loc[\"a\",:]\n\nCol1        1\nCol2    Hello\nCol3        3\nName: a, dtype: object"
  },
  {
    "objectID": "03_01_IntroductionPython.html#basic-operations",
    "href": "03_01_IntroductionPython.html#basic-operations",
    "title": "An Introduction to Python",
    "section": "Basic Operations",
    "text": "Basic Operations\nThe standard mathematical operations in Python likely work how you would expect. Basic mathematical operations are as follows, with an example of their use below:\n\n\n\nSymbol\nOperation\n\n\n\n\n+\nAddition\n\n\n-\nSubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n**\nPower\n\n\n%\nModulus\n\n\n//\nInteger Division\n\n\n\nA few examples\n\n10 + 3\n10 - 3\n10 * 3\n10 / 3\n10 ** 3\n10 % 3\n10 // 3\n\n3\n\n\nTwo important things to note are that the power symbol is not ^, but rather **, and that integer division simply returns the integer, discarding any remainder.\nAlso note that when you combine different operators Python follows the PEMDAS order, first resolving Parentheses, second solve Exponents, third Multiplication and Division, fourth Addition and Substraction. Both multiplication and division, and addition and substraction are evaluated left to right, rather than sequentially by operation, so it is important to indicate to Python explicitly what you need using parentheses in certain cases. For example in order to solve \\[-\\frac{5 + 3^{5-3}}{5\\times 3}\\] You should use the code\n\n-(5+3**(5-3))/(5*3)\n\n-0.9333333333333333\n\n\nPython also has logical operatos that returns a boolean value (True or False) pointing if a statement is true or false. These operators are:\n\n\n\nSymbol\nComparison\n\n\n\n\n==\nEquals\n\n\n!=\nDifference\n\n\n>\nGreater\n\n\n>=\nGreater or Equal\n\n\n<\nLess than\n\n\n<=\nLess than or Equal\n\n\n| or or\nOr\n\n\n& or and\nAnd\n\n\nnot\nNegation\n\n\n\nA few examples\n\n4 == 3\n4 != 3\n4 > 3\n4 >= 3\n4 < 3\n4 <= 3\n4 in [1,2,3,4]\nTrue | False\nTrue or False\nTrue & False\nTrue and False\nnot(True)\n\nFalse\n\n\nThese boolean values are interpreted by Python as the numbers 0 for False and 1 for True, a characteristic which allows you to work with them numerically and is useful when you want to check a series of conditions."
  },
  {
    "objectID": "03_01_IntroductionPython.html#functions",
    "href": "03_01_IntroductionPython.html#functions",
    "title": "An Introduction to Python",
    "section": "Functions",
    "text": "Functions\nBeyond these logical and mathematical operators, Python comes with a basic suite of built in functions which perform specific tasks, as well as a large number of other functions (discussed below) which can be imported, or installed and imported, to extend this basic functionality. Every time you open Python, a range of built in functions are always available (listed here).\nFunctions are designed to perform specific tasks, potentially accepting input (known as “arguments”), and returning output which you can interact with. We have already seen a number of functions above such as print(), sum() and type(), all of which we have used with arguments. Arguments are simply the contents we provide the function inside the parentheses. Understanding how packages work, including the arguments they accept and the objects they return is key. This can be found by consulting the help file by typing help(function_name). For example, if we type help(abs) we will see that the function accepts an argument “x”, and “Return the absolute value of the argument”. We can explore this to confirm that this is what happens with a simple example:\n\nabs(-1)\n\n1"
  },
  {
    "objectID": "03_01_IntroductionPython.html#managing-modules",
    "href": "03_01_IntroductionPython.html#managing-modules",
    "title": "An Introduction to Python",
    "section": "Managing modules",
    "text": "Managing modules\nWhile not available “out of the box”, an enormous range of functions can be imported into Python by loading Modules or libraries. Modules are a set of predefined objects and functions that can be imported and used in Python to extend functionality in many directions. This modules can themselves contain various levels of submodules which contain individual functions, and in general the syntax to access functions within a module is module.function, or if there are submodules module.submodule.function.\nThere are a large number of modules which are available with a standard installation of Python. If you would like to see a list of all the modules available – which will likely be large – you can see this is in Python by typing:\nhelp(\"modules\")\nTo access the functions contained in a particular module we must import this into Python every time that we run Python. This process of importing a function uses the import command, and should generally be done at the top of any script we write. So, for example, if we wish to access functions from within the os module, we can do this by typing:\n\nimport os\n\nAnd similarly, if we’d like to be able to access functions from the math module we can do this as:\n\nimport math\n\nWe can then use function which are available within these modules provided that we prepend the module name. Thus, for example, from math:\n\nmath.log(5)\n\n1.6094379124341003\n\n\nand\n\nmath.pi\n\n3.141592653589793\n\n\nare quite self-explanatory, while from os we can find a large arrange of tools to interact with our operating system, for example the getcwd function if we would like to see the directory where we are currently working or the chdir function to change to a new working directory:\n\nos.getcwd()\n\n'C:\\\\Users\\\\Usuario\\\\Desktop\\\\Prim_2023\\\\Ayudantias\\\\Investigación\\\\Clarke_MicroeconometriaCausalidad\\\\Ejemplo_QuartoBook'\n\n\nHowever, at times we may wish to install modules which are not available in our standard installation. A range of additional packages are available via pip, which is the package installer for Python (hence pip). Two packages which we referred to previously were NumPy and Pandas. Both of these packages will be fundamental for our work, and can be installed from pip using the command pip install packagename. For example, to install Pandas we can type:\npip install pandas\nNow that the module is installed you will not need to install it again unless you want to update the module. However, as above we will need to import the package every time we wish to use it using import. While this can be done as previously:\n\nimport pandas\n\nIn practice we can assign the package an alias which makes it easier to use later. It is standard usage to instead load pandas as follows, where by writing as pd means that we now refer to the package as pd rather than Python:\n\nimport pandas as pd\ndf = pd.DataFrame(df)\ndf\n\n\n\n\n\n  \n    \n      \n      Col1\n      Col2\n      Col3\n    \n  \n  \n    \n      a\n      1\n      Hello\n      3\n    \n    \n      b\n      2\n      World\n      String\n    \n  \n\n\n\n\nSimilarly, it is standard to load numpy as:\n\nimport numpy as np\n\nImplying that we can refer to any numpy functions as simply np.function_name(). If there are specific functions that will be particulary commonly used, we can import these directly, and then they can be used without first typing the package name. For example, if we want to simplify access to numpy’s sqrt function to take square roots, we can do this as below:\n\nfrom numpy import sqrt\nsqrt(4)\n\n2.0\n\n\nWhat’s more, if for some reason we wish to provide alternative names for specific functions, we can do this once again with as:\n\nfrom numpy import sqrt as squareroot\nsquareroot(4)\n\n2.0"
  },
  {
    "objectID": "03_01_IntroductionPython.html#loading-and-viewing-data",
    "href": "03_01_IntroductionPython.html#loading-and-viewing-data",
    "title": "An Introduction to Python",
    "section": "Loading and viewing data",
    "text": "Loading and viewing data\nIn most – if not all – of our research projects, we will not actually type in data to Python by hand which is a cumbersome and highly error-prone activity, but rather we will wish to read in data which we have stored on our disk in one or a number of files. Such data may be stored in many formats; among others this may be free formats such as csv or txt, or even proprietry formats such as excel or dta. We will generally do this using functions which are a part of Python which has the added benefit of importing our data as an easy to use data frame.\nFor data stored as csv (comma seperated values) or txt formats Panda comes with the functions read_csv, and read_table. If you consult the help documentation, which is something I would recommend anytime you come across a new function, you will see that these functions provide a large number of options to control how data is read into Python. If we would like to import a standard comma separated value (csv) file, the basic usage is\npd.read_csv(\"Path/To/Your/File.csv\")\nFor txt data, or in fact any type of delimited data, a useful function is read_table:\npd.read_table(\"Path/To/Your/File.csv\")\nIf you explore the help documentation you will notice that you can specify a variety of arguments such as indicating which character is used to indicate column breaks, whether a header is included, and so forth.\nWhile the options vary, a standardised syntax is also available for excel files such as those ending in .xls or .xlsx:\npd.read_excel(\"Path/To/Your/File.xlsx\")\nIf you refer to the help documentation you will see that there are a variety of arguments which allows you to point to a specific sheet you wish to import, a specific range of columns or rows, whether or not to use the first row as column names, etc.\nFinally in order to import dta the function is read_stata\npd.read_stata(\"Path/To/Your/File.dta\")\nThe examples so far have been to illustrate basic syntax, though imported data has not been stored in any particular object for later manipulation. In general we will of course want to go on to use the data we load, which is why we loaded it in the first place. This can be done in the same way as any other assignment in Python. Below we will import a particular dataset which is available as part of the wooldridge library, to have some data to work with below. This library is available for download from pip if you do not have access on your system. And contains 115 data sets from Wooldridge’s undergraduate Econometrics Textbook (“Introductory Econometrics: A Modern Approach”, 7th Edition by Jeffrey Wooldridge):\n\nimport wooldridge\nmroz = wooldridge.data('mroz')\nmroz\n\n\n\n\n\n  \n    \n      \n      inlf\n      hours\n      kidslt6\n      kidsge6\n      age\n      educ\n      wage\n      repwage\n      hushrs\n      husage\n      ...\n      faminc\n      mtr\n      motheduc\n      fatheduc\n      unem\n      city\n      exper\n      nwifeinc\n      lwage\n      expersq\n    \n  \n  \n    \n      0\n      1\n      1610\n      1\n      0\n      32\n      12\n      3.3540\n      2.65\n      2708\n      34\n      ...\n      16310.0\n      0.7215\n      12\n      7\n      5.0\n      0\n      14\n      10.910060\n      1.210154\n      196\n    \n    \n      1\n      1\n      1656\n      0\n      2\n      30\n      12\n      1.3889\n      2.65\n      2310\n      30\n      ...\n      21800.0\n      0.6615\n      7\n      7\n      11.0\n      1\n      5\n      19.499981\n      0.328512\n      25\n    \n    \n      2\n      1\n      1980\n      1\n      3\n      35\n      12\n      4.5455\n      4.04\n      3072\n      40\n      ...\n      21040.0\n      0.6915\n      12\n      7\n      5.0\n      0\n      15\n      12.039910\n      1.514138\n      225\n    \n    \n      3\n      1\n      456\n      0\n      3\n      34\n      12\n      1.0965\n      3.25\n      1920\n      53\n      ...\n      7300.0\n      0.7815\n      7\n      7\n      5.0\n      0\n      6\n      6.799996\n      0.092123\n      36\n    \n    \n      4\n      1\n      1568\n      1\n      2\n      31\n      14\n      4.5918\n      3.60\n      2000\n      32\n      ...\n      27300.0\n      0.6215\n      12\n      14\n      9.5\n      1\n      7\n      20.100058\n      1.524272\n      49\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      748\n      0\n      0\n      0\n      2\n      40\n      13\n      NaN\n      0.00\n      3020\n      43\n      ...\n      28200.0\n      0.6215\n      10\n      10\n      9.5\n      1\n      5\n      28.200001\n      NaN\n      25\n    \n    \n      749\n      0\n      0\n      2\n      3\n      31\n      12\n      NaN\n      0.00\n      2056\n      33\n      ...\n      10000.0\n      0.7715\n      12\n      12\n      7.5\n      0\n      14\n      10.000000\n      NaN\n      196\n    \n    \n      750\n      0\n      0\n      0\n      0\n      43\n      12\n      NaN\n      0.00\n      2383\n      43\n      ...\n      9952.0\n      0.7515\n      10\n      3\n      7.5\n      0\n      4\n      9.952000\n      NaN\n      16\n    \n    \n      751\n      0\n      0\n      0\n      0\n      60\n      12\n      NaN\n      0.00\n      1705\n      55\n      ...\n      24984.0\n      0.6215\n      12\n      12\n      14.0\n      1\n      15\n      24.983999\n      NaN\n      225\n    \n    \n      752\n      0\n      0\n      0\n      3\n      39\n      9\n      NaN\n      0.00\n      3120\n      48\n      ...\n      28363.0\n      0.6915\n      7\n      7\n      11.0\n      1\n      12\n      28.363001\n      NaN\n      144\n    \n  \n\n753 rows × 22 columns\n\n\n\nThis dataset contains the data from T. A. Mroz (1987), “The Sensitivity of and Empirical Model of Married Women’s Hours of Work to Economic and Statistical Assumptions”, Econometrica 55, 765-799, and we have stored it as an object called mroz. We can inspect this element to see how the data has been imported:\n\ntype(mroz)\n\npandas.core.frame.DataFrame\n\n\nseeing that, conveniently, it has been stored as a data frame. We can also look at a quick snapshot of the data by using the head() method, which will print just the first few rows of our data:\n\nmroz.head()\n\n\n\n\n\n  \n    \n      \n      inlf\n      hours\n      kidslt6\n      kidsge6\n      age\n      educ\n      wage\n      repwage\n      hushrs\n      husage\n      ...\n      faminc\n      mtr\n      motheduc\n      fatheduc\n      unem\n      city\n      exper\n      nwifeinc\n      lwage\n      expersq\n    \n  \n  \n    \n      0\n      1\n      1610\n      1\n      0\n      32\n      12\n      3.3540\n      2.65\n      2708\n      34\n      ...\n      16310.0\n      0.7215\n      12\n      7\n      5.0\n      0\n      14\n      10.910060\n      1.210154\n      196\n    \n    \n      1\n      1\n      1656\n      0\n      2\n      30\n      12\n      1.3889\n      2.65\n      2310\n      30\n      ...\n      21800.0\n      0.6615\n      7\n      7\n      11.0\n      1\n      5\n      19.499981\n      0.328512\n      25\n    \n    \n      2\n      1\n      1980\n      1\n      3\n      35\n      12\n      4.5455\n      4.04\n      3072\n      40\n      ...\n      21040.0\n      0.6915\n      12\n      7\n      5.0\n      0\n      15\n      12.039910\n      1.514138\n      225\n    \n    \n      3\n      1\n      456\n      0\n      3\n      34\n      12\n      1.0965\n      3.25\n      1920\n      53\n      ...\n      7300.0\n      0.7815\n      7\n      7\n      5.0\n      0\n      6\n      6.799996\n      0.092123\n      36\n    \n    \n      4\n      1\n      1568\n      1\n      2\n      31\n      14\n      4.5918\n      3.60\n      2000\n      32\n      ...\n      27300.0\n      0.6215\n      12\n      14\n      9.5\n      1\n      7\n      20.100058\n      1.524272\n      49\n    \n  \n\n5 rows × 22 columns\n\n\n\nDepending on the way that we are working with Python, we may also be able to explore the data more interactively if we have some data viewer available."
  },
  {
    "objectID": "03_01_IntroductionPython.html#tidying-data",
    "href": "03_01_IntroductionPython.html#tidying-data",
    "title": "An Introduction to Python",
    "section": "Tidying data",
    "text": "Tidying data\nOnce we have loaded data in Python, we will likely want to manipulate the data in a number of ways. This can include relatively simple things like sub-setting to certain columns or rows (which we have already encountered above), but also more complicated things like aggregating data to a higher level, calculating statistics for one or mutliple variable(s) over other variables, joining together a number of datasets, or “reshaping” data in certain ways. Here we will provide a very brief overview of these procedures, and introduce other things as we need them thoughout the book.\nData manipulation in data frames will largely depend upon Panda’s functions and methods. We have seen examples of methods above, namely loc() and iloc(), and head(). Methods are functions which are associated with an object and which can be called from the object, which will work in a specific way depending on the type of object this is. We have seen above that data frames have methods, but methods can also be applied to columns within data frames. As an example, we can consider describe which can be applied to a column of a data frame to generate descriptive statistics:\n\nmroz.wage.describe()\n\ncount    428.000000\nmean       4.177682\nstd        3.310282\nmin        0.128200\n25%        2.262600\n50%        3.481900\n75%        4.970750\nmax       25.000000\nName: wage, dtype: float64\n\n\nYou will note here that we have used a new way to refer to an entire column of a data frame, namely using its column name. In order to see the range of methods which can be applied to an object you can use the dir() function. For example if you want to know the methods which can be applied to a data frame you can type:\ndir(mroz)\nwhich will provide you a long list of methods.\n\nAggregating Data\nTo start with some basic data manipulations, we will consider data aggregation. For example, imagine that we wished to turn our individual level data frame we imported as mroz above into some group-level averages. We can do this using as we have seen previously, we have been able to generate summary stats, and if we are interested in particular in the mean, example, we can use the mean() method. Below, we will calculate the mean number of children aged less than six in our data:\n\nmroz.kidslt6.mean()\n\n0.23771580345285526\n\n\nIndeed, here with summarise we have requested that it provide us with the mean of this variable, but we could just as easily request some other statistic such as std, min, max, median, quantile etc. It is quite simple to concatenate operations to conduct a range of operations. For example, imagine that if instead of wishing to simply know the mean number of children less than 6 in our data, we wanted to know this for women in the labour market and those out of the labour market. We could easily add this distinction using the groupby method. We will also name the resulting data frame meankidslt6:\n\nmeankidslt6 = mroz.groupby(['inlf']).kidslt6.mean()\nmeankidslt6\n\ninlf\n0    0.366154\n1    0.140187\nName: kidslt6, dtype: float64\n\n\nThis process of data aggregation can be considerably extended depending on what we require, for example working with multiple levels over which to group, generating a larger range of statistics, and so forth.\n\n\nMerging/Joining Data\nWe will also, at times, wish to join two (or more) pieces of data together, for example “merging” data frames or tibbles by a column or columns. For example, it may be that we have a survey where each individuals surveyed belongs to a household, where all of the individual-level questions in the data are stored as an individual-level file (along with the individual’s household ID), while the household level measures are stored in another file at the household level. If we wish to work with information on the individual and their household characteristics, we will need to “join”, or “merge” the file using the household ID.\nWe can see an example of this using the join() method. A simple example is if we would like to take the grouped-level statistics generated previously, and join them back into our original mroz data. What we want to do then, is for each individual who is in the labour force (inlf==1), join the information on the mean number of children under 6 in this group to their row of data, and similarly, for individuals not in the labour force (inlf==0), incorporate the group-level value to their row.\n\nmroz_kids_mean = mroz.groupby(['inlf']).kidslt6.mean()\nmroz_mod = mroz.join(mroz_kids_mean, on = ['inlf'], how = 'left', rsuffix = '_Mean')\nmroz_mod.loc[:,['inlf', 'kidslt6', 'kidslt6_Mean']].head()\n\n\n\n\n\n  \n    \n      \n      inlf\n      kidslt6\n      kidslt6_Mean\n    \n  \n  \n    \n      0\n      1\n      1\n      0.140187\n    \n    \n      1\n      1\n      0\n      0.140187\n    \n    \n      2\n      1\n      1\n      0.140187\n    \n    \n      3\n      1\n      0\n      0.140187\n    \n    \n      4\n      1\n      1\n      0.140187\n    \n  \n\n\n\n\n\nmroz_mod.loc[:,['inlf', 'kidslt6', 'kidslt6_Mean']].tail()\n\n\n\n\n\n  \n    \n      \n      inlf\n      kidslt6\n      kidslt6_Mean\n    \n  \n  \n    \n      748\n      0\n      0\n      0.366154\n    \n    \n      749\n      0\n      2\n      0.366154\n    \n    \n      750\n      0\n      0\n      0.366154\n    \n    \n      751\n      0\n      0\n      0.366154\n    \n    \n      752\n      0\n      0\n      0.366154\n    \n  \n\n\n\n\nHere we have used the join() method to conduct our merge, and specified how='left'. In this particular setting, we could have used any if the various join modes which can be left, right, inner or outer. This is because in this case, all of the levels of “inlf” which are in one set of data are also in the other set of data. In many cases this may not be the case, and we may have observations in one dataset that are not in the other. In this case, we must be careful to indicate whether we wish to keep observations that appear in both sets of data (inner), keep those that only appear in the left-hand side of the command (left), those that only appear in the right-hand side of the command (right), or those appearing in either data set (outer). Note that here when we refer to the left-hand side and the right-hand side, we refer to the data frame indicated before .join( as the left-hand side, and that which appears after as the right-hand side.\nWhile there is some mechanics in setting this up, once we have understood the logic of group by and join it would be exceedingly easy to generalise this, for example if we wished to instead have the aggregates produced by labour market status and the individual’s age (or any other less aggregated level). It would be worth exploring such generalisations in the code above to ensure that you are happy with the way things are working.\n\n\nFiltering Data\nPreviously we have discussed how to select particular columns or rows of data, however there are other ways which we can do this. For example, the filter method allows us to filter columns or rows that meet certain condition in their labels. For example selecting columns:\n\nmroz.filter(items = [\"kidslt6\", \"age\"])\n\n\n\n\n\n  \n    \n      \n      kidslt6\n      age\n    \n  \n  \n    \n      0\n      1\n      32\n    \n    \n      1\n      0\n      30\n    \n    \n      2\n      1\n      35\n    \n    \n      3\n      0\n      34\n    \n    \n      4\n      1\n      31\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      748\n      0\n      40\n    \n    \n      749\n      2\n      31\n    \n    \n      750\n      0\n      43\n    \n    \n      751\n      0\n      60\n    \n    \n      752\n      0\n      39\n    \n  \n\n753 rows × 2 columns\n\n\n\nPerhaps more useful, we can also filter on observations values. This is done using logical indexing\n\nmroz[mroz.kidslt6 == 3]\n\n\n\n\n\n  \n    \n      \n      inlf\n      hours\n      kidslt6\n      kidsge6\n      age\n      educ\n      wage\n      repwage\n      hushrs\n      husage\n      ...\n      faminc\n      mtr\n      motheduc\n      fatheduc\n      unem\n      city\n      exper\n      nwifeinc\n      lwage\n      expersq\n    \n  \n  \n    \n      483\n      0\n      0\n      3\n      1\n      31\n      13\n      NaN\n      0.0\n      3600\n      36\n      ...\n      73600.0\n      0.4415\n      12\n      16\n      11.0\n      0\n      3\n      73.599998\n      NaN\n      9\n    \n    \n      604\n      0\n      0\n      3\n      3\n      37\n      13\n      NaN\n      0.0\n      2419\n      39\n      ...\n      13440.0\n      0.7715\n      7\n      0\n      5.0\n      0\n      4\n      13.440000\n      NaN\n      16\n    \n    \n      714\n      0\n      0\n      3\n      0\n      31\n      15\n      NaN\n      0.0\n      3000\n      34\n      ...\n      51200.0\n      0.4415\n      7\n      10\n      7.5\n      1\n      5\n      51.200001\n      NaN\n      25\n    \n  \n\n3 rows × 22 columns\n\n\n\n\n\nGenerating variables\nIn order to create new columns, Pandas automatically understands that when we include a name that is not a part of the data frame, that we wish to generated a new column of data. For example, in the below we will generate a new variable, which is experience squared:\n\nmroz[\"exper2\"] = mroz.exper**2\nmroz.head()\n\n\n\n\n\n  \n    \n      \n      inlf\n      hours\n      kidslt6\n      kidsge6\n      age\n      educ\n      wage\n      repwage\n      hushrs\n      husage\n      ...\n      mtr\n      motheduc\n      fatheduc\n      unem\n      city\n      exper\n      nwifeinc\n      lwage\n      expersq\n      exper2\n    \n  \n  \n    \n      0\n      1\n      1610\n      1\n      0\n      32\n      12\n      3.3540\n      2.65\n      2708\n      34\n      ...\n      0.7215\n      12\n      7\n      5.0\n      0\n      14\n      10.910060\n      1.210154\n      196\n      196\n    \n    \n      1\n      1\n      1656\n      0\n      2\n      30\n      12\n      1.3889\n      2.65\n      2310\n      30\n      ...\n      0.6615\n      7\n      7\n      11.0\n      1\n      5\n      19.499981\n      0.328512\n      25\n      25\n    \n    \n      2\n      1\n      1980\n      1\n      3\n      35\n      12\n      4.5455\n      4.04\n      3072\n      40\n      ...\n      0.6915\n      12\n      7\n      5.0\n      0\n      15\n      12.039910\n      1.514138\n      225\n      225\n    \n    \n      3\n      1\n      456\n      0\n      3\n      34\n      12\n      1.0965\n      3.25\n      1920\n      53\n      ...\n      0.7815\n      7\n      7\n      5.0\n      0\n      6\n      6.799996\n      0.092123\n      36\n      36\n    \n    \n      4\n      1\n      1568\n      1\n      2\n      31\n      14\n      4.5918\n      3.60\n      2000\n      32\n      ...\n      0.6215\n      12\n      14\n      9.5\n      1\n      7\n      20.100058\n      1.524272\n      49\n      49\n    \n  \n\n5 rows × 23 columns\n\n\n\nIf we instead include a variable name which is already present, any existing information will be replaced with the new values we assign.\n\n\nReshaping Data\nFinally the last tool that we introduce for data manipulation is the idea of “reshaping” or transforming data from a wide format to a long format (and vice versa). We refer to data in a long format if for each observation we have multiple rows of data, covering different levels of some other variable. For example, a classic example of this would be a panel of data over countries and years. On the other hand, we refer to data as being in a wide format if for each observation we have multiple columns (or variables) which cover different levels of some other variable. In certain settings we will wish to work with data in one format, and in other settings we may find it more convenient to work with data in another format. To make this a bit clearer, consider the below data frame:\n\nData = pd.DataFrame({'Zone': [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\"], 'Year': [2020, 2021, 2022, 2020, 2021, 2022], 'Sales': [90, 100, 115, 82, 98, 106]})\nData\n\n\n\n\n\n  \n    \n      \n      Zone\n      Year\n      Sales\n    \n  \n  \n    \n      0\n      A\n      2020\n      90\n    \n    \n      1\n      A\n      2021\n      100\n    \n    \n      2\n      A\n      2022\n      115\n    \n    \n      3\n      B\n      2020\n      82\n    \n    \n      4\n      B\n      2021\n      98\n    \n    \n      5\n      B\n      2022\n      106\n    \n  \n\n\n\n\nHere we can see that data is in the long format, with multiple observations for one unit (zone in this example) over a number of years. To convert this data to wide format we can use the pivot method, as follows:\n\nData = Data.pivot(index = 'Zone', columns = 'Year', values = 'Sales')\nData\n\n\n\n\n\n  \n    \n      Year\n      2020\n      2021\n      2022\n    \n    \n      Zone\n      \n      \n      \n    \n  \n  \n    \n      A\n      90\n      100\n      115\n    \n    \n      B\n      82\n      98\n      106\n    \n  \n\n\n\n\nAs you can see now there is now one observation per unit, where each observation has three resulting columns (one for each year). Now, if we wish to return to the long format we can do so with the converse melt function:\n\nData = Data.melt(value_vars=[2020,2021,2022], value_name = 'Sales')\nData\n\n\n\n\n\n  \n    \n      \n      Year\n      Sales\n    \n  \n  \n    \n      0\n      2020\n      90\n    \n    \n      1\n      2020\n      82\n    \n    \n      2\n      2021\n      100\n    \n    \n      3\n      2021\n      98\n    \n    \n      4\n      2022\n      115\n    \n    \n      5\n      2022\n      106\n    \n  \n\n\n\n\nIn practice, we will generally use these commands with a considerably larger number of units, but the basic principles are the same regardless."
  },
  {
    "objectID": "03_02_Chapter2Python.html",
    "href": "03_02_Chapter2Python.html",
    "title": "3.2 – Details required for Chapter 2",
    "section": "",
    "text": "Preliminaries\nWe will begin by examining a number of details which will be required during the exercises encountered in this chapter. This includes regression, simulation with pseudo random numbers, and some basic graphing procedures.\nTo start, we will simulate some data based on the following data generating process:\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\)\nwhere \\(\\beta_0=1\\), \\(\\beta_1=2\\), and both \\(x_i\\) and \\(\\varepsilon_i\\) are distributed \\(\\mathcal{N}(0,1)\\). Having conducted this simulation, we will estimate a regression model to estimate \\(\\widehat\\beta_1\\). In the book, you will be asked to consider examples which are more appropriate for the treatment effects framework which we are considering.\nFinally, we will do this 500 times, to see what the distribution of estimated paramters \\(\\widehat\\beta_1\\) looks like:\nRandomization inference, although a theoretical concept, is best illustrated with practical examples. A particularly illustrative approach is visualization through tabular permutation. The following online coding resource provides a detailed introduction to this method. In this context, we will work with data from the study “Long-Term effects of the Targeting the Ultra Poor Program” conducted by Abhijit Banerjee, Esther Duflo, and Garima Sharma.\nHere we will work with data from Banerjee, Duflo, and Sharma (2021)"
  },
  {
    "objectID": "03_02_Chapter2Python.html#an-exact-p-value",
    "href": "03_02_Chapter2Python.html#an-exact-p-value",
    "title": "3.2 – Details required for Chapter 2",
    "section": "An Exact p-value",
    "text": "An Exact p-value\nIt is perhaps useful to see a simple example. Consider the case of 6 units, with 3 observations randomly assigned treatment. Imagine that the observed outcomes were then, in the treatment group: \\((34,27,29)\\), and in the control group: \\((14,18,24)\\). A simple comparison of means estimator suggests that the treatment effect is 11.33. To calculate a p-value, we can permute all the possible combinations, and ask what proportion of these are greater than or equal to this treatment effect. If we consider random orderings of 6 units, this suggests that there are \\(6!\\) possible combinations, but in reality, as we are randomly choosing 3 units from these 6 to assign a permuted treatment status, the actual value of different combinations is \\(6\\choose 3\\) \\(=\\frac{6!}{3!*(6-3)!}=20\\). We document each of these possible permutations, as well as their permuted treatment effect in the Table below. In this case, we can see that only 1 of the 20 different permutations is greater than or equal to 11.33 he original treatment status). Suggesting an exact p-value of \\(1/20=0.05\\).\n\nA Simple Illustration of Randomization Inference {.striped .hover .borderless .secondary}\n\n\nPermutation\nT1\nT2\nT3\nC1\nC2\nC3\nEstimate\n\n\n\n\nOriginal (1)\n34\n27\n29\n14\n18\n24\n11.33\n\n\n2\n34\n27\n14\n29\n18\n24\n1.33\n\n\n3\n34\n27\n18\n14\n29\n24\n4\n\n\n4\n34\n27\n24\n14\n18\n29\n8\n\n\n5\n34\n14\n29\n27\n18\n24\n2.67\n\n\n6\n34\n18\n29\n14\n27\n24\n5.33\n\n\n7\n34\n24\n29\n14\n18\n27\n9.33\n\n\n8\n14\n27\n29\n34\n18\n24\n-2\n\n\n9\n18\n27\n29\n14\n34\n24\n0.67\n\n\n10\n24\n27\n29\n14\n18\n34\n4.67\n\n\n11\n34\n14\n18\n27\n29\n24\n-4.67\n\n\n12\n34\n14\n24\n27\n18\n29\n-0.67\n\n\n13\n34\n18\n24\n14\n27\n29\n2\n\n\n14\n14\n27\n18\n34\n29\n24\n-9.33\n\n\n15\n14\n27\n24\n34\n18\n29\n-5.33\n\n\n16\n18\n27\n24\n14\n34\n29\n-2.67\n\n\n17\n14\n18\n29\n34\n27\n24\n-8\n\n\n18\n14\n24\n29\n34\n18\n27\n-4\n\n\n19\n18\n24\n29\n14\n34\n27\n-1.33\n\n\n20\n14\n18\n24\n34\n27\n29\n-11.33\n\n\n\nWe will set this up in Python. First, we will load the required libraries:\n\n#Load required libraries\nimport pandas as pd\nimport numpy as np\nfrom itertools import permutations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nWe will now enter data as a Pandas data frame, and subsequently calculate the difference in means estimator in a number of lines. You will note that in calculating the differnce in means estimator, we are first sub-setting using logical indexing (for example: data[data['W']==1], which means “choose all rows of data for which \\(W=1\\)”). Then, we calculate the means in each group using the Pandas mean() operator.\n\n#Enter data as Pandas data frame\ndata = {'Y': [34, 27, 29, 14, 18, 24],\n        'W': [1, 1, 1, 0, 0, 0]}\ndata = pd.DataFrame(data)\n\n\n#Calculate Difference in means estimator  \nY1 = data[data['W']==1]['Y']\nY0 = data[data['W']==0]['Y']\ntau = Y1.mean() - Y0.mean()\n\n\n#Generate permutations of W\nperm=permutations([1,1,1,0,0,0])\n#Wperm=perm\nWperm = set(perm)\n\nTaus = []\nfor p in Wperm:\n    dataP = pd.DataFrame({'Y': [34, 27, 29, 14, 18, 24], 'W': p})\n    tauP = dataP[dataP['W']==1]['Y'].mean()-dataP[dataP['W']==0]['Y'].mean()\n    Taus.append(tauP)\n\n\np_2side = sum(np.absolute(Taus)>=tau)/len(Taus)\np_1side = sum(Taus>=tau)/len(Taus)\n\nprint(\"The two-sided p-value is: \" + str(p_2side))\n\nThe two-sided p-value is: 0.1\n\n\n\n#Generate graph\nsns.set_palette(\"pastel\")\nplt.hist(Taus,bins=10, edgecolor='black', density=True, label=\"Permutations\")\nplt.ylabel(\"Density\")\nplt.xlabel(\"Test statistic\")\nplt.axvline(tau, color='red', linestyle='dashed', label=r'$\\widehat\\tau$')\nplt.legend() \nplt.show()\n\n\n\n\nFigure 14.1: Permutation inference"
  },
  {
    "objectID": "03_02_Chapter2Python.html#randomization-inference-with-a-larger-dataset",
    "href": "03_02_Chapter2Python.html#randomization-inference-with-a-larger-dataset",
    "title": "3.2 – Details required for Chapter 2",
    "section": "Randomization inference with a larger dataset",
    "text": "Randomization inference with a larger dataset\nHere we will return to the example from Banerjee, Duflo, and Sharma (2021). ### Block 2.2\n\nimport pandas as pd\nimport numpy as np\n\n# Read the data\ndata = pd.read_csv(\"Datasets/Banerjee_et_al_2021.csv\")\n\n# Function to perform randomization inference\ndef randomization_inference(data, column_name):\n    # Observed treatment effect\n    treated = data[data['treatment'] == 1]\n    control = data[data['treatment'] == 0]\n    obs_effect = treated[column_name].mean() - control[column_name].mean()\n\n    # Randomization inference\n    n_permutations = 10000\n    permuted_effects = np.zeros(n_permutations)\n    for i in range(n_permutations):\n        permuted_treatment = np.random.permutation(data['treatment'].values)\n        treated_effect = data[column_name][permuted_treatment == 1].mean()\n        control_effect = data[column_name][permuted_treatment == 0].mean()\n        permuted_effects[i] = treated_effect - control_effect\n\n    # Calculate p-value\n    p_value = np.mean(np.abs(permuted_effects) >= np.abs(obs_effect))\n    return {\"observed_effect\": obs_effect, \"p_value\": p_value}\n\n# Function to print results in a descriptive manner\ndef print_results(index_name, results):\n    significance = \"significative\" if results['p_value'] < 0.05 else \"not significative\"\n    print(f\"The observed effect of {index_name} is {results['observed_effect']:.4f} and it's p-value is {results['p_value']:.4f}, which is {significance}.\")\n\n# Financial Index\nfinancial_results = randomization_inference(data, 'ind_fin_el1')\nprint_results(\"Financial Index\", financial_results)\n\n# Asset Index\nasset_results = randomization_inference(data, 'asset_ind_tot_el1')\nprint_results(\"Asset Index\", asset_results)\n\nThe observed effect of Financial Index is -0.0096 and it's p-value is 0.8198, which is not significative.\n\n\nThe observed effect of Asset Index is 0.4078 and it's p-value is 0.0002, which is significative.\n\n\n\nPreserve the original treatment assignment.\nGenerate placebo treatment statuses according to the original assignment method.\nEstimate the original regression equation with an additional term for the placebo treatment.\nRepeat #1–3.\nThe randomization inference p-value is the proportion of times the placebo treatment effect was larger than the estimated treatment effect."
  },
  {
    "objectID": "03_02_Chapter2Python.html#dehejia-and-wahba",
    "href": "03_02_Chapter2Python.html#dehejia-and-wahba",
    "title": "3.2 – Details required for Chapter 2",
    "section": "Dehejia and Wahba",
    "text": "Dehejia and Wahba\nIn observational studies, where treatment assignment is not random, estimating causal effects can be challenging due to potential confounding factors. One method to address this challenge is Propensity Score Matching (PSM). PSM aims to control for observed confounding by matching treated units with untreated units that have similar propensity scores. The propensity score for a unit is the probability of receiving the treatment given observed covariates. By matching on propensity scores, we aim to create a scenario where the distribution of observed covariates is similar between the treated and untreated groups, mimicking a randomized experiment. This method allows to estimate causal treatment effects in observational settings, making it a valuable tool in microeconometrics.\nYou have data from an observational study on a job training program. The dataset contains information on individuals’ participation in the program (treat), their earnings in 1978 (re78), and several other covariates such as age, education, race, marital status, and earnings in 1974 and 1975. The main objective of this exercise is to estimate the Average Treatment Effect on the Treated (ATT) of the job training program on earnings in 1978 using Propensity Score Matching.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import NearestNeighbors\n\n# Load the dataset\ndata = pd.read_stata('Datasets/Dehejia_Wahba_1999.dta')\n\n# Define the covariates and the treatment variable\nX = data[['age', 'education', 'black', 'hispanic', 'married', 'nodegree', 're74', 're75']]\ny = data['treat']\n\n# Estimate propensity scores using logistic regression\nlogit = LogisticRegression(max_iter=1000)\nlogit.fit(X, y)\ndata['propensity_score'] = logit.predict_proba(X)[:, 1]\n\n# Perform matching\ntreated = data[data.treat == 1]\nuntreated = data[data.treat == 0]\nneigh = NearestNeighbors(n_neighbors=1)\nneigh.fit(untreated[['propensity_score']])\nindices = neigh.kneighbors(treated[['propensity_score']], return_distance=False)\nmatched = untreated.iloc[indices.flatten()]\n\n# Reset indexes for treated and matched DataFrames\ntreated = treated.reset_index(drop=True)\nmatched = matched.reset_index(drop=True)\n\n# Diagnostic Checks\n# 1. Check for Missing Values\nprint(data[['age', 'education', 'black', 'hispanic', 'married', 'nodegree', 're74', 're75', 're78']].isnull().sum())\n\n# 2. Check Matching Results\nprint(\"Number of treated individuals:\", len(treated))\nprint(\"Number of matched untreated individuals:\", len(matched))\n\n# 3. Check Propensity Scores\nprint(\"Unique propensity scores:\", data['propensity_score'].nunique())\n\n# Calculate ATT\nATT = (treated['re78'] - matched['re78']).mean()\nprint(f\"Average Treatment Effect on the Treated (ATT): {ATT}\")\n\nage          0\neducation    0\nblack        0\nhispanic     0\nmarried      0\nnodegree     0\nre74         0\nre75         0\nre78         0\ndtype: int64\nNumber of treated individuals: 185\nNumber of matched untreated individuals: 185\nUnique propensity scores: 336\nAverage Treatment Effect on the Treated (ATT): 1454.528076171875\n\n\n\n\n\n\nBanerjee, Abhijit, Esther Duflo, and Garima Sharma. 2021. “Long-Term Effects of the Targeting the Ultra Poor Program.” American Economic Review: Insights 3 (4): 471–86. https://doi.org/10.1257/aeri.20200667."
  },
  {
    "objectID": "03_03_Chapter3Python.html",
    "href": "03_03_Chapter3Python.html",
    "title": "Details required for Chapter 3",
    "section": "",
    "text": "Here will be the Chapter 3 excersises and examples."
  },
  {
    "objectID": "06_references.html",
    "href": "06_references.html",
    "title": "References",
    "section": "",
    "text": "Athey, Susan, and Guido W. Imbens. 2022. “Design-based analysis in Difference-In-Differences\nsettings with staggered adoption.” Journal of\nEconometrics 226 (1): 62–79. https://doi.org/https://doi.org/10.1016/j.jeconom.2020.10.012.\n\n\nChaisemartin, Clément de, and Xavier D’Haultfoeuille. 2020.\n“Two-Way Fixed Effects Estimators with Heterogeneous Treatment\nEffects.” American Economic Review 110 (9): 2964–96. https://doi.org/10.1257/aer.20181169.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with\nVariation in Treatment Timing.” Journal of Econometrics\n225 (2): 254–77. https://doi.org/https://doi.org/10.1016/j.jeconom.2021.03.014."
  },
  {
    "objectID": "01_04_Chapter4R.html#tools-required-for-chapter-4",
    "href": "01_04_Chapter4R.html#tools-required-for-chapter-4",
    "title": "Chapter 4",
    "section": "Tools required for Chapter 4",
    "text": "Tools required for Chapter 4"
  },
  {
    "objectID": "01_04_Chapter4R.html#code-call-out-4.1",
    "href": "01_04_Chapter4R.html#code-call-out-4.1",
    "title": "Chapter 4",
    "section": "Code Call Out 4.1",
    "text": "Code Call Out 4.1\nTwo-Way Fixed Effects Estimators and Heterogeneous Treatment Effects To understand the potential issues related to heterogeneous treatment effects over time and two-way fixed effect estimators, we will examine a pair of numerical examples. In particular, we will focus on the composition of the two way FE estimator \\(\\tau\\) estimated from: \\[\ny_{st} = \\gamma_s + \\lambda_t + \\tau w_{st} + \\varepsilon_{st}\n\\tag{6.1}\\] where \\(y_{st}\\) is the outcome variable, \\(\\gamma_s\\) and \\(\\lambda_t\\) are state (unit) and time fixed effects, \\(w_{st}\\) is the binary treatment variable that takes the value of 1 if a state (unit) \\(s\\) is treated at time \\(t\\) and otherwise takes 0. We will work with a quite tractable example based on three units and 10 time periods, and will document how the approaches taken by Goodman-Bacon (2021) and by Chaisemartin and D’Haultfoeuille (2020) to understand the two-way FE estimator compare.\nThe results from Goodman-Bacon (2021) and those from Chaisemartin and D’Haultfoeuille (2020) are similar, however they take quite different paths to get there. Goodman-Bacon’s (like that laid out in Athey and Imbens (2022)) is “mechanical” in that it is based on the underlying difference-in-differences comparisons between all groups. The result in Chaisemartin and D’Haultfoeuille (2020) is based on a potential outcomes frame-work, and counterfactuals under parallel trend assumptions. Thus to examine how these methods work requires somewhat different frameworks. In the case of Goodman-Bacon (2021), we should consider all possible DD comparisons, while in the case of Chaisemartin and D’Haultfoeuille (2020) we should consider the treatment effect for each unit and time period, which requires knowing the observed and counterfactual state. While the approaches the two papers take to understand the content of the estimator differ, they refer to the same estimator, so always recover the same parameter estimate. To examine this in a more applied way, we will look at a simulated example.\nTo do this, let’s consider a panel of 3 states/areas over the 10 years (\\(t\\)) of 2000 to 2009. One of these units is entirely untreated (\\(unit = 1\\) or group \\(U\\)), one is treated at an early time period, 2003, (\\(unit = 2\\) or group \\(k\\)), and the other is treated at a later time period, 2006, (\\(unit = 3\\) or group \\(l\\)). We will construct a general structure for this data below:\n\nData <- data.frame(unit = ceiling(1:30/10), year = rep(2000:2009, 3))\nhead(Data)\n\n  unit year\n1    1 2000\n2    1 2001\n3    1 2002\n4    1 2003\n5    1 2004\n6    1 2005\n\n\nWe will consider a simple-case where the actual data-generating process is known as: \\[y_{unit,t} = 2 + 0.2 \\times (t - 2000) + 1 \\times unit + \\beta_1 \\times post \\times unit + \\beta_2 \\times post \\times unit \\times (t - treat).\\] Here \\(unit\\) refers to the unit number listed above (1, 2 or 3), \\(post\\) indicates that a unit is receiving treatment in the relevant time period \\(t\\), and \\(treat\\) refers to the treatment period (2003 for unit 2, and 2006 for unit 3). Let’s generate treatment, time to treatment, and post-treatment variables in R:\n\nData$treat <- ifelse(Data$unit == 2, 2006, ifelse(Data$unit == 3, 2003, 0))\nData$time  <- ifelse(Data$treat == 0, 0, Data$year - Data$treat)\nData$post  <- ifelse(Data$time >= 0 & Data$treat != 0, 1, 0)\n\nThis specification allows for each unit to have its own fixed effect, given that \\(unit\\) is multiplied by 1, and allows for a general time trend increasing by 0.2 units each period across the whole sample. These parameters are not so important, as what we care about are the treatment effects themselves. The impact of treatment comes from the units \\(\\beta_1\\) and \\(\\beta_2\\). The first of these, \\(\\beta_1\\), captures an immediate unit-specific jump when treatment is implemented which remains stable over time. The second of these, \\(\\beta_2\\), implies a trend break occurring only for the treated units once treatment comes into place. We will consider 2 cases below. In the first case \\(\\beta_1 = 1\\) and \\(\\beta_2 = 0\\) (a simple case with a constant treatment effect per unit):\n\nData$y1 <- 2 + (Data$year - 2000) * 0.2 + 1 * Data$unit + 1 * Data$post * Data$unit + \n  0 * Data$post * Data$unit * (Data$time)\n\nand in a second case \\(\\beta_1 = 1\\) and \\(\\beta_2 = 0.45\\). This is a more complex case in which there are heterogeneous treatment effects over time:\n\nData$y2 <- 2 + (Data$year - 2000) * 0.2 + 1 * Data$unit + 1 * Data$post * Data$unit +\n  0.45 * Data$post * Data$unit * (Data$time)\n\nThese two cases are plotted next where the line with empty circles refers to group \\(U\\), the line with black filled circles refers to group \\(k\\) and the line with squares refers to group \\(l\\)\n\n\nShow the plot code\nlibrary(ggplot2)\nlibrary(ggpubr)\np1 <- ggplot(data = Data, aes(x = year, y = y1, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5) +\n  geom_point(aes(shape = as.factor(unit)), size = 2) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 12, by = 2),\n                     labels = seq(from = 0, to = 12, by = 2),\n                     limits = c(0,12)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\nt1 <- ggplot() + geom_text(aes(x = 0, y = 0, label = \"(a) Simple Decomposition\")) +\n  theme_void()\np2 <- ggplot(data = Data, aes(x = year, y = y2, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5) +\n  geom_point(aes(shape = as.factor(unit)), size = 2) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 20, by = 5),\n                     labels = seq(from = 0, to = 20, by = 5),\n                     limits = c(0,20)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\nt2 <- ggplot() + geom_text(aes(x = 0, y = 0, label = \"(b) Decomposition with trends\")) +\n  theme_void()\nggarrange(plotlist = list(p1, p2, t1, t2), ncol = 2, nrow = 2, heights = c(0.9, 0.1))\n\n\n\n\n\n\n\n\n\nThe Two-way Fixed Effect Estimator\nFirst we will estimate the parameter by two-way fixed effects regression. This will provide us with the parameter estimate that both Goodman-Bacon (2021) and Chaisemartin and D’Haultfoeuille (2020) will construct in a piece-wise fashion. This is done relatively simply in R. We simply estimate Equation 6.1 by linear regression using lm as laid out below:\n\ncase1 <- lm(data = Data,\n            formula = y1 ~ factor(unit) + factor(year) + post)\npaste0(\"The parameter estimates by two-way fixed effects regression for the \",\n       \"case 1 is: \", case1$coefficients[\"post\"])\n\n[1] \"The parameter estimates by two-way fixed effects regression for the case 1 is: 2.45454545454545\"\n\ncase2 <- lm(data = Data,\n            formula = y2 ~ factor(unit) + factor(year) + post)\npaste0(\"The parameter estimates by two-way fixed effects regression for the \",\n       \"case 2 is: \", case2$coefficients[\"post\"])\n\n[1] \"The parameter estimates by two-way fixed effects regression for the case 2 is: 3.80454545454545\"\n\n\nHere we see that the coefficient of interest is 2.454545. We can see that this is between the two unit-specific jumps that occur with treatment (2 and 3). We will see below why it takes this particular weighted average.\n\n\nGoodman-Bacon (2021) Decomposition\nUsing the values simulated above, let’s see how the Goodman-Bacon (2021) decomposition allows us to understand estimated treatment effects. We will consider both:\n- (a) Simple Decomposition\n- (b) Decomposition with trends\nThe methodology Goodman-Bacon (2021) decomposition suggests that we should calculate all \\(2 \\times 2\\) combinations of states and time where post-treatment units are compared to “untreated” unit (laid out at more length in the boo). In this example, this provides four specific effects, which contribute to \\(\\widehat{\\tau}\\) as a weighted mean. The specific effects desired are:\n\nA. \\(\\widehat{\\beta}^{2\\times2}_{kU}\\) from the comparison of the early treated unit with the untreated unit.\n\nB. \\(\\widehat{\\beta}^{2\\times2}_{lU}\\), from the comparison of the latter treated unit with the untreated unit.\n\nC. \\(\\widehat{\\beta}^{2\\times2,k}_{kl}\\), from the comparison of the early and latter treated units, when the early unit begin to be treated.\n\nD. \\(\\widehat{\\beta}^{2\\times2,l}_{kl}\\), from the comparison of the early and latter treated units, when the latter unit begin to be treated.\n\nThese will then be weighted as laid out in Goodman-Bacon (2021) to provide the regression-based estimate.\n\n(a) Simple Decomposition\nIn this case the Goodman-Bacon (2021) methodology estimate \\(\\widehat{\\tau}\\) weighting the next four DD comparisons\n\n\nShow the plot code\nlibrary(dplyr)\np1 <- ggplot(data = Data, aes(x = year, y = y1, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(1,0.1,1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 12, by = 2),\n                     labels = seq(from = 0, to = 12, by = 2),\n                     limits = c(0,12)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\np2 <- ggplot(data = Data, aes(x = year, y = y1, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(1,1,0.1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 12, by = 2),\n                     labels = seq(from = 0, to = 12, by = 2),\n                     limits = c(0,12)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\np3 <- Data %>% filter(year < 2006) %>%\n  ggplot(aes(x = year, y = y1, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(0.1,1,1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2),\n                     limits = c(2000,2009)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 12, by = 2),\n                     labels = seq(from = 0, to = 12, by = 2),\n                     limits = c(0,12)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\np4 <- Data %>% filter(year >= 2003) %>%\n  ggplot(aes(x = year, y = y1, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(0.1,1,1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2),\n                     limits = c(2000,2009)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 12, by = 2),\n                     labels = seq(from = 0, to = 12, by = 2),\n                     limits = c(0,12)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\nt1 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"A. Early Group v/s Untreated Group\"), size = 3) +\n  theme_void()\nt2 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"B. Later Group v/s Untreated Group\"), size = 3) +\n  theme_void()\nt3 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"C. Early Group v/s Later Group Before 2006\"), \n            size = 3) +\n  theme_void()\nt4 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"D. Early Group v/s Later Group After 2003\"), \n            size = 3) +\n  theme_void()\nggarrange(plotlist = list(t1, t2, p1, p2, t3, t4, p3, p4), ncol = 2, nrow = 4, \n          heights = c(0.1, 0.4, 0.1, 0.4))\n\n\n\n\n\n\n\n\nAs seen in the plots, in the simple decomposition these effects are constants of 3 and 2 for early and later treated units given that the “treatment effect” is simply \\(1 \\times unit\\) in each case.\n\nA. Early Group v/s Untreated Group\nIn order to calculate the effects we start making the simple DD comparison of the untreated group \\(U\\) (\\(unit = 1\\)) with the early treated group \\(k\\) (\\(unit = 3\\)) getting \\(\\widehat{\\beta}^{2 \\times 2}_{kU}\\) as \\[\\widehat{\\beta}^{2 \\times 2}_{kU} = \\left( \\overline{y}_k^{Post(k)} - \\overline{y}_k^{Pre(k)} \\right) - \\left( \\overline{y}_U^{Post(k)} - \\overline{y}_U^{Pre(k)} \\right)\\] Where \\(\\overline{y}_k^{Post(k)}\\) is the mean of the outcome variable for the early treated group \\(k\\) (\\(unit = 3\\)) posterior to treatment, from 2003, \\(\\overline{y}_k^{Pre(k)}\\) is the mean for of the outcome variable for the early treated group \\(U\\) (\\(unit = 3\\)) prior to treatment, (up until 2002), and \\(\\overline{y}_U^{Post(k)}, \\overline{y}_U^{Post(k)}\\) are the analogous quantities for the untreated group \\(U\\) (\\(unit = 1\\))\n\n(mean(Data$y1[Data$unit == 3 & Data$post == 1]) -\n   mean(Data$y1[Data$unit == 3 & Data$post == 0])) -\n  (mean(Data$y1[Data$unit == 1 & Data$year >= 2003]) -\n     mean(Data$y1[Data$unit == 1 & Data$year < 2003]))\n\n[1] 3\n\n\nThis result also can be obtained from the linear regression with the canonical DD formula \\[y_{unit,t} = \\alpha_0 + \\alpha_1 \\times Post(k) + \\alpha_2 \\times \\mathbf{1}(unit = 3) + \\beta_{kU}^{2\\times2} \\times Post(k) \\times \\mathbf{1}(unit = 3) + \\varepsilon_i\\] Where \\(Post(k)\\) indicates that the year is equal or greater than the year where the group \\(k\\) (\\(unit = 3\\)) received the treatment (2003) and \\(\\mathbf{1}(unit = 3)\\) indicates if the observation is from the early treated group \\(k\\) (\\(unit = 3\\))\n\nsummary(lm(y1 ~ factor(year >= 2003) + factor(unit) + factor(year >= 2003):factor(unit), \n           data = Data, subset = (unit != 2)))\n\n\nCall:\nlm(formula = y1 ~ factor(year >= 2003) + factor(unit) + factor(year >= \n    2003):factor(unit), data = Data, subset = (unit != 2))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -0.6   -0.2    0.0    0.2    0.6 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                              3.2000     0.2236  14.311 1.54e-10 ***\nfactor(year >= 2003)TRUE                 1.0000     0.2673   3.742  0.00178 ** \nfactor(unit)3                            2.0000     0.3162   6.325 1.01e-05 ***\nfactor(year >= 2003)TRUE:factor(unit)3   3.0000     0.3780   7.937 6.14e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3873 on 16 degrees of freedom\nMultiple R-squared:  0.9804,    Adjusted R-squared:  0.9767 \nF-statistic: 266.1 on 3 and 16 DF,  p-value: 7.35e-14\n\n\nA third way to obtain this is from the next linear regression \\[y_{unit,t} = \\alpha_0 + \\beta_{kU}^{2 \\times 2} \\times Post + \\sum_{i = 2001}^{2009} \\alpha_{i-2000} \\times \\mathbf{1}(year = i) + \\alpha_{10} \\times \\mathbf{1}(unit = 3) + \\varepsilon_i\\] Where in this case \\(Post\\) indicates if the unit is treated (note for group \\(U\\) this will be always 0), \\(\\mathbf{1}(year = i)\\) indicates if the observation is in period \\(i \\in \\{2001, \\ldots, 2009\\}\\) and \\(\\mathbf{1}(unit = 3)\\) keep its meaning\n\nsummary(lm(y1 ~ post + factor(year) + factor(unit), data = Data, subset = (unit != 2)))\n\n\nCall:\nlm(formula = y1 ~ post + factor(year) + factor(unit), data = Data, \n    subset = (unit != 2))\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-2.324e-15 -3.940e-16  0.000e+00  3.940e-16  2.324e-15 \n\nCoefficients:\n                  Estimate Std. Error   t value Pr(>|t|)    \n(Intercept)      3.000e+00  1.217e-15 2.466e+15   <2e-16 ***\npost             3.000e+00  1.454e-15 2.063e+15   <2e-16 ***\nfactor(year)2001 2.000e-01  1.490e-15 1.342e+14   <2e-16 ***\nfactor(year)2002 4.000e-01  1.490e-15 2.684e+14   <2e-16 ***\nfactor(year)2003 6.000e-01  1.658e-15 3.619e+14   <2e-16 ***\nfactor(year)2004 8.000e-01  1.658e-15 4.825e+14   <2e-16 ***\nfactor(year)2005 1.000e+00  1.658e-15 6.031e+14   <2e-16 ***\nfactor(year)2006 1.200e+00  1.658e-15 7.238e+14   <2e-16 ***\nfactor(year)2007 1.400e+00  1.658e-15 8.444e+14   <2e-16 ***\nfactor(year)2008 1.600e+00  1.658e-15 9.650e+14   <2e-16 ***\nfactor(year)2009 1.800e+00  1.658e-15 1.086e+15   <2e-16 ***\nfactor(unit)3    2.000e+00  1.217e-15 1.644e+15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.49e-15 on 8 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 5.002e+30 on 11 and 8 DF,  p-value: < 2.2e-16\n\n\nNow we store this result for posterior use\n\nbku <- lm(y1 ~ post + factor(year) + factor(unit), data = Data,\n          subset = (unit != 2))$coefficient[\"post\"]\n\n\n\nB. Later Group v/s Untreated Group\nThe next DD comparison we calculate is that which compares the later treated group \\(l\\) (\\(unit = 2\\)) with the untreated group \\(U\\) (\\(unit = 1\\)), resulting in \\(\\widehat{\\beta}^{2 \\times 2}_{lU}\\). As above, we can generate this DD estimate in a number of ways (most simply by double-differencing with means), and this will then be stored.\n\nblu <- lm(y1 ~ post + factor(year) + factor(unit), data = Data, \n   subset = (unit != 3))$coefficient[\"post\"]\nblu\n\npost \n   2 \n\n(mean(Data$y1[Data$unit == 2 & Data$post == 1]) -\n   mean(Data$y1[Data$unit == 2 & Data$post == 0])) -\n  (mean(Data$y1[Data$unit == 1 & Data$year >= 2006]) -\n     mean(Data$y1[Data$unit == 1 & Data$year < 2006]))\n\n[1] 2\n\nsummary(lm(y1 ~ factor(year >= 2006) + factor(unit) + factor(year >= 2006):factor(unit), \n           data = Data, subset = (unit != 3)))\n\n\nCall:\nlm(formula = y1 ~ factor(year >= 2006) + factor(unit) + factor(year >= \n    2006):factor(unit), data = Data, subset = (unit != 3))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -0.5   -0.3    0.0    0.3    0.5 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                              3.5000     0.1369  25.560 2.12e-14 ***\nfactor(year >= 2006)TRUE                 1.0000     0.2165   4.619 0.000285 ***\nfactor(unit)2                            1.0000     0.1936   5.164 9.42e-05 ***\nfactor(year >= 2006)TRUE:factor(unit)2   2.0000     0.3062   6.532 6.91e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3354 on 16 degrees of freedom\nMultiple R-squared:  0.9571,    Adjusted R-squared:  0.9491 \nF-statistic: 119.1 on 3 and 16 DF,  p-value: 3.726e-11\n\n\n\n\nC. Early Group v/s Later Group Before 2006\nNext we calculate the effects from the DD comparisons of early and later treated groups, up until the later treated group receives treatment (2006). This is: \\[\\widehat{\\beta}^{2 \\times 2, k}_{kl} \\equiv \\left( \\overline{y}^{Mid(k,l)}_{k} - \\overline{y}^{Pre(k)}_{k} \\right) - \\left( \\overline{y}^{Mid(k,l)}_{l} - \\overline{y}^{Pre(k)}_{l} \\right)\\] where \\(\\overline{y}^{Mid(k,l)}_{k}\\) is the mean of the outcome variable for the early treated group \\(k\\) (\\(unit = 3\\)) in the period between the treatment for the group \\(k\\) and the group \\(l\\) (\\(unit = 2\\)), from 2003 to 2005, \\(\\overline{y}^{Pre(k)}_{k}\\) is the mean for of the outcome variable for the early treated group \\(k\\) (\\(unit = 3\\)) previous to treatment, until 2002, and \\(\\overline{y}^{Mid(k,l)}_{l}, \\overline{y}^{Pre(k)}_{l}\\) are the analogous for the later treated group \\(l\\) (\\(unit = 2\\))\n\nbklk <- lm(y1 ~ post + factor(year) + factor(unit), data = Data,\n   subset = (unit != 1 & year < 2006))$coefficient[\"post\"]\nbklk\n\npost \n   3 \n\n(mean(Data$y1[Data$unit == 3 & (Data$year >= 2003 & Data$year < 2006)]) -\n   mean(Data$y1[Data$unit == 3 & Data$year < 2003])) -\n  (mean(Data$y1[Data$unit == 2 & (Data$year >= 2003 & Data$year < 2006)]) -\n     mean(Data$y1[Data$unit == 2 & Data$year < 2003]))\n\n[1] 3\n\nsummary(lm(y1 ~ factor(year >= 2003) + factor(unit) + factor(year >= 2003):factor(unit), \n           data = Data, subset = (unit != 1 & year < 2006)))\n\n\nCall:\nlm(formula = y1 ~ factor(year >= 2003) + factor(unit) + factor(year >= \n    2003):factor(unit), data = Data, subset = (unit != 1 & year < \n    2006))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -0.2   -0.2    0.0    0.2    0.2 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                              4.2000     0.1155  36.373 3.58e-10 ***\nfactor(year >= 2003)TRUE                 0.6000     0.1633   3.674 0.006271 ** \nfactor(unit)3                            1.0000     0.1633   6.124 0.000282 ***\nfactor(year >= 2003)TRUE:factor(unit)3   3.0000     0.2309  12.990 1.17e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2 on 8 degrees of freedom\nMultiple R-squared:  0.9918,    Adjusted R-squared:  0.9887 \nF-statistic: 322.7 on 3 and 8 DF,  p-value: 1.106e-08\n\n\n\n\nD. Early Group v/s Later Group After 2003\nThe last DD comparison is for early and later treated groups, starting from 2006 \\[\\widehat{\\beta}^{2 \\times 2, l}_{kl} \\equiv \\left( \\overline{y}^{Post(l)}_{l} - \\overline{y}^{Mid(k,l)}_{l} \\right) - \\left( \\overline{y}^{Post(l)}_{k} - \\overline{y}^{Mid(k,l)}_{k} \\right)\\] Where \\(\\overline{y}^{Post(l)}_{l}\\) is the mean of the outcome variable for the later treated group \\(l\\) (\\(unit = 2\\)) in the period after this group received the treatment, from 2006, \\(\\overline{y}^{Mid(k,l)}_{l}\\) is the mean for of the outcome variable for the later treated group \\(l\\) (\\(unit = 2\\)) in the period between the treatment for the group \\(k\\) (\\(unit = 3\\)) and the group \\(l\\), from 2003 to 2005, and \\(\\overline{y}^{Post(l)}_{k}, \\overline{y}^{Mid(k,l)}_{k}\\) are the analogous quantities for the early treated group \\(k\\) (\\(unit = 3\\)). We can generate and save this quantity as we have previously:\n\nbkll <- lm(y1 ~ post + factor(year) + factor(unit), data = Data,\n   subset = (unit != 1 & year > 2002))$coefficient[\"post\"]\nbkll\n\npost \n   2 \n\n(mean(Data$y1[Data$unit == 2 & Data$year > 2005]) -\n   mean(Data$y1[Data$unit == 2 & (Data$year >= 2003 & Data$year < 2006)])) -\n  (mean(Data$y1[Data$unit == 3 & Data$year > 2005]) -\n     mean(Data$y1[Data$unit == 3 & (Data$year >= 2003 & Data$year < 2006)]))\n\n[1] 2\n\nsummary(lm(y1 ~ factor(year >= 2006) + factor(unit) + factor(year >= 2006):factor(unit==2), \n           data = Data, subset = (unit != 1 & year > 2002)))\n\n\nCall:\nlm(formula = y1 ~ factor(year >= 2006) + factor(unit) + factor(year >= \n    2006):factor(unit == 2), data = Data, subset = (unit != 1 & \n    year > 2002))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-0.300 -0.175  0.000  0.175  0.300 \n\nCoefficients: (1 not defined because of singularities)\n                                                Estimate Std. Error t value\n(Intercept)                                       6.8000     0.2160  31.478\nfactor(year >= 2006)TRUE                          0.7000     0.1807   3.873\nfactor(unit)3                                     2.0000     0.1673  11.952\nfactor(year >= 2006)FALSE:factor(unit == 2)TRUE  -2.0000     0.2556  -7.825\nfactor(year >= 2006)TRUE:factor(unit == 2)TRUE        NA         NA      NA\n                                                Pr(>|t|)    \n(Intercept)                                     2.46e-11 ***\nfactor(year >= 2006)TRUE                         0.00309 ** \nfactor(unit)3                                   3.03e-07 ***\nfactor(year >= 2006)FALSE:factor(unit == 2)TRUE 1.43e-05 ***\nfactor(year >= 2006)TRUE:factor(unit == 2)TRUE        NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2366 on 10 degrees of freedom\nMultiple R-squared:  0.9868,    Adjusted R-squared:  0.9829 \nF-statistic: 249.5 on 3 and 10 DF,  p-value: 1.073e-09\n\n\nThis comparison is the comparison which can potentially result in undesired results if treatment effects are dynamic over time because it views group 3 (the previously treated group) as a control. However, in this case, given that treatment effects are homogenous over time we do not have a major problem here, and we observe that \\(\\widehat{\\beta}^{2 \\times 2, l}_{kl}=2\\).\n\n\nWeights\nWe can now arrive to the OLS estimate of this two-way fixed effect model by generating the weighted mean of the previous estimates as: \\[\\widehat{\\tau} = W_{kU} \\cdot \\widehat{\\beta}^{2\\times 2}_{kU} + W_{lU} \\cdot \\widehat{\\beta}^{2\\times 2}_{lU} + W_{kl}^{k} \\cdot \\widehat{\\beta}^{2\\times 2,k}_{kl} + W_{kl}^{l} \\cdot \\widehat{\\beta}^{2\\times 2,l}_{kl}\\] Where each \\(W\\) is the weight that the respective \\(\\beta\\) has in this weighted mean, specifically: \\[\\begin{align*}\nW_{kU} & = \\frac{(n_k + n_U)^2\\widehat{V}^D_{kU}}{\\widehat{V}^D} \\quad &  \\quad W_{lU} & = \\frac{(n_l + n_U)^2\\widehat{V}^D_{lU}}{\\widehat{V}^D} \\\\\nW_{kl}^k & = \\frac{[(n_k + n_l)(1 - \\overline{D}_l)]^2\\widehat{V}^{D,k}_{kl}}{\\widehat{V}^D} \\quad &  \\quad W_{kl}^l & = \\frac{[(n_k + n_l)(1 - \\overline{D}_k)]^2\\widehat{V}^{D,l}_{kl}}{\\widehat{V}^D}\n\\end{align*}\\] Where \\(n\\) refers to the sample share of the group\n\nnk = 1/3\nnl = 1/3\nnu = 1/3\n\n\\(\\overline{D}\\) referes to the share of time the group is treated\n\nDk = mean(Data$post[Data$unit==3])\nDl = mean(Data$post[Data$unit==2])\n\nand \\(\\widehat{V}\\) refers to how much treatment varies\n\nVkU = 0.5*0.5*(Dk)*(1-Dk)\nVlU = 0.5*0.5*(Dl)*(1-Dl) \nVklk = 0.5*0.5*((Dk-Dl)/(1-Dl))*((1-Dk)/(1-Dl))\nVkll = 0.5*0.5*(Dl/Dk)*((Dk-Dl)/(Dk))\nVD   = sum(lm(post ~ factor(unit) + factor(year), \n              data = Data)$residuals^2)/30\n\nThe weights are thus the following:\n\nwkU = ((nk + nu)^2*VkU)/VD\nwkU\n\n[1] 0.3181818\n\nwlU = ((nl + nu)^2*VlU)/VD\nwlU\n\n[1] 0.3636364\n\nwklk = (((nk + nl)*(1-Dl))^2*Vklk)/VD\nwklk\n\n[1] 0.1363636\n\nwkll = (((nk + nl)*Dk)^2*Vkll)/VD\nwkll\n\n[1] 0.1818182\n\n\nWith this in mind the \\(\\tau\\) estimate is\n\ntau = wkU * bku + wlU * blu + wklk * bklk + wkll * bkll\ntau\n\n    post \n2.454545 \n\n\nas observed in the two-way fixed effect estimate above.\n\n\n\n(b) Decomposition with trends\nIn this case the Goodman-Bacon (2021) decomposition follows as above generating the treatment effect as follows:\n\n\nShow the plot code\nlibrary(dplyr)\np1 <- ggplot(data = Data, aes(x = year, y = y2, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(1,0.1,1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 20, by = 5),\n                     labels = seq(from = 0, to = 20, by = 5),\n                     limits = c(0,20)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\np2 <- ggplot(data = Data, aes(x = year, y = y2, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(1,1,0.1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 20, by = 5),\n                     labels = seq(from = 0, to = 20, by = 5),\n                     limits = c(0,20)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\np3 <- Data %>% filter(year < 2006) %>%\n  ggplot(aes(x = year, y = y2, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(0.1,1,1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2),\n                     limits = c(2000,2009)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 20, by = 5),\n                     labels = seq(from = 0, to = 20, by = 5),\n                     limits = c(0,20)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\np4 <- Data %>% filter(year >= 2003) %>%\n  ggplot(aes(x = year, y = y2, color = as.factor(unit))) +\n  geom_line(linetype = 1, size = 0.5, aes(alpha = as.factor(unit))) +\n  geom_point(aes(shape = as.factor(unit), alpha = as.factor(unit)), size = 2) +\n  scale_alpha_manual(values = c(0.1,1,1)) +\n  scale_shape_manual(values = c(1, 16, 12)) +\n  scale_color_manual(values = c(\"black\", \"black\", \"black\")) +\n  labs(x = \"Time\", y = \"Outcome Variable\") +\n  scale_x_continuous(breaks = seq(from = 2000, to = 2009, by = 2),\n                     limits = c(2000,2009)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 20, by = 5),\n                     labels = seq(from = 0, to = 20, by = 5),\n                     limits = c(0,20)) +\n  geom_vline(xintercept = 2002, color = \"red\", linetype = 2) +\n  geom_vline(xintercept = 2005, color = \"red\", linetype = 2) +\n  theme(legend.position = \"none\")\nt1 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"A. Early Group v/s Untreated Group\"), size = 3) +\n  theme_void()\nt2 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"B. Later Group v/s Untreated Group\"), size = 3) +\n  theme_void()\nt3 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"C. Early Group v/s Later Group Before 2006\"), \n            size = 3) +\n  theme_void()\nt4 <- ggplot() + \n  geom_text(aes(x = 0, y = 0, label = \"D. Early Group v/s Later Group After 2003\"), \n            size = 3) +\n  theme_void()\nggarrange(plotlist = list(t1, t2, p1, p2, t3, t4, p3, p4), ncol = 2, nrow = 4, \n          heights = c(0.1, 0.4, 0.1, 0.4))\n\n\n\n\n\n\n\n\nAs seen in the plots, in the decomposition with trends these effects are no longer constants of 3 and 2 for early and later treated units given that the “treatment effect” is no longer simply \\(1 \\times unit\\) in each case.\n\n# 2X2 DD Regressions\nA <- lm(y2 ~ post + factor(year) + factor(unit), data = Data, subset=(unit!=2))\nB <- lm(y2 ~ post + factor(year) + factor(unit), data = Data, subset=(unit!=3))\nC <- lm(y2 ~ post + factor(year) + factor(unit), data = Data, subset=(unit!=1 & year<2006))\nD <- lm(y2 ~ post + factor(year) + factor(unit), data = Data, subset=(unit!=1 & year>2002))\n# 2x2 Betas\nbkUk <- A$coefficient[\"post\"]\nbkUl <- B$coefficient[\"post\"]\nbklk <- C$coefficient[\"post\"]\nbkll <- D$coefficient[\"post\"]\nbkll\n\n  post \n-1.375 \n\n# Share of time treated\nDk = mean(Data$post[Data$unit==3])\nDl = mean(Data$post[Data$unit==2])\n# How much treatment varies\nVkUk = 0.5*0.5*(Dk)*(1-Dk)\nVkUl = 0.5*0.5*(Dl)*(1-Dl) \nVklk = 0.5*0.5*((Dk-Dl)/(1-Dl))*((1-Dk)/(1-Dl))\nVkll = 0.5*0.5*(Dl/Dk)*((Dk-Dl)/(Dk))\nVD <- sum(lm(post ~ factor(unit) + factor(year), data = Data)$residuals^2/30)\n# Share of sample\nnk   = 1/3\nnl   = 1/3\nnu   = 1/3\n# Weights\nwkUk = ((nk + nu)^2*VkUk)/VD\nwkUl = ((nl + nu)^2*VkUl)/VD\nwklk = (((nk + nl)*(1-Dl))^2*Vklk)/VD\nwkll = (((nk + nl)*Dk)^2*Vkll)/VD\n# Tau\ntau = bkUk*wkUk + bkUl*wkUl + bklk*wklk + bkll*wkll\ntau\n\n    post \n3.804545 \n\n\nWhat is noteworthy here is the surprising behaviour flagged by Goodman-Bacon (2021) for the final comparison based on the case where the earlier treated unit (unit 3) is used as a control for the later trated unit (unit 2). In this case, given that there are time-varying treatment effects, despite the fact that each unit-specific treatment effect is positive, we observe that the parameter \\(\\widehat{\\beta}^{2 \\times 2, l}_{kl}\\) is actually negative. In this particular example this negative value (-1.375) is not sufficient to turn the weighted treatment effect estimate negative, but if you play around with the size of the parameters \\(\\beta_1\\) and \\(\\beta_2\\) above, you will see that large enough differences in trends can result in such estimates! Here, as above, we see that when we aggregate unit-specific estimates as tau, the estimate (by definition) agrees with the estimate generated by two-way fixed effect models previously.\n\n\n\nChaisemartin and D’Haultfoeuille (2020)’s Procedure\nNow, we will show that the procedures described in Chaisemartin and D’Haultfoeuille (2020), despite arriving to the estimator in a different way, also let us understand how the regression weights the two-way fixed effect estimator. The authors define \\(\\widehat{\\beta}_{fe}\\) as the coefficient estimated in the following (standard) two-way fixed effects regression: \\[y_{i,s,t} = \\beta_0 + \\beta_{fe} D_{s,t} + \\mu_s + \\lambda_t + \\varepsilon_{s,t}\\] Where \\(D_{s,t}\\) is the mean over \\(i\\) of a binary indicator variable that takes value of 1 if the unit \\(i\\) in state \\(s\\) is treated at period \\(t\\) and 0 otherwise, in our case as we have one observartion per state \\(D_{s,t} = post_{s,t}\\), meanwhile \\(\\mu_s\\) and \\(\\lambda_t\\) are state and time fixed effects. Following we example with both outcomes\n\ncase1 <- lm(data = Data,\n            formula = y1 ~ factor(unit) + factor(year) + post)\npaste0(\"The parameter estimates by two-way fixed effects regression for the \",\n       \"case 1 is: \", case1$coefficients[\"post\"])\n\n[1] \"The parameter estimates by two-way fixed effects regression for the case 1 is: 2.45454545454545\"\n\ncase2 <- lm(data = Data,\n            formula = y2 ~ factor(unit) + factor(year) + post)\npaste0(\"The parameter estimates by two-way fixed effects regression for the \",\n       \"case 2 is: \", case2$coefficients[\"post\"])\n\n[1] \"The parameter estimates by two-way fixed effects regression for the case 2 is: 3.80454545454545\"\n\n\nSame as Equation 6.1. also define the ATE for any (\\(s,t\\)) cell as: \\[\\Delta_{s,t} = \\frac{1}{N_{s,t}} \\sum_{i = 1}^{N_{s,t}}[Y_{i,s,t}(1) - Y_{i,s,t}(0)]\\] Next an example for the simple decomposition case. First is created the counterfactual\n\nData$y1_c <- 2 + (Data$year - 2000) * 0.2 + 1 * Data$unit + 0 * Data$post * Data$unit + \n  0 * Data$post * Data$unit * (Data$time)\n\nNext \\(\\Delta_{s,t}\\)\n\nData$Delta_st[Data$post == 1] = Data$y1[Data$post == 1] - Data$y1_c[Data$post == 1]\n\nOne of the key results of Chaisemartin and D’Haultfoeuille (2020) is to show that under a series of standard assumptions \\[\\beta_{fe} = E \\left[ \\sum_{s,t:D_{s,t}=1}\\frac{N_{s,t}}{N_1}w_{s,t}\\Delta_{s,t} \\right]\\] Where \\(N_1\\) refers to the sum of all treated observations and \\[w_{s,t} = \\frac{\\varepsilon_{s,t}}{\\sum_{s,t:D_{s,t}=1}\\frac{N_{s,t}}{N_1}\\varepsilon_{s,t}}\\] Where \\(\\varepsilon_{s,t}\\) is the residual from a regression of \\(D_{s,t}\\) on state and time fixed-effects.\n\nauxreg <- lm(post ~ factor(unit) + factor(year), data = Data)\nData$eps_st = auxreg$residuals\nData$eps_st[Data$post != 1] = NA\nData$w_st = Data$eps_st / sum(Data$eps_st, na.rm = T)\nprint(paste0(\"Clément de Chaisemartin and Xavier D'Haultfoeuille's procudure \",\n             \"gives an estimates of: \", \n             sum(Data$Delta_st*Data$w_st, na.rm = T)))\n\n[1] \"Clément de Chaisemartin and Xavier D'Haultfoeuille's procudure gives an estimates of: 2.45454545454545\"\n\n\nNext you can find the code for the decomposition with trends\n\nData$y2_c <- 2 + (Data$year - 2000) * 0.2 + 1 * Data$unit + 0 * Data$post * Data$unit + \n  0 * Data$post * Data$unit * (Data$time)\nData$Delta_st2[Data$post == 1] = Data$y2[Data$post == 1] - Data$y2_c[Data$post == 1]\nprint(paste0(\"Clément de Chaisemartin and Xavier D'Haultfoeuille's procudure \",\n             \"gives an estimates of: \", \n             sum(Data$Delta_st2*Data$w_st, na.rm = T)))\n\n[1] \"Clément de Chaisemartin and Xavier D'Haultfoeuille's procudure gives an estimates of: 3.80454545454545\"\n\n\n\n\n\n\nAthey, Susan, and Guido W. Imbens. 2022. “Design-based analysis in Difference-In-Differences settings with staggered adoption.” Journal of Econometrics 226 (1): 62–79. https://doi.org/https://doi.org/10.1016/j.jeconom.2020.10.012.\n\n\nChaisemartin, Clément de, and Xavier D’Haultfoeuille. 2020. “Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects.” American Economic Review 110 (9): 2964–96. https://doi.org/10.1257/aer.20181169.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with Variation in Treatment Timing.” Journal of Econometrics 225 (2): 254–77. https://doi.org/https://doi.org/10.1016/j.jeconom.2021.03.014."
  },
  {
    "objectID": "01_02_Chapter2R.html#tools-required-for-chapter-2",
    "href": "01_02_Chapter2R.html#tools-required-for-chapter-2",
    "title": "Chapter 2",
    "section": "Tools Required for Chapter 2",
    "text": "Tools Required for Chapter 2\nHere will be the Chapter 2 excersises and examples."
  },
  {
    "objectID": "01_02_Chapter2R.html#code-call-out-2.1",
    "href": "01_02_Chapter2R.html#code-call-out-2.1",
    "title": "Chapter 2",
    "section": "Code Call Out 2.1",
    "text": "Code Call Out 2.1\nBanerjee, Duflo, and Sharma (2021)\n\n\n\nCall:\nlm(formula = ind_fin_el1 ~ treatment, data = subset(data, el1 == \n    1))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2555 -0.3219 -0.1846  0.0458  4.6377 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.135439   0.030306   4.469 8.97e-06 ***\ntreatment   -0.009589   0.041626  -0.230    0.818    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5931 on 813 degrees of freedom\n  (166 observations deleted due to missingness)\nMultiple R-squared:  6.527e-05, Adjusted R-squared:  -0.001165 \nF-statistic: 0.05307 on 1 and 813 DF,  p-value: 0.8179\n\n\nCoefficient for treatment in regression: -0.009589323 \n\n\nDifference in means (Treatment - Control): -0.009589323 \n\n\nThe coefficient from the regression should be equal to the difference in means to demonstrate equivalence.\n\n\n\nBlock 2.2\nRandomization inference, although a theoretical concept, is best illustrated with practical examples. A particularly illustrative approach is visualization through tabular permutation. The following online coding resource provides a detailed introduction to this method. In this context, we will work with data from the study “Long-Term effects of the Targeting the Ultra Poor Program” conducted by Abhijit Banerjee, Esther Duflo, and Garima Sharma.\n\n\n$observed_effect\n[1] -0.009589323\n\n$p_value\n[1] 0.8221\n\n\n$observed_effect\n[1] 0.4077528\n\n$p_value\n[1] 4e-04\n\n\n\n\n\n\nBanerjee, Abhijit, Esther Duflo, and Garima Sharma. 2021. “Long-Term Effects of the Targeting the Ultra Poor Program.” American Economic Review: Insights 3 (4): 471–86. https://doi.org/10.1257/aeri.20200667."
  }
]