---
title: "Chapter 2"
bibliography: refs.bib
---

# Tools Required for Chapter 2

We will begin by examining a number of details which will be required during the exercises encountered in this chapter or in the code call outs (as well as the rest of this book). First, we provide a very brief introduction to simulation with pseudo random numbers and regression, and then we discuss the use of functions which we write ourselves with Python.

## Simulation and Regression
To start with, let's have a look at some simple simulations, like those referred to in the end of chapter questions.  Let's begin by considering the data generating process (DGP) which we wish to simulate:

$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i.$

Having a DGP defined, all we need to do is define how any variables area distributed, as well as any necessary scalar values.  In this case, let's set $\beta_0=1$ and $\beta_1=2$.  And let's define both $x_i$ and $\varepsilon_i$ as being distributed $\mathcal{N}(0,1)$.  We will simulate these separately implying that they are independent, though we could simulate them as coming from a bivariate normal if we wanted to generate correlations between the variables.  Our interest in the below demonstration will be to conduct a simulation, and then implement a regression model to estimate $\widehat\beta_1$ by OLS. In the book, you will be asked to consider examples which are more appropriate for the treatment effects framework which we are considering.

Let's begin by loading the packages we need.  We will use `sklearn.linear_model` to estimate regression (though we could use other packages such as `statsmodels` and the `OLS` function). Once we have loaded the packages we need, we can then simulate our data.  Note that we have set the seed with `np.random.seed()` which ensures that we can replicate the exact same simulation when running this code in the future (or if you run the code with the same seed on your machine).

```{python}
#| warning: false
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Generate independent variable (X) and dependent variable (Y)
np.random.seed(1213)
N = 1000
x = np.random.rand(N, 1)  # 1000 observations of a single independent variable
y = 1 + 2*x + np.random.randn(N, 1)  # Linear relationship with some noise


data = pd.DataFrame({'x': x[:, 0], 'y': y[:, 0]})
#Could also do:
#data = pd.DataFrame({'x': x.flatten(), 'y': y.flatten()})

# Create a LinearRegression model
model = LinearRegression()

# Fit the model to the data
model.fit(data[['x']], data[['y']])

# Coefficients
beta1hat = model.coef_[0][0]
beta0hat = model.intercept_[0]

print(f"beta 1 (Coefficient): {beta1hat}")
print(f"Intercept: {beta0hat}")
```
In the above after simulating $N=1,000$ observations from the DGP of interest, we have estimated a regression, and we see that both $\widehat\beta_0$ and $\widehat\beta_1$ are close to the values implied by the DGP.  They only differ due to sampling variation in our simulated data.  Below we can present the output of this linear model graphically, which gives us slightly more practice working with Python's plotting tools.  Note that we are using `matplotlib` as a base to set up a graph frame, and then using `seaborn` to super-impose plots and provide some slightly fancier output options.  Though we could of course do this all within `matplotlib` if we prefer.


```{python}
#| label: fig-regscatter
#| fig-cap: "Two-way scatter plot with regression model"

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
sns.set_palette("viridis")

# Make predictions using the model
data['yhat'] = model.predict(data[['x']])

# Create a scatter plot with regression line
plt.figure(figsize=(8, 6))
sns.scatterplot(x='x', y='y', data=data, label='Data Points')
sns.lineplot(x='x', y='yhat', data=data, color='red', label='Regression Line')
plt.xlabel(r'$x$')
plt.ylabel(r'$y$')
plt.legend()
plt.grid(True)
plt.show()
```

Once we have simulated a single realisation of this DGP, we can now consider many realisations, examining the distribution of the estimator across realisations.  Below we do this 500 times in a `for` loop.  Because we do not need to refer to anything specific within each loop -- we simply wish to repeat the process 500 times, we iterate using `_`, which is never referred to within the loop.  Below we store our estimated $\widehat\beta_1$ in each of the iterations in our loop, before finally plotting these in a histogram, where we see the variation that we would expect given our DBP:

```{python}
#| label: fig-reghist
#| fig-cap: "Regression estimates in 500 simulations"

#Now do this 500 times to see distribution of beta hat
# Create an empty list to store slope coefficients
beta1hats = []

# Perform the simulations and regressions 500 times
for _ in range(500):
    # Generate synthetic data
    x = np.random.rand(N, 1)
    y = 1 + 2 *x + np.random.randn(N, 1)

    # Create a DataFrame
    data = pd.DataFrame({'x': x[:, 0], 'y': y[:, 0]})

    # Create a LinearRegression model and fit it to the data
    model = LinearRegression()
    model.fit(data[['x']], data[['y']])

    # Extract the slope coefficient and store it
    beta1hats.append(model.coef_[0][0])

# Calculate mean estimate of beta 1 hat
print('Mean coefficient estimate is: ' + str(np.mean(beta1hats)))

# Create a histogram of the slope coefficients
plt.figure(figsize=(8, 6))
plt.hist(beta1hats, bins=30, edgecolor='black', label='Estimates')
plt.axvline(2, color='red', linestyle='dashed', label=r'$\beta_1$')
plt.xlabel(r'$\widehat\beta_1$')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True)
plt.show()
```


## Introduction to Functions in Python
Before moving on to the code call outs introduced in Chapter 2, we will introduce the idea of functions which we can write ourself to conduct specific stand-alone tasks in an organised wey. Functions in Python are key to efficient and organized programming. They allow you to encapsulate specific tasks into reusable blocks of code, enhancing modularity and readability. Functions are fundamental in Python, whether for simple operations or complex data analysis tasks.

### Creating a Simple Function
In Python, functions are defined using the `def` keyword. A basic function typically includes a name, arguments, and a return statement.  Names must begin with a letter or underscore, and should be followed by letters, underscore or number, and are case-sensitive.  Here's an example of a simple function that adds two numbers:

```{python}

def add_values(a, b):
    return a + b 

result = add_values(12, 3)
print(result) 
```

This function, `add_values`, takes two arguments, `a` and `b`, adds them together, and returns the sum.  Functions need not have arguments.  If no arguments are required, a function can be simply defined as `function_name()`.  Similarly, functions need not return a result.  If a function is issued with a `return` statement, the function will simply execute the lines with any arguments provided, and return nothing.

### Understanding Function Arguments
Function arguments are the values passed to the function when it is called. They can be mandatory or optional. Optional arguments have default values, allowing the function to be called with fewer arguments.  For example, below we define a function with an optional second argument.  We will call it once with only one argument, in which case the second argument will revert to default.  We will then call it a second time with two arguments, and in these cases, the default behaviour will be over-ridden:

```{python}
def multiply_values(a, b=2):
    return a * b

print(multiply_values(10))
print(multiply_values(10,3))
```

### Documenting our Functions
Finally note that we can document our function using what are known as *docstrings*.  Docstrings are strings enclosed in triple quotes that appear as the first statement in a function body. They provide documentation about the purpose and usage of the function.  We can then consult these docstrings using Python's `help` function.  For example, see below the edit we have made to the above `multiply_values` function, which now makes it much easier to quickly remember what the function does when we use `help` below:

```{python}
def multiply_values(a, b=2):
    """
    This function multiplies two values which are provided as arguments.
    The function returns the product of the two arguments provided.
    Two or optionally one argument can be provided.  If only one argument is
    provided, the second argument defaults to two.
    """
    return a * b

help(multiply_values)
```

<!--#
### Advanced Functions with Arguments

Using functions with arguments is extremely useful in Python, particularly for tasks like data manipulation and analysis.  Below we consider a slightly more elaborate function 

### Example: Linear Regression Analysis Function

For a more complex example, we can create a function to perform linear regression analysis using Python's **`statsmodels`** library:

```{python}

import statsmodels.api as sm

def perform_linear_regression(X, y):
    X = sm.add_constant(X)  # Adding a constant
    model = sm.OLS(y, X).fit()
    return model.summary()

```

This function, **`perform_linear_regression`**, takes two arguments: **`X`** (independent variables) and **`y`** (dependent variable), performs the regression analysis, and returns the summary of the model.

#### Simulating Data for Demonstration

To demonstrate the linear regression function, we first create some simulated data:

```{python}

import numpy as np
import pandas as pd

np.random.seed(123)
data = pd.DataFrame({
    'marketing_spend': np.random.uniform(1, 1000, 100),
    'sales': np.random.normal(500, 50, 100)
})

```

#### Using the Linear Regression Function

We then apply the linear regression function to our simulated data:

```{python}

X = data['marketing_spend']
y = data['sales']

# Perform Linear Regression
regression_results = perform_linear_regression(X, y)

# Print Regression Results
print(regression_results)

```
-->
# Code Call Outs

## Call Out 2.1: Regression versus comparison of means
To understand the equivalence between regression analysis and the comparison of means in a binary regression set-up, we refer to Section 2.1 of the online coding resource. In this section, we work with data from a randomized control trial that examines asset transfers to poor households in India, as discussed in the paper by @Banerjeeetal2021.  We will begin by calculating the two elements of the linear regression by hand.  This is the constant term: $E[Y_i|Treat_i=0]$, and the treatment effect: $E[Y_i|Treat_i=1]-E[Y_i|Treat_i=0]$.

```{python}
import pandas as pd

# Load the household data
data = pd.read_csv("Datasets/Banerjee_et_al_2021.csv")

# Assign the loaded data to subset_data
subset_data = data

# Check for missing values
print("Missing values in 'asset_ind_tot_el1':", subset_data['asset_ind_tot_el1'].isna().sum())
print("Missing values in 'treatment':", subset_data['treatment'].isna().sum())

# Remove rows with missing values
subset_data = subset_data.dropna(subset=['asset_ind_tot_el1', 'treatment'])

mean_treatment = subset_data.loc[(subset_data['treatment'] == 1), 'asset_ind_tot_el1'].mean()
mean_control   = subset_data.loc[(subset_data['treatment'] == 0), 'asset_ind_tot_el1'].mean()
diff_means = mean_treatment - mean_control

# Display means and regression parameters
print("Control Mean:", mean_control, " Treatment Mean:", mean_treatment)
print(f"Difference in means (Treatment - Control): {diff_means}")
```

Now, let's confirm that mechanically the bivariate regression returns to us the same parameters.  Below we conduct this regression using the `statsmodels` library's `OLS` function.  As expected, we confirm that the constant term equals -0.1961 as calculated above, and the treatment effect is equivalent to the difference of means value of 0.4078.


```{python, echo=13:18}
import pandas as pd
import statsmodels.api as sm

# Load the household data
data = pd.read_csv("Datasets/Banerjee_et_al_2021.csv")

# Assign the loaded data to subset_data
subset_data = data

# Remove rows with missing values
subset_data = subset_data.dropna(subset=['asset_ind_tot_el1', 'treatment'])

# Run the regression
X = subset_data['treatment']
y = subset_data['asset_ind_tot_el1']
X = sm.add_constant(X)
model = sm.OLS(y, X).fit()
print(model.summary())
```


## Call Out 2.2: Randomization Inference

Randomization inference is perhaps best-illustrated with practical examples. A particularly illuminating approach to understand how randomisation inference works is visualization through tabular permutation. The following online coding resource provides a detailed introduction to this method. In this context, we will first consider a made-up example based on 3 treated units and 3 control units, before working with a larger number of control and treated units in data from the study "Long-Term effects of the Targeting the Ultra Poor Program" conducted by Abhijit Banerjee, Esther Duflo, and Garima Sharma.

### An Exact p-value
It is perhaps useful to see a simple example. Consider the case of 6 units, with 3 observations randomly assigned treatment. Imagine that the observed outcomes were then, in the treatment group: $(34,27,29)$, and in the control group: $(14,18,24)$. A simple comparison of means estimator suggests that the treatment effect is 11.33. To calculate a p-value, we can permute all the possible combinations, and ask what proportion of these are greater than or equal to this treatment effect. If we consider random orderings of 6 units, this suggests that there are $6!$ possible combinations, but in reality, as we are randomly choosing 3 units from these 6 to assign a permuted treatment status, the actual value of different combinations is $6\choose 3$ $=\frac{6!}{3!*(6-3)!}=20$. We document each of these possible permutations, as well as their permuted treatment effect in the Table below. In this case, we can see that only 1 of the 20 different permutations is greater than or equal to 11.33 he original treatment status). Suggesting an exact p-value of $1/20=0.05$.

| Permutation  | T1  | T2  | T3  | C1  | C2  | C3  | Estimate |
|--------------|-----|-----|-----|-----|-----|-----|----------|
| Original (1) | 34  | 27  | 29  | 14  | 18  | 24  | 11.33    |
| 2            | 34  | 27  | 14  | 29  | 18  | 24  | 1.33     |
| 3            | 34  | 27  | 18  | 14  | 29  | 24  | 4        |
| 4            | 34  | 27  | 24  | 14  | 18  | 29  | 8        |
| 5            | 34  | 14  | 29  | 27  | 18  | 24  | 2.67     |
| 6            | 34  | 18  | 29  | 14  | 27  | 24  | 5.33     |
| 7            | 34  | 24  | 29  | 14  | 18  | 27  | 9.33     |
| 8            | 14  | 27  | 29  | 34  | 18  | 24  | -2       |
| 9            | 18  | 27  | 29  | 14  | 34  | 24  | 0.67     |
| 10           | 24  | 27  | 29  | 14  | 18  | 34  | 4.67     |
| 11           | 34  | 14  | 18  | 27  | 29  | 24  | -4.67    |
| 12           | 34  | 14  | 24  | 27  | 18  | 29  | -0.67    |
| 13           | 34  | 18  | 24  | 14  | 27  | 29  | 2        |
| 14           | 14  | 27  | 18  | 34  | 29  | 24  | -9.33    |
| 15           | 14  | 27  | 24  | 34  | 18  | 29  | -5.33    |
| 16           | 18  | 27  | 24  | 14  | 34  | 29  | -2.67    |
| 17           | 14  | 18  | 29  | 34  | 27  | 24  | -8       |
| 18           | 14  | 24  | 29  | 34  | 18  | 27  | -4       |
| 19           | 18  | 24  | 29  | 14  | 34  | 27  | -1.33    |
| 20           | 14  | 18  | 24  | 34  | 27  | 29  | -11.33   |

: A Simple Illustration of Randomization Inference {.striped .hover .borderless .secondary}

We will set this up in Python. First, we will load the required libraries:

```{python}
#Load required libraries
import pandas as pd
import numpy as np
from itertools import permutations
import matplotlib.pyplot as plt
import seaborn as sns
```

We will now enter data as a Pandas data frame, and subsequently calculate the difference in means estimator in a number of lines. You will note that in calculating the differnce in means estimator, we are first sub-setting using logical indexing (for example: `data[data['W']==1]`, which means "choose all rows of data for which $W=1$"). Then, we calculate the means in each group using the Pandas `mean()` operator.

```{python}
#Enter data as Pandas data frame
data = {'Y': [34, 27, 29, 14, 18, 24],
        'W': [1, 1, 1, 0, 0, 0]}
data = pd.DataFrame(data)


#Calculate Difference in means estimator  
Y1 = data[data['W']==1]['Y']
Y0 = data[data['W']==0]['Y']
tau = Y1.mean() - Y0.mean()
```

```{python}
#Generate permutations of W
perm=permutations([1,1,1,0,0,0])
#Wperm=perm
Wperm = set(perm)

Taus = []
for p in Wperm:
    dataP = pd.DataFrame({'Y': [34, 27, 29, 14, 18, 24], 'W': p})
    tauP = dataP[dataP['W']==1]['Y'].mean()-dataP[dataP['W']==0]['Y'].mean()
    Taus.append(tauP)


p_2side = sum(np.absolute(Taus)>=tau)/len(Taus)
p_1side = sum(Taus>=tau)/len(Taus)

print("The two-sided p-value is: " + str(p_2side))
```

```{python}
#| label: fig-permute
#| fig-cap: "Permutation inference"

#Generate graph
plt.hist(Taus,bins=10, edgecolor='black', density=True, label="Permutations")
plt.ylabel("Density")
plt.xlabel("Test statistic")
plt.axvline(tau, color='red', linestyle='dashed', label=r'$\widehat\tau$')
plt.legend() 
plt.show()

```

### A Real Example

@Banerjeeetal2021

```{python}
import pandas as pd
import numpy as np

# Read the data
data = pd.read_csv("Datasets/Banerjee_et_al_2021.csv")

# Function to perform randomization inference
def randomization_inference(data, column_name):
    # Observed treatment effect
    treated = data[data['treatment'] == 1]
    control = data[data['treatment'] == 0]
    obs_effect = treated[column_name].mean() - control[column_name].mean()

    # Randomization inference
    n_permutations = 10000
    permuted_effects = np.zeros(n_permutations)
    for i in range(n_permutations):
        permuted_treatment = np.random.permutation(data['treatment'].values)
        treated_effect = data[column_name][permuted_treatment == 1].mean()
        control_effect = data[column_name][permuted_treatment == 0].mean()
        permuted_effects[i] = treated_effect - control_effect

    # Calculate p-value
    p_value = np.mean(np.abs(permuted_effects) >= np.abs(obs_effect))
    return {"observed_effect": obs_effect, "p_value": p_value}


def print_results(index_name, results):
    significance = "significative" if results['p_value'] < 0.05 else "not significative"
    print(f"The observed effect of {index_name} is {results['observed_effect']:.4f} and it's p-value is {results['p_value']:.4f}, which is {significance}.")

# Financial Index
financial_results = randomization_inference(data, 'ind_fin_el1')
print_results("Financial Index", financial_results)

# Asset Index
asset_results = randomization_inference(data, 'asset_ind_tot_el1')
print_results("Asset Index", asset_results)
```

## Code Call Out 2.3: Bootstrap

For an introduction to the bootstrap, refer to Section 2.4.4 of the book. Here we will examine a computational implementation of a bootstrap standard error and confidence intervals, using data from @Banerjeeetal2021 which we have worked with above. First we will consider the main regression in Code Call-Out 2.1 $$Y_i = \mu + \tau_{ATE}W_i + \varepsilon_i.$$ Here we will work with the data we have loaded previously as data, and the condition `el1==1` which are those observations from the first endline survey, and examine the asset index (`asset_ind_tot_el1`) as our outcome measure $Y_i$. We can load the data and examine the estimated treatment effect via regression below.

```{python}
import pandas as pd
import statsmodels.api as sm
### Import data
data = pd.read_csv("Datasets/Banerjee_et_al_2021.csv")
### Filter data
data = data[data["el1"] == 1]
data = data.dropna(subset=["treatment", "asset_ind_tot_el1"])
### First OLS
#### Data Prep
X = data['treatment']
Y = data['asset_ind_tot_el1']
X = sm.add_constant(X)
#### Model fitting
model = sm.OLS(Y, X).fit()
print(model.summary())
```

From the summary we can note that $\widehat{\tau}_{ATE} = 0.408$ and its standard error equals 0.117. This suggests a p-value of 0.0005, and 95% confidence intervals of \[0.177;0.638\]. In this case the standard errors are computed via the standard OLS variance estimator, assuming homoscedastic errors, although heteroscedasticity-robust standard errors can be requested quite simply using the `get_robustcov_results` method from the `statsmodels.OLS` regression results. This is showed below, resulting in slight variations to the reported standard error (and correspondingly, p-value and resulting confidence intervals).

```{python}
#### Robust standard errors
print(model.get_robustcov_results().summary())
```

An alternative to these closed-form methods for variance estimation is to use bootstrap resampling methods. There are multiple ways a bootstrap can be implemented, but the simplest here will be to simply conduct a paired bootstrap, which is also robust to heteroscedasticity. The pairs bootstrap consists of, first, sampling with replacement the ordered pairs $\left\{(y_i,w_i)\right\}_{i=1}^{N} = \left\{(y_1,w_1),\ldots,(y_N,w_N)\right\}$ from original data, obtaining a "new" dataset of $N$ resampled pairs $\left\{(y_i^*,w_i^*)\right\}_{i=1}^{N} = \left\{(y_1^*,w_1^*),\ldots,(y_N^*,w_N^*)\right\}$. The new dataset thus simply consists of (potentially repeated) randomly selected rows of the original data. Let's see what this looks like with a single bootstrap replicate. First, we will generate this new dataset, having a look at the first 10 rows of the new dataset.

```{python}
#### Resample data
resampled_data = data.sample(frac=1, replace=True, random_state=121316)
print(resampled_data.head(10))
```

With this bootstrap sample we estimate again the coefficient of interest with standard OLS procedure:

```{python}
#### Artificial model
X = resampled_data['treatment']
X = sm.add_constant(X)
Y = resampled_data['asset_ind_tot_el1']
artificial_model = sm.OLS(Y, X).fit()
print(artificial_model.summary())
```

With this sample, we have obtained an estimate of $\widehat{\tau}_{ATE}^* = 0.395$; a different value to the original value, in line with variation in the sample used for estimation. If we want some idea of how much estimates vary as the sample changes (ie the sampling variation of our estimate), the next step simply consists of repeating this bootstrap process $B$ times, resulting in $B$ estimates of $\widehat{\tau}_{ATE,(b)}^*$ with $b\in\{1,\ldots,B\}$. Let's set $B=1,000$, and do this:

```{python}
#### List for taus
taus = []
#### Bootstrap
B = 1000
for i in range(B):
    resampled_data = data.sample(frac=1, replace = True)
    X = resampled_data['treatment']
    X = sm.add_constant(X)
    Y = resampled_data['asset_ind_tot_el1']
    artificial_model = sm.OLS(Y, X).fit()
    taus.append(artificial_model.params['treatment'])
```

You may wish to step through each of the lines in this loop to ensure that you can see what is going on, but the end product of this code is a vector called `taus` which has been filled in with $B$ estimates of $\tau$, $\left\{\widehat{\tau}^{(b)}\right\}_{b=1}^{B}$. We can have a look at the entire empirical distribution of these estimates as follows:

```{python}
import seaborn as sns
import numpy as np
sns.set_theme()
plot = sns.kdeplot(taus, color='black');
plot.axvline(x = model.params['treatment'], color="gray", linewidth = 4, 
             alpha = 0.6);
plot.axvline(np.quantile(taus, 0.025), color = 'red', linestyle = "--");
plot.axvline(np.quantile(taus, 0.975), color = 'red', linestyle = "--");
plt.show()
```

As we can see, this empirical distribution is close to centred on the original estimate (in the limit, they will be exactly the same, which is something you may wish to confirm by using a larger value for $B$), while also giving us some idea of the variation of the estimate over alternative resamples. We additionally plot the empirical 2.5^th^ and 97.5^th^ quantiles of the distribution, which provides a 95% confidence interval for $\widehat\tau$. Clearly, and in line with the regression results observed above, we can reject the null that $\tau=0$ with some certainty. Using these values $\left\{\widehat{\tau}^{(b)}\right\}_{b=1}^{B}$, finally we display the estimate's standard error as the standard deviation of the bootstrap estimates, and the confidence intervals can be generated from empirical quantiles. <!-- Similarly, if we wish to calculate a p-value, we can do so, asking how many values of the empirical distribution are more extreme than the estimate of interest.    --> We illustrate this below:

```{python}
var = np.var(taus)
print("The variance estimate is: " + str(round(var, 4)) +
      " as such, the standard error estimates is: " + 
      str(round(np.sqrt(var), 4)))
lower = np.quantile(taus, 0.025)
upper = np.quantile(taus, 0.975)
print("The empirical 95% confidence interval is: [" +
      str(round(lower, 3)) + "," + str(round(upper, 3)) + "].")
```

We can see that the standard error, and hence 95% confidence interval, is very closely alligned to that from regression documented above, given both are valid under broadly similar assumptions.
