---
title: "Chapter 9"
bibliography: references.bib
---

## Code Call Out 9.1
```{python}
"""
The data consist of 9,915 observations at the household level drawn from the 1991 Survey of Income and Program Participation (SIPP). 
All the variables are referred to 1990. We use net financial assets (net_tfa) as the outcome variable. 
The net financial assets are computed as the sum of IRA balances, 401(k) balances, 
checking accounts, saving bonds, other interest-earning accounts, other interest-earning assets, stocks,
and mutual funds less non mortgage debts.

Among the 9,915 individuals, 3,682 are eligible to participate in the program. 
The variable e401 indicates eligibility and p401 indicates participation, respectively.
"""
from doubleml import DoubleMLData
from doubleml.datasets import fetch_401K
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
colors = sns.color_palette()

data = fetch_401K(return_type='DataFrame')

dml_data = DoubleMLData(data, y_col='net_tfa', d_cols='e401',
                        x_cols=['age', 'inc', 'educ', 'fsize', 'marr',
                                'twoearn', 'db', 'pira', 'hown'])


data['e401'].value_counts().plot(kind='bar', color=colors)
plt.title('Eligibility, 401(k)')
plt.xlabel('e401')
_ = plt.ylabel('count')

data['p401'].value_counts().plot(kind='bar', color=colors)
plt.title('Participation, 401(k)')
plt.xlabel('p401')
_ = plt.ylabel('count')

_ = sns.displot(data, x="net_tfa", hue="e401", col="e401",
                kind="kde", fill=True)

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
 
ml_l_rf = RandomForestRegressor(n_estimators=500, max_depth=7,max_features=3,min_samples_leaf=3)
ml_m_rf = RandomForestClassifier(n_estimators=500, max_depth=5,max_features=4,min_samples_leaf=7)

import numpy as np
from doubleml import DoubleMLPLR

np.random.seed(42)
# Paramatrized by user
dml_plr_rf = DoubleMLPLR(dml_data,
                         ml_l = ml_l_rf,
                         ml_m = ml_m_rf,
                         n_folds = 3,
                         n_rep = 1,
                         score = 'partialling out')

# Estimation
dml_plr_rf.fit()

# Coefficient estimate
dml_plr_rf.coef

# Standard error
dml_plr_rf.se

# Summary 
dml_plr_rf.summary

print(dml_plr_rf)
```
##Causal Forest
```{python}
import pandas as pd
import numpy as np
from econml.dml import CausalForestDML
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier
from statsmodels.stats.outliers_influence import variance_inflation_factor
import matplotlib.pyplot as plt
from statsmodels.iolib.table import SimpleTable
from statsmodels.stats.diagnostic import het_breuschpagan
from statsmodels.regression.linear_model import OLS
import statsmodels.api as sm
from statsmodels.stats.api import anova_lm

# Cargar los datos
path = "./Datasets/"
filename = "Oreopoulos2011skilled.dta"  
df = pd.read_stata(path + filename)


# Ensure specified columns exist and have data
assert 'callback' in df.columns, "'callback' is not found in the DataFrame"
assert 'canadian_name' in df.columns, "'canadian_name' is not found in the DataFrame"
for column in ['female', 'ba_quality', 'extracurricular_skills', 'language_skills', 'ma', 'same_exp', 'exp_highquality', 'reference', 'accreditation', 'legal']:
    assert column in df.columns, f"'{column}' is not found in the DataFrame"

Y = df['callback']
D = df['canadian_name']
X = df[['female', 'ba_quality', 'extracurricular_skills', 'language_skills', 'ma', 'same_exp', 'exp_highquality', 'reference', 'accreditation', 'legal']]

# Updating cf to use GradientBoostingClassifier for the treatment model
cf = CausalForestDML(
    model_y=GradientBoostingRegressor(),
    model_t=GradientBoostingClassifier(),  # Now using a classifier for the treatment
    discrete_treatment=True,
    random_state=123
)

# Fitting the model
cf.fit(Y, D, X=X)
tau_hat = cf.effect(X.values)
tau_hat_se = cf.effect_interval(X.values)

# Average Treatment Effect (ATE)
ate = np.mean(tau_hat)
print("ATE:", ate)

# Histogram of treatment effects
plt.hist(tau_hat, bins=30, color='lightblue', edgecolor='grey')
plt.axvline(x=ate, color='red', linestyle='--', label='ATE')
plt.xlabel('Treatment Effects')
plt.ylabel('Count')
plt.legend()
plt.show()

#Ordered effects along with 95% CI
effects = cf.effect(X.values).flatten()
CIs     = cf.effect_interval(X.values)[1]-effects
indices = np.argsort(effects)
effects_sorted = effects[indices]
ci_sorted = CIs[indices]

color_palette = ['#3380FF', '#FFC300']
plt.figure(figsize=(8, 5))
plt.errorbar(np.arange(len(effects)), effects_sorted, yerr=ci_sorted,
             fmt='o', markersize=5, capsize=3,elinewidth=0.01, capthick=0.01)
plt.xlabel('Data Point Index (Ordered by Effect Size)', fontsize=12)
plt.ylabel(r'$\Delta$ Callback Rate', fontsize=12)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.show()

# Linear Regression to evaluate treatment effect heterogeneity
model_ols = OLS(Y, sm.add_constant(X)).fit()
print(model_ols.summary())

# To add interaction terms and perform Breusch-Pagan test or similar, we are going to do it manually.
# In this example, we are adding an interaction between 'canadian_name' and 'female'
X_interaction = X.copy()
X_interaction['canadian_name_female'] = X_interaction['female'] * df['canadian_name']
model_ols_interaction = OLS(Y, sm.add_constant(X_interaction)).fit()
print(model_ols_interaction.summary())

# Breusch-Pagan test for heteroscedasticity
bp_test = het_breuschpagan(model_ols.resid, model_ols.model.exog)
print('Breusch-Pagan test:', bp_test)

# First, let's investigate heterogeneity through subgroups.
# Suppose we want to explore heterogeneity in the treatment effect, based on the level of education ('ba_quality' in this case).

# We divide the dataset into subgroups based on 'ba_quality'.

high_ba_quality = X['ba_quality'] == 1
low_ba_quality = X['ba_quality'] == 0

# Calculate treatment effects for each group
effect_high_ba_quality = cf.effect(X[high_ba_quality].values)
effect_low_ba_quality = cf.effect(X[low_ba_quality].values)

# Compare average treatment effects
ate_high_ba_quality = np.mean(effect_high_ba_quality)
ate_low_ba_quality = np.mean(effect_low_ba_quality)

print(f"ATE for high BA quality: {ate_high_ba_quality}")
print(f"ATE for low BA quality: {ate_low_ba_quality}")

# To visualize the distribution of treatment effects in both groups
plt.hist(effect_high_ba_quality, bins=30, alpha=0.5, label='High BA Quality')
plt.hist(effect_low_ba_quality, bins=30, alpha=0.5, label='Low BA Quality')
plt.legend()
plt.xlabel('Treatment Effect')
plt.ylabel('Density')
plt.title('Distribution of Treatment Effects by BA Quality')
plt.show()

```
