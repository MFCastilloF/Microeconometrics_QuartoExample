---
title: "Chapter 2"
bibliography: refs.bib
---
# Tools Required for Chapter 2

**TRAER INFORMACIÓN AQUÍ DEL ARCHIVO DE PYTHON**

# Code Call Outs

**TRAER INFORMACIÓN AQUÍ DEL ARCHIVO DE PYTHON**

## Simulation and Regression
To start, we will simulate some data based on the following data generating process:

$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$

where $\beta_0=1$, $\beta_1=2$, and both $x_i$ and $\varepsilon_i$ are distributed $\mathcal{N}(0,1)$. Having conducted this simulation, we will estimate a regression model to estimate $\widehat\beta_1$. In the book, you will be asked to consider examples which are more appropriate for the treatment effects framework which we are considering.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(Statamarkdown)
```

```{stata}
clear all
set seed 1213
set obs 1000
gen x = rnormal()
gen epsilon = rnormal()
gen y = 1 + 2*x + epsilon

reg y x
predict yhat
scatter y x || line yhat x, legend(label(1 "Data Points") label(2 "Regression Line"))

```

Finally, we will do this 500 times, to see what the distribution of estimated paramters $\widehat\beta_1$ looks like:
```{stata}
set seed 1213
set obs 1000
gen beta1hat = .

forvalues i = 1/500 {
    quietly {
        replace x = rnormal()
        replace epsilon = rnormal()
        replace y = 1 + 2*x + epsilon
        reg y x
        replace beta1hat = _b[x] in `i'
    }
}

sum beta1hat
histogram beta1hat, frequency title("Regression estimates in 500 simulations") ///
    xtitle("{&beta}1 hat") ytitle("Frequency") lcolor(black) ///
	xline(2)
```
### An Exact p-value

It is perhaps useful to see a simple example. Consider the case of 6 units, with 3 observations randomly assigned treatment. Imagine that the observed outcomes were then, in the treatment group: $(34,27,29)$, and in the control group: $(14,18,24)$. A simple comparison of means estimator suggests that the treatment effect is 11.33. To calculate a p-value, we can permute all the possible combinations, and ask what proportion of these are greater than or equal to this treatment effect. If we consider random orderings of 6 units, this suggests that there are $6!$ possible combinations, but in reality, as we are randomly choosing 3 units from these 6 to assign a permuted treatment status, the actual value of different combinations is $6\choose 3$ $=\frac{6!}{3!*(6-3)!}=20$. We document each of these possible permutations, as well as their permuted treatment effect in the Table below. In this case, we can see that only 1 of the 20 different permutations is greater than or equal to 11.33 he original treatment status). Suggesting an exact p-value of $1/20=0.05$.

| Permutation  | T1  | T2  | T3  | C1  | C2  | C3  | Estimate |
|--------------|-----|-----|-----|-----|-----|-----|----------|
| Original (1) | 34  | 27  | 29  | 14  | 18  | 24  | 11.33    |
| 2            | 34  | 27  | 14  | 29  | 18  | 24  | 1.33     |
| 3            | 34  | 27  | 18  | 14  | 29  | 24  | 4        |
| 4            | 34  | 27  | 24  | 14  | 18  | 29  | 8        |
| 5            | 34  | 14  | 29  | 27  | 18  | 24  | 2.67     |
| 6            | 34  | 18  | 29  | 14  | 27  | 24  | 5.33     |
| 7            | 34  | 24  | 29  | 14  | 18  | 27  | 9.33     |
| 8            | 14  | 27  | 29  | 34  | 18  | 24  | -2       |
| 9            | 18  | 27  | 29  | 14  | 34  | 24  | 0.67     |
| 10           | 24  | 27  | 29  | 14  | 18  | 34  | 4.67     |
| 11           | 34  | 14  | 18  | 27  | 29  | 24  | -4.67    |
| 12           | 34  | 14  | 24  | 27  | 18  | 29  | -0.67    |
| 13           | 34  | 18  | 24  | 14  | 27  | 29  | 2        |
| 14           | 14  | 27  | 18  | 34  | 29  | 24  | -9.33    |
| 15           | 14  | 27  | 24  | 34  | 18  | 29  | -5.33    |
| 16           | 18  | 27  | 24  | 14  | 34  | 29  | -2.67    |
| 17           | 14  | 18  | 29  | 34  | 27  | 24  | -8       |
| 18           | 14  | 24  | 29  | 34  | 18  | 27  | -4       |
| 19           | 18  | 24  | 29  | 14  | 34  | 27  | -1.33    |
| 20           | 14  | 18  | 24  | 34  | 27  | 29  | -11.33   |

: A Simple Illustration of Randomization Inference {.striped .hover .borderless .secondary}
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(Statamarkdown)
```
```{stata}
clear
input Y W
34 1
27 1
29 1
14 0
18 0
24 0
end


sum Y if W == 1
scalar mean_Y1 = r(mean)
sum Y if W == 0
scalar mean_Y0 = r(mean)

scalar tau_hat = mean_Y1 - mean_Y0


set seed 12345
local num_perm 20
tempfile originaldata
save `originaldata'
tempfile permresults
clear

* Crear permutaciones y calcular las diferencias de medias
qui forval i = 1/`num_perm' {
    clear
    use `originaldata', clear
    generate double u = runiform()
    sort u
    generate Wperm = cond(_n <= 3, 1, 0)
    drop u

    sum Y if Wperm == 1
    scalar mean_Y1_perm = r(mean)
    sum Y if Wperm == 0
    scalar mean_Y0_perm = r(mean)
    scalar tau_perm = mean_Y1_perm - mean_Y0_perm

    clear
    set obs 1
    gen tau_perm_i = scalar(tau_perm)
    if `i' == 1 {
        save `permresults', replace
    }
    else {
        append using `permresults'
        save `permresults', replace
    }
}

* Cargar los resultados de permutaciones y calcular p-valores
use `permresults', clear
gen abs_tau_perm = abs(tau_perm_i)
gen abs_tau_hat = abs(scalar(tau_hat))


count if abs_tau_perm >= abs_tau_hat
scalar p_2side = r(N) / `num_perm'

display "Two sided p-value: " p_2side

histogram tau_perm_i, bin(5) 
```



## Block 2.1

To understand the equivalence between regression analysis and the comparison of means in a binary regression set-up, we refer to Section 2.1 of the online coding resource. In this section, we work with data from a randomized control trial that examines asset transfers to poor households in India, as discussed in the paper by Banerjee et al. (2021).

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(Statamarkdown)
```

```{stata}
clear all
set more off
* Load the household data
use "Datasets/Banerjee_et_al_2021.dta", clear

/* Simple Regression for ind_fin_el1: We run a simple regression of the variable ind_fin_el1 on the treatment variable, filtering the data to only include observations where el1 == 1.This is done to isolate the effect of the treatment on a particular group or under certain conditions, where in this case el1 corresponds to the first wave of responses.
*/
  
reg ind_fin_el1 treatment if el1==1
scalar coef_treatment = _b[treatment]


* Calculate mean for the treatment group
summarize ind_fin_el1 if treatment==1 & el1==1
scalar mean_treatment = r(mean)


* Calculate mean for the control group
summarize ind_fin_el1 if treatment==0 & el1==1
scalar mean_control = r(mean)


* Calculate and display the difference in means
scalar diff_means = mean_treatment - mean_control
display "Coefficient for treatment in regression: " coef_treatment
display "Difference in means (Treatment - Control): " diff_means

* Display the comparison
di "The coefficient from the regression should be equal to the difference in means to demonstrate equivalence."
```

By following these steps, we can empirically verify that in a binary regression set-up, the coefficient for the treatment variable in a simple OLS regression is equivalent to the difference in means between the treatment and control groups. This serves as a useful check for the validity of our regression model and strengthens the causal interpretation of the treatment effect.

## Block 2.2: Randomization Inference

Randomization inference, although a theoretical concept, is best illustrated with practical examples. A particularly illustrative approach is visualization through tabular permutation. The following online coding resource provides a detailed introduction to this method. In this context, we will work with data from the study "Long-Term effects of the Targeting the Ultra Poor Program" conducted by Abhijit Banerjee, Esther Duflo, and Garima Sharma.  Here we will work with data from @Banerjeeetal2021
```{stata}
clear all
set more off

// Replace "path_to_file" with the actual path to your CSV file
import delimited "Datasets/Banerjee_et_al_2021.csv", clear

program randomization_inference
    args varname
    tempvar treated_effect control_effect
    quietly {
        sum `varname' if treatment == 1
        scalar treated_effect = r(mean)
        sum `varname' if treatment == 0
        scalar control_effect = r(mean)
    }
    scalar obs_effect = treated_effect - control_effect

    // Set number of permutations
    local n_permutations = 10000
    scalar p_value = 0

    forvalues i = 1/`n_permutations' {
        // Generate a permuted treatment variable
        gen perm_treatment = runiform() > 0.5
        quietly {
            sum `varname' if perm_treatment
            scalar treated_effect = r(mean)
            sum `varname' if !perm_treatment
            scalar control_effect = r(mean)
        }
        scalar perm_effect = treated_effect - control_effect

        // Update p-value count
        if (abs(perm_effect) >= abs(obs_effect)) {
            scalar p_value = p_value + 1
        }
        drop perm_treatment
    }

    // Calculate p-value
    scalar p_value = p_value / `n_permutations'
    display "The observed effect of " "`varname'" " is " obs_effect " and its p-value is " p_value
end


randomization_inference ind_fin_el1
randomization_inference asset_ind_tot_el1


```

## Code Call Out 2.3: Bootstrap

For an introduction to the bootstrap, refer to Section 2.4.4 of the book. Here we will examine a computational implementation of a bootstrap standard error and confidence intervals, using data from @Banerjeeetal2021 which we have worked with above. First we will consider the main regression in Code Call-Out 2.1 $$Y_i = \mu + \tau_{ATE}W_i + \varepsilon_i.$$ Here we will work with the data we have loaded previously as data, and the condition `el1==1` which are those observations from the first endline survey, and examine the asset index (`asset_ind_tot_el1`) as our outcome measure $Y_i$. We can load the data and examine the estimated treatment effect via regression below.

```{stata echo = 2:3}
qui import delimited "Datasets/Banerjee_et_al_2021.csv", clear
keep if el1 == 1 & treatment != . & asset_ind_tot_el1 != .
reg asset_ind_tot_el1 treatment
```

From the summary we can note that $\widehat{\tau}_{ATE} = 0.408$ and its standard error equals 0.117. This suggests a p-value of 0.0005, and 95% confidence intervals of \[0.177;0.638\]. In this case the standard errors are computed via the standard OLS variance estimator, assuming homoscedastic errors, although heteroscedasticity-robust standard errors can be requested quite simply using the `robust` option from the command `reggress` This is showed below, resulting in slight variations to the reported standard error (and correspondingly, p-value and resulting confidence intervals).

```{stata echo = 3}
qui import delimited "Datasets/Banerjee_et_al_2021.csv", clear
qui keep if el1 == 1 & treatment != . & asset_ind_tot_el1 != .
reg asset_ind_tot_el1 treatment, robust
```

An alternative to these closed-form methods for variance estimation is to use bootstrap resampling methods. There are multiple ways a bootstrap can be implemented, but the simplest here will be to simply conduct a paired bootstrap, which is also robust to heteroscedasticity. The pairs bootstrap consists of, first, sampling with replacement the ordered pairs $\left\{(y_i,w_i)\right\}_{i=1}^{N} = \left\{(y_1,w_1),\ldots,(y_N,w_N)\right\}$ from original data, obtaining a "new" dataset of $N$ resampled pairs $\left\{(y_i^*,w_i^*)\right\}_{i=1}^{N} = \left\{(y_1^*,w_1^*),\ldots,(y_N^*,w_N^*)\right\}$. The new dataset thus simply consists of (potentially repeated) randomly selected rows of the original data. Let's see what this looks like with a single bootstrap replicate. First, we will generate this new dataset, having a look at the first 10 rows of the new dataset.

```{stata echo = 3:6}
qui import delimited "Datasets/Banerjee_et_al_2021.csv", clear
qui keep if el1 == 1 & treatment != . & asset_ind_tot_el1 != .
preserve
set seed 121316
bsample
list in 1/10
```

With this bootstrap sample we estimate again the coefficient of interest with standard OLS procedure:

```{stata echo = 8:9}
quietly{
  import delimited "Datasets/Banerjee_et_al_2021.csv", clear
  keep if el1 == 1 & treatment != . & asset_ind_tot_el1 != .
  preserve
  set seed 121316
  bsample
}
reg asset_ind_tot_el1 treatment
restore
```

With this sample, we have obtained an estimate of $\widehat{\tau}_{ATE}^* = 0.418$; a different value to the original value, in line with variation in the sample used for estimation. If we want some idea of how much estimates vary as the sample changes (ie the sampling variation of our estimate), the next step simply consists of repeating this bootstrap process $B$ times, resulting in $B$ estimates of $\widehat{\tau}_{ATE,(b)}^*$ with $b\in\{1,\ldots,B\}$. Let's set $B=1,000$, and do this:

```{stata echo = 5:14}
quietly{
  import delimited "Datasets/Banerjee_et_al_2021.csv", clear
  keep if el1 == 1 & treatment != . & asset_ind_tot_el1 != .
}
matrix taus = J(1000, 1, .)
forvalues i = 1/1000 {
  quietly{
    preserve
    bsample
    reg asset_ind_tot_el1 treatment
    matrix define taus[`i',1] = _b[treatment]
    restore
  }
}
```

You may wish to step through each of the lines in this loop to ensure that you can see what is going on, but the end product of this code is a column matrix called `taus` which has been filled in with $B$ estimates of $\tau$, $\left\{\widehat{\tau}^{(b)}\right\}_{b=1}^{B}$. The next step is to convert this column matrix into a variable in our dataset

```{stata echo = 15:16}
quietly{
  import delimited "Datasets/Banerjee_et_al_2021.csv", clear
  keep if el1 == 1 & treatment != . & asset_ind_tot_el1 != .
  matrix taus = J(1000, 1, .)
  forvalues i = 1/1000 {
    quietly{
        preserve
        bsample
        reg asset_ind_tot_el1 treatment
        matrix define taus[`i',1] = _b[treatment]
        restore
      }
  }
}
clear 
svmat taus, names("taus")
```

We can have a look at the entire empirical distribution of these estimates as follows: <!-- In order to get the graph the blindschemes package from SSC mut be installed.    -->

```{stata echo = 17:20}
quietly{
  import delimited "Datasets/Banerjee_et_al_2021.csv", clear
  keep if el1 == 1 & treatment != . & asset_ind_tot_el1 != .
  matrix taus = J(1000, 1, .)
  forvalues i = 1/1000 {
    quietly{
        preserve
        bsample
        reg asset_ind_tot_el1 treatment
        matrix define taus[`i',1] = _b[treatment]
        restore
      }
  }
  clear 
  svmat taus, names("taus")
}
_pctile taus1, percentiles(2.5 97.5)
local p25 = r(r1)
local p975 = r(r2)
kdensity taus1, scheme(plottig) title("{&tau} Empirical Distribution") xtitle("{&tau}") xlabel(, format(%3.2f)) xline(`p25' `p975', lcolor(red) lpattern(dash)) xline(0.407753, lcolor(gs8) lpattern(solid) lwidth(vthick))
qui graph export "BootstrapExample.png", replace
```

![Bootstrap Plot Example](BootstrapExample.png)

As we can see, this empirical distribution is close to centred on the original estimate (in the limit, they will be exactly the same, which is something you may wish to confirm by using a larger value for $B$), while also giving us some idea of the variation of the estimate over alternative resamples. We additionally plot the empirical 2.5^th^ and 97.5^th^ quantiles of the distribution, which provides a 95% confidence interval for $\widehat\tau$. Clearly, and in line with the regression results observed above, we can reject the null that $\tau=0$ with some certainty. Using these values $\left\{\widehat{\tau}^{(b)}\right\}_{b=1}^{B}$, finally we display the estimate's standard error as the standard deviation of the bootstrap estimates, and the confidence intervals can be generated from empirical quantiles. <!-- Similarly, if we wish to calculate a p-value, we can do so, asking how many values of the empirical distribution are more extreme than the estimate of interest.    --> We illustrate this below:

```{stata, echo=17:23}
quietly{
  import delimited "Datasets/Banerjee_et_al_2021.csv", clear
  keep if el1 == 1 & treatment != . & asset_ind_tot_el1 != .
  matrix taus = J(1000, 1, .)
  forvalues i = 1/1000 {
    quietly{
        preserve
        bsample
        reg asset_ind_tot_el1 treatment
        matrix define taus[`i',1] = _b[treatment]
        restore
      }
  }
  clear 
  svmat taus, names("taus")
}
qui sum taus1
di as text "The variance estimate is: " as result round(r(Var), 0.0001) ///
as text " as such, the standard error estimates is: " as result round(r(sd), 0.0001)
_pctile taus1, percentiles(2.5 97.5)
di as text "The empirical 95% confidence interval is: [" ///
as result round(r(r1), 0.0001) as text "," ///
as result round(r(r2), 0.0001) as text "]."
```

We can see that the standard error, and hence 95% confidence interval, is very closely alligned to that from regression documented above, given both are valid under broadly similar assumptions.
